
<!-- TOC -->

- [什么是状态一致性？](#什么是状态一致性)
- [分类：](#分类)
- [面试题：](#面试题)
  - [flink是如何保证状态一致性的？（内部保证）](#flink是如何保证状态一致性的内部保证)
- [Exactly-once两阶段提交步骤](#exactly-once两阶段提交步骤)

<!-- /TOC -->

## 什么是状态一致性？

- 有状态的流处理，内部每个算子任务都可以有自己的状态。
- **对于流处理内部来说，所谓的状态一致性，其实就是我们所说的计算结果要保证准确。**
- 一条数据不应该丢失，也不应该重复计算。
- 在遇到故障的时候可以状态恢复，恢复以后重新计算，计算结果也是完全正确的。

## 分类：

- AT-MOST-ONCE(最多一次):
  
  当故障发生的时候，什么都不干。就是说每条消息就只消费一次。
- AT-LEAST-ONCE(至少一次):


  为了确保数据不丢失，确保每个时间都得到处理，一些时间可能会被处理多次。
- EXACTLY-ONCE(精确一次):
  
  每个时间都精确处理一次。

## 面试题：

### flink是如何保证状态一致性的？（内部保证）

- flink使用了一种轻量级的快照机制--**检查点**（checkpoint）来保证exactly-once语义。
- **所有任务的状态，在某个时间点的一份快照。这个时间点，所有任务都恰好处理完一个相同的数据输入。**
- flink的故障恢复核心集市状态**一致性检查。**

**端到端（end-to-end）状态一致性**

- 在flink的流处理器内部由**流处理器**实现一致性，**一致性除了需要在flink内部保证还需要保证数据源和输出端的一致性。**
- **端到端的一致性，意味着整个数据流，每个组件都有自己的一致性。**
- **一致性的级别取决于所有组件中一致性最弱的组件。**

**端到端的保证：**

- **内部保证-- - checkpoint**
- **source端- -可重设数据的读取位置**
- **sink端一-从故障恢复时，数据不会重复写入外部系统**

```text
两种写入方式：
幂等写入：一个操作多次执行，但是只改一次结果，重复的不起作用。
事务写入：构建的事务对应着checkpoint,等到checkpoint真正完成的时候，才把对应的结果写入sink中。
```

**两种事务写入的实现方式:**

```text
1. 预写日志( Write- Ahead-Log , WAL )
2. 把结果数据先当成状态保存，然后在收到checkpoint完成的通知时,一次性写入sink系统
3. 简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定
4. DataStream API提供了-个模板类: GenericWriteAheadSink, 来实现这种事务性sink

两阶段提交(Two- Phase-Commit,2PC )
1. 对于每个checkpoint, sink任务会启动一个事务，并将接下来所有接收的数据添加到事务里
2. 然后将这些数据写入外部sink系统，但不提交它们--这时只是“预提交”)
3. 当它收到checkpoint完成的通知时，它才正式提交事务，实现结果的真正写入

这种方式真正实现了exactly-once，它需要一个提供事 务支持的外部sink系统。

Flink 提供了TwoPhaseCommitSinkFunction接口。
```

> 2PC对外部sink系统的要求:

- 外部sink系统必须提供事务支持，或者sink任务必须能够模拟外部系统上的事务。
- 在checkpoint的间隔期间里，必须能够开启一个事务并接受数据写入。
- 在收到checkpoint完成的通知之前，事务必须是“等待提交”的状态。在故障恢复的情况下，这可能需要-些时间。如果这个时候sink系统关闭事务(例如超时了) ，那么未提交的数据就会丢失
- sink任务必须能够在进程失败后恢复事务。
- 提交事务必须是幂等操作。

## Exactly-once两阶段提交步骤

- 第一条数据来了之后，开启一个kafka的事务(transaction) ，正常写入kafka分区日志但标记为未提交,这就是“预提交”
- jobmanager 触发checkpoint操作，barrier 从source开始向下传递，遇到barrier的算子将状态存入状态后端,并通知jobmanager
- sink 连接器收到barrier,保存当前状态，存入checkpoint,通知jobmanager,并开启下一阶段的事务，用于提交下个检查点的数据:
- jobmanager收到所有任务的通知，发出确认信息，表示checkpoint完成
- sink任务收到jobmanager的确认信息，正式提交这段时间的数据
- 外部kafka关闭事务，提交的数据可以正常消费了。