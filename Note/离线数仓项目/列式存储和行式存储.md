## 列式存储和行式存储

### 为什么需要列式存储

列式存储(Columnar or column-based)是相对于传统关系型数据库的行式存储(Row-basedstorage)来说的。简单来说两者的区别就是如何组织表。

传统的数据编码方式是以**行**为单位进行，列式存储则是将数据**划分成数据块**，每个数据块内部按**列**的方式进行编码存储，通过使用列式存储会有以下好处：

- 存储效率更高，因为同一列的数据类型一致，编码效率也会更高
- 查询效率更高，利用列式存储的统计信息，可以跳过大量的数据，减少IO压力

我们来看看行式存储和列式存储式如何组织数据的：

![1640318659118](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/120420-334270.png)

从上图可以很清楚地看到，行式存储下一张表的数据都是放在一起的，但列式存储下都被分开保存了。所以它们就有了如下这些优缺点：

![1640318694066](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/120455-63974.png)

![1640318726245](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/120527-239448.png)

### 存储

传统RDBMS以行单位做数据存储（字段为空则赋值为‘NULL'），列式存储数据库以列为单位做数据存储。如下：

![1640318828634](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/120710-889241.png)

对于列式存储来说，一行数据包含一个列或者多个列，每个列一单独一个cell来存储数据。而行式存储，则是把一行数据作为一个整体来存储。

 另外一个不得不提的是，列式存储天生就是适合压缩，因为同一列里面的**数据类型**基本是相同，笔者在之前使用普通的gzip压缩，200MB的字符串数据，经过压缩之后只剩下8MB。当然gzip并不属于增量压缩，我们可以选择增量压缩的方式来满足一些数据的随机查找。

### 查询

从查询来说，**行式存储比较适合随机查询**，并且RDBMS大多提供二级索引，在整行数据的读取上，要优于列式存储。但是，**行式存储不适合扫描**，这意味着要查询一个范围的数据，行式存储需要扫描整个表（因为这些记录不是顺序存储的），在索引建立不当的情况下，查询大表（千万数据）简直是噩梦。

列式存储，最早接触的可能不是所谓的BigTable，大多数以前的数据仓库都是采用列式存储。列式存储一般行KEY，列KEY(不是列值）都是顺序存储，比如我要查询一个时间段里面，某个值的出现频率，我可能只需要涉及到两个列。

在分析上，列式存储要优于行式存储，列式存储的数据库一般情况下也强烈建议用户按需查找，而不是整行数据去获取。列式存储在这方面减少了IO的压力。

### 性能对比

**一般原因**：查询需要的字段时，Column-Store **只需读取需要的列**，Row-Store读一条记录会把 **所有字段都读出来**。Column-Stroe在IO上效率更高。

**块遍历**
单记录遍历：读出行，对每行抽取出需要的列，再对这些列调用相应的函数。函数调用的次数与数据的条数为1:11:11:1。
块遍历：批量处理，一次性处理多条数据，函数调用次数下降，性能提高。
 .
块遍历和单记录遍历是两种遍历方法，Column-Store和Row-Store都可以使用。区别是：对于Column-Store，块遍历已经成为**共识**，大家都这样做。对于Row-Store，需要用**case-by-case**实现。
并且Column-Store的列值在**字节意义上等宽**时（例如，每个列都是数字类型），还可以进一步提高性能。因为从一个块里取值时可以直接用**数组下标**获取数据。以数组的方式我们还可以用现代**CPU**的一些优化措施比如**SIMD**(Single Instruction Multiple Data)来实现**并行化**执行，进一步提高性能。

### **压缩**

压缩就是对数据进行**更高效的编码**，从而用更少的空间表达相同的意思。

而能够进行更高效的编码的前提是**数据要有某种规律**。比如很多数据一样，或者数据的类型一样。

Column把一列数据保存在一起，而**一列的数据类型相同**。当然比Row-Store把一条记录里面不同类型的字段值保存在一起，更有规律，更有规律意味着可以有更高的压缩比。

数据占用的硬盘空间越少，查询时 **IO的时间就越少**。

数据压缩之后，要进行处理很多时候也需要解压缩，解压缩也需要花时间，所以一般需要在**压缩比和解压速度之间做一个权衡**。

有的场景下，可以避免解压缩，直接在压缩数据的格式上进行计算，这样就可以进一步提升性能 。

最好对**数据排序**之后，再对Column-Store使用压缩进行优化。如果不经过排序，数据就没那么“有规律”，也就达不到最好的压缩比。

> 没有说谁比谁更优，在正真实战的情况，考虑实际情况。比如传统的RDBMS提供ACID原子操作和事务，在大多数列式存储数据库上是不具备的，大多数列式存储数据库对分布式支持友好。

### hive数仓文件存储格式和压缩格式

从Hive官网得知，Apache Hive支持Apache Hadoop中使用的几种熟悉的文件格式，如 `TextFile（文本格式）`，`RCFile（行列式文件）`，`SequenceFile（二进制序列化文件）`，`AVRO`，`ORC（优化的行列式文件）`和`Parquet` 格式，而这其中我们目前使用最多的是`TextFile`，`SequenceFile`，`ORC`和`Parquet`。

#### orc文件存储格式

![1640319567213](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1640319567213.png)

 左边的图就表示了传统的行式数据库存储方式，按行存储，如果没有存储索引的话，如果需要查询一个字段，就需要把整行的数据都查出来然后做筛选，这么做式比较消耗IO资源的，于是在Hive种最开始也是用了索引的方式来解决这个问题。

但是由于索引的高成本，在**「目前的Hive3.X 中，已经废除了索引」**，当然也早就引入了列式存储。

列式存储的存储方式，其实和名字一样，它是按照一列一列存储的，如上图中的右图，这样的话如果查询一个字段的数据，就等于是索引查询，效率高。但是如果需要查全表，它因为需要分别取所有的列最后汇总，反而更占用资源。于是ORC行列式存储出现了。

1. 在需要全表扫描时，可以按照行组读取
2. 如果需要取列数据，在行组的基础上，读取指定的列，而不需要所有行组内所有行的数据和一行内所有字段的数据。

了解了ORC存储的基本逻辑后，我们再来看看它的存储模型图。 

![1640319698000](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/122138-852773.png)

- 条带( stripe)：ORC文件存储数据的地方，每个stripe一般为HDFS的块大小。（包含以下3部分）

```text
index data:保存了所在条带的一些统计信息,以及数据在 stripe中的位置索引信息。
rows data:数据存储的地方,由多个行组构成，每10000行构成一个行组，数据以流( stream)的形式进行存储。
stripe footer:保存数据所在的文件目录
```

- 文件脚注( file footer)：包含了文件中sipe的列表,每个 stripe的行数,以及每个列的数据类型。它还包含每个列的最小值、最大值、行计数、求和等聚合信息。
- postscript：含有压缩参数和压缩大小相关的信息

所以其实发现，ORC提供了3级索引，文件级、条带级、行组级，所以在查询的时候，利用这些索引可以规避大部分不满足查询条件的文件和数据块。

但注意，ORC中所有数据的描述信息都是和存储的数据放在一起的，并没有借助外部的数据库。

**「特别注意：ORC格式的表还支持事务ACID，但是支持事务的表必须为分桶表，所以适用于更新大批量的数据，不建议用事务频繁的更新小批量的数据」**

~~~ java
#开启并发支持,支持插入、删除和更新的事务
set hive. support concurrency=truei
#支持ACID事务的表必须为分桶表
set hive. enforce bucketing=truei
#开启事物需要开启动态分区非严格模式
set hive.exec,dynamicpartition.mode-nonstrict
#设置事务所管理类型为 org. apache.hive.q1. lockage. DbTxnManager
#原有的org. apache. hadoop.hive.q1.1 eckmar. DummyTxnManager不支持事务
set hive. txn. manager=org. apache. hadoop. hive. q1. lockmgr DbTxnManageri
#开启在相同的一个 meatore实例运行初始化和清理的线程
set hive. compactor initiator on=true:
#设置每个 metastore实例运行的线程数 hadoop
set hive. compactor. worker threads=l
#(2)创建表
create table student_txn
(id int,
name string
)
#必须支持分桶
clustered by (id) into 2 buckets
#在表属性中添加支持事务
stored as orc
TBLPROPERTIES('transactional'='true‘);
#(3)插入数据
#插入id为1001,名字为student 1001
insert into table student_txn values('1001','student 1001');
#(4)更新数据
#更新数据
update student_txn set name= 'student 1zh' where id='1001';
# (5)查看表的数据,最终会发现id为1001被改为 sutdent_1zh
~~~

#### **Parquet**

上面说过ORC后，我们对行列式存储也有了基本的了解，而Parquet是另一种高性能的行列式存储结构。

**Parquet的存储结构**

既然ORC都那么高效了，那为什么还要再来一个Parquet，那是因为**「Parquet是为了使Hadoop生态系统中的任何项目都可以使用压缩的，高效的列式数据表示形式」**

> ❝ Parquet 是语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与 Parquet 配合的组件有：
>  查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL
>  计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite
>  数据模型: Avro, Thrift, Protocol Buffers, POJOs
>  ❞

![1640319821658](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/122342-158629.png)

 Parquet文件是以二进制方式存储的，所以不可以直接读取，和ORC一样，文件的元数据和数据一起存储，所以Parquet格式文件是自解析的。

1. 行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。
2. 列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。
3. 页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。

**Parquet的表配置属性**

- parquet. block size:默认值为134217728byte,即128MB,表示 Row Group在内存中的块大小。该值设置得大,可以提升 Parquet文件的读取效率,但是相应在写的时候需要耗费更多的内存
- parquet.   page:size:默认值为1048576byt,即1MB,表示每个页(page)的大小。这个特指压缩后的页大小,在读取时会先将页的数据进行解压。页是  Parquet操作数据的最小单位,每次读取时必须读完一整页的数据才能访问数据。这个值如果设置得过小,会导致压缩时出现性能问题
- parquet. compression:默认值为 UNCOMPRESSED，表示页的压缩方式。**「可以使用的压缩方式有 UNCOMPRESSED、 SNAPPY、GZP和LZO」**。
- Parquet enable. dictionary:默认为tue,表示是否启用字典编码。
- parquet.  dictionary page.size:默认值为1048576byte,即1MB。在使用字典编码时,会在  Parquet的每行每列中创建一个字典页。使用字典编码,如果存储的数据页中重复的数据较多,能够起到一个很好的压缩效果,也能减少每个页在内存的占用。

#### 对比

![1640319922921](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/24/122523-134385.png)

#### 压缩方式

ORC格式存储，Snappy压缩

Parquet格式存储，Lzo压缩

Parquet格式存储，Snappy压缩

因为Hive 的SQL会转化为MR任务，如果该文件是用ORC存储，Snappy压缩的，因为Snappy不支持文件分割操作，所以压缩文件**「只会被一个任务所读取」**，如果该压缩文件很大，那么处理该文件的Map需要花费的时间会远多于读取普通文件的Map时间，这就是常说的**「Map读取文件的数据倾斜」**。

那么为了避免这种情况的发生，就需要在数据压缩的时候采用bzip2和Zip等支持文件分割的压缩算法。但恰恰ORC不支持刚说到的这些压缩方式，所以这也就成为了大家在可能遇到大文件的情况下不选择ORC的原因，避免数据倾斜。

在Hve  on  Spark的方式中，也是一样的，Spark作为分布式架构，通常会尝试从多个不同机器上一起读入数据。要实现这种情况，每个工作节点都必须能够找到一条新记录的开端，也就需要该文件可以进行分割，但是有些不可以分割的压缩格式的文件，必须要单个节点来读入所有数据，这就很容易产生性能瓶颈。(下一篇文章详细写Spark读取文件的源码分析)

**「所以在实际生产中，使用Parquet存储，lzo压缩的方式更为常见，这种情况下可以避免由于读取不可分割大文件引发的数据倾斜。 但是，如果数据量并不大（预测不会有超大文件，若干G以上）的情况下，使用ORC存储，snappy压缩的效率还是非常高的。」**