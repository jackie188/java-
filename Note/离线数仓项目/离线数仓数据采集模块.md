## 第三章，数据采集

### Hadoop安装和配置

1. 集群规划

|      | 服务器hadoop100  | 服务器hadoop102            | 服务器hadoop103           |
| ---- | ---------------- | -------------------------- | ------------------------- |
| HDFS | NameNodeDataNode | DataNode                   | DataNodeSecondaryNameNode |
| Yarn | NodeManager      | ResourcemanagerNodeManager | NodeManager               |

**配置文件的一些说明**

- core-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop100:8020</value> //端口号表示datanode向namenode请求文件时候访问的端口号，既内部交流使用的端口号，我使用的是9000端口号，版本3默认使用的是8020端口
</property>
<!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        //指的是namenode或者datanode存储数据的目录，实际上真正存储数据的是另外两个参数，只不过另外两个参数的默认值是当前这里的参数值
        <value>/opt/module/hadoop-3.1.3/data</value>
</property>

//hadoop3后添加的内容
<!-- 配置HDFS网页登录使用的静态用户为atguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>rzf</value>
</property>
//那个用户启动hdfs，那个用户就是hadfs的超级用户
<!-- 配置该rzf(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.rzf.hosts</name>
        <value>*</value>
</property>
<!-- 配置该rzf(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.rzf.groups</name>
        <value>*</value>
</property>
<!-- 配置该rzf(superUser)允许通过代理的用户-->
    <property>
        <name>hadoop.proxyuser.rzf.groups</name>
        <value>*</value>
</property>
</configuration>
~~~

- hdfs-site.xml

~~~ JAVA
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- nn web端访问地址,也就是输入url时候的地址和端口-->
	<property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop100:9870</value>
    </property>
    
	<!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop102:9868</value>
    </property>
    
    <!-- 测试环境指定HDFS副本的数量1 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
~~~

- 配置yarn-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    
    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop102</value>
    </property>
    
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    
    <!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    
    <!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>
    
    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
~~~

- 配置mapred-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
~~~

- 配置workers

~~~ java
hadoop100
hadoop101
hadoop102
~~~

- 配置历史服务器，在mapred-site.xml添加如下内容

~~~ java
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop101:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop101:19888</value>
</property>
~~~

历史服务器部署在那一台节点上面，以后就要在那一台节点上面启动。

配置日志聚集功能

- 日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。

- 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。

- 注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。

开启日志聚集功能具体步骤如下：

- 配置yarn-site.xml

~~~ java
<!-- 开启日志聚集功能 -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>

<!-- 设置日志聚集服务器地址，也就是历史服务器的地址 -->
<property>  
    <name>yarn.log.server.url</name>  
    <value>http://hadoop101:19888/jobhistory/logs</value>
</property>

<!-- 设置日志保留时间为7天 -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
~~~

### 多目录配置

- 生产环境服务器磁盘情况,在生产环境中，可能有多个磁盘，多个位置可以存储namenode和datanode的数据，如果不配置多目录，那么数据仅仅会存储在一个目录中，所以要进行多目录配置，把数据存储在多个磁盘中。就像下面图中，有多个磁盘，所以需要配置多目录将namenode和datanode的数据存储在多个磁盘上面。

![1621242833109](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1621242833109.png)

- 在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

- HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为file://${hadoop.tmp.dir}/dfs/data，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。

- 真正决定datanode和namenode数据存储的两个参数是：dfs.datanode.data.dir，dfs.namenode.name.dir，只不过这两个参数的默认值是file://${hadoop.tmp.dir}/dfs/data

```java
<property>
//这个路径才是决定datanode数据存储的路径
    <name>dfs.datanode.data.dir</name>
    file://  ：表示的是协议
    /data1:	：表示的是根目录下的data1目录
<value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
</property>
```

### 集群的数据均衡

**节点间数据均衡**

- 开启数据均衡命令：

~~~ java
start-balancer.sh -threshold 10
~~~

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

- 停止数据均衡命令：

~~~ java
stop-balancer.sh
//注意，停止数据均衡之后，之前已经均衡过的数据不会再变化
~~~

**磁盘间数据均衡**

- 生成均衡计划（**我们只有一块磁盘，不会生成计划**）

~~~ java
hdfs diskbalancer -plan hadoop100
//执行后会生成一个文件
~~~

- 执行均衡计划

~~~ java
hdfs diskbalancer -execute hadoop100.plan.json
~~~

- 查看当前均衡任务的执行情况

~~~ java
hdfs diskbalancer -query hadoop100
~~~

- 取消均衡任务

~~~ java
hdfs diskbalancer -cancel hadoop100.plan.json
~~~

### 支持LZO的压缩配置

hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译，编译步骤如下。

```java
//1,先下载lzo的jar项目
https://github.com/twitter/hadoop-lzo/archive/master.zip
//2,下载后的文件名是hadoop-lzo-master，它是一个zip格式的压缩包，先进行解压，然后用maven编译。生成hadoop-lzo-0.4.20.jar。
//3,将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/
//4，同步hadoop-lzo-0.4.20.jar到hadoop102、hadoop103
//5,core-site.xml增加配置支持LZO压缩
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
<name>io.compression.codecs</name>
<value>
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.BZip2Codec,
org.apache.hadoop.io.compress.SnappyCodec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec
</value>
</property>

<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
</configuration>
//6, 同步core-site.xml到hadoop102、hadoop103
//7,启动及查看集群
//如果以上发生错误，可以进入hadoop-2.7.2/logs目录查看日志
```

lzo压缩的好处是支持切片，这样多个切片之间的数据就可以并行处理。但是要支持切片是有条件的，必须要建立索引。

### 项目经验之LZO创建索引

创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。

~~~ java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
~~~

**测试**

1. 将bigtable.lzo（200M）上传到集群的根目录

~~~ java
hadoop fs -mkdir /input
hadoop fs -put bigtable.lzo /input
~~~

2. 执行wordcount程序

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output1
~~~

![1621245492651](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1621245492651.png)

3. 对上传的LZO文件建索引

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo
~~~

4. 再次执行WordCount程序

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output2
~~~

![1621245556752](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/175918-350510.png)

注意：如果以上任务，在运行过程中报如下异常

~~~ java
Container [pid=8468,containerID=container_1594198338753_0001_01_000002] is running 318740992B beyond the 'VIRTUAL' memory limit. Current usage: 111.5 MB of 1 GB physical memory used; 2.4 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree for container_1594198338753_0001_01_000002 :
~~~

解决办法：在hadoop102的/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml文件中增加如下配置，然后分发到hadoop103、hadoop104服务器上，并重新启动集群。

~~~ java
<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.pmem-check-enabled</name>
   <value>false</value>
</property>

<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
</property>
~~~

### 基准测试

1. 测试HDFS写性能

   测试内容：向HDFS集群写10个128M的文件，写数据时候，写入的速度可能与网络有关。

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

//参数说明
-write:表示测试写性能
-nFiles 10 :表示写入10个文件，一般情况下一个文件一个maptask任务，所以一般把maptask设置为最大来测试写性能，一般设置为电脑的核心数-1
-fileSize 128m:表示每一个文件的大小

19/05/02 11:45:23 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
19/05/02 11:45:23 INFO fs.TestDFSIO:            Date & time: Thu May 02 11:45:23 CST 2019
19/05/02 11:45:23 INFO fs.TestDFSIO:        Number of files: 10
19/05/02 11:45:23 INFO fs.TestDFSIO: Total MBytes processed: 1280.0 //写的文件大小
19/05/02 11:45:23 INFO fs.TestDFSIO:      Throughput mb/sec: 10.69751115716984//衡量每一个节点的吞吐量
19/05/02 11:45:23 INFO fs.TestDFSIO: Average IO rate mb/sec: 14.91699504852295//求每一个文件写速率的平均值
19/05/02 11:45:23 INFO fs.TestDFSIO:  IO rate std deviation: 11.160882132355928//反应平均值波动情况的标准差
19/05/02 11:45:23 INFO fs.TestDFSIO:     Test exec time sec: 52.315
```

2, 测试HDFS读性能

测试内容：读取HDFS集群10个128M的文件，读取数据，副本不会产生影响，但是磁盘会产生影响。

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
//重点参数
同写文件时候一样

19/05/02 11:56:36 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
19/05/02 11:56:36 INFO fs.TestDFSIO:            Date & time: Thu May 02 11:56:36 CST 2019
19/05/02 11:56:36 INFO fs.TestDFSIO:        Number of files: 10
19/05/02 11:56:36 INFO fs.TestDFSIO: Total MBytes processed: 1280.0
19/05/02 11:56:36 INFO fs.TestDFSIO:      Throughput mb/sec: 16.001000062503905
19/05/02 11:56:36 INFO fs.TestDFSIO: Average IO rate mb/sec: 17.202795028686523
19/05/02 11:56:36 INFO fs.TestDFSIO:  IO rate std deviation: 4.881590515873911
19/05/02 11:56:36 INFO fs.TestDFSIO:     Test exec time sec: 49.116
19/05/02 11:56:36 INFO fs.TestDFSIO:
```

3. 删除测试生成数据

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean
```

4. 使用Sort程序评测MapReduce

   （1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar randomwriter random-data
```

​	(2) 执行sort程序

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar sort random-data sorted-data
```

​	(3) 验证数据是否真正排好序了

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

### Hadoop参数调优

#### HDFS参数调优hdfs-site.xml

1. dfs.namenode.handler.count

~~~ java
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。
对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
~~~

dfs.namenode.handler.count=20 * loge(Cluster Size)，比如集群规模为8台时，此参数设置为60

> NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数，dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。

2. 编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟

#### YARN参数调优yarn-site.xml

1. 情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

- 解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。

~~~ java
yarn.nodemanager.resource.memory-mb
~~~

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

~~~ java
yarn.scheduler.maximum-allocation-mb
~~~

单个任务可申请的最多物理内存量，默认是8192（MB）。

#### Hadoop宕机

- 如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）

- 如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。

## 第四章，zookeeper

### 集群规划

|           | 服务器hadoop100 | 服务器hadoop101 | 服务器hadoop102 |
| --------- | --------------- | --------------- | --------------- |
| Zookeeper | Zookeeper       | Zookeeper       | Zookeeper       |

- 安装zookeeper请参考另一篇文章
- zookeeper集群启动和停止脚本

```java
#! /bin/bash
case $1 in
"start"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh status"
	done
};;
esac
//增加执行的权限
chmod 777 zk.sh
```

### 集群日志生成脚本

1）将生成的jar包log-collector-0.0.1-SNAPSHOT-jar-with-dependencies.jar拷贝到hadoop101、服务器上，并同步到hadoop102的/opt/module路径下，

2）在hadoop102上执行jar程序

~~~ java
java -classpath log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.atguigu.appclient.AppMain  >/opt/module/test.log//这是日志生成的路径
//最后可以再此路径下面查看生成的日志
~~~

- 集群日志生成启动脚本

~~~ java
#! /bin/bash

	for i in hadoop101 hadoop102 
	do
		ssh $i "java -classpath /opt/module/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.atguigu.appclient.AppMain $1 $2 >/opt/module/test.log &"
	done
//升级权限
hmod 777 lg.sh
~~~

- 集群时间同步脚本

~~~ jabva
#!/bin/bash

log_date=$1 //获取输入的时间

for i in hadoop101 hadoop102 hadoop103
do
	ssh -t $i "sudo date -s $log_date"
done
~~~

在这里为什么要使用-t参数：需要登录线上的一台目标机器A,但是不能直接登录（没有登录权限），需要先登录B机器，然后从B机器跳转到A机器。需要增加-t -t参数来强制伪终端分配，即使标准输入不是终端

~~~ java
https://www.cnblogs.com/kevingrace/p/6110842.html//博文很详细
~~~

- 集群所有进程查看脚本

~~~ java
#!/bin/bash

for i in hadoop101 hadoop102 hadoop103
do
echo***********$i***********************
	ssh $i "$*"
done
~~~

## 第五章，日志采集之Flume

### 集群规划

|                 | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| --------------- | --------------- | --------------- | --------------- |
| Flume(采集日志) | Flume           | Flume           |                 |

### 项目经验之Flume组件（重要）

**Source**

- Taildir Source相比Exec Source、Spooling Directory Source的优势
  - TailDir Source：断点续传、多目录，继承了Exec Source 和Spooling Directory Source的优点。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。
  - Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。假设source传输一段时间的数据之后，source挂掉，如果重新启动source(相当于执行exec source命令），只会重新传输新追加进来的数据，在source挂掉的那一段时间，还会重新追加数据，这些数据在source重新启动后就会丢失。
  - Spooling Directory Source监控目录，不支持断点续传。实时性比较差，只能把一个文件写完之后放入文件夹内，不可以对文件进行追加操作。

- batchSize大小如何设置？
  - Event 1K左右时，500-1000合适（默认为100）

**Channel**

采用Kafka Channel，省去了Sink，提高了效率。

### 日志采集之Flume配置

![1621323833856](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/18/154355-129791.png)

Flume直接读log日志的数据，log日志的格式是app-yyyy-mm-dd.log。

**flume具体配置**

写flume配置文件分为三步骤：

- 给每一个组件命名
- 配置source
- 配置channel
- 配置seek
- 绑定seek和channel和source之间的关系

~~~ java
//（1）在/opt/module/flume/conf目录下创建file-flume-kafka.conf文件

//agent相当于flume中的一个进程，a1是第一个agent的名字
a1.sources=r1 //第一个agent的第一个source命名为r1,可以命名多个source，中间使用空格隔开
a1.channels=c1 //第一个agent的第一个channel命名为c1，也可以有多个channel
  
//配置sources
# configure source
//sources类型,第一个agent的第一个source名字为r1的类型
a1.sources.r1.type = TAILDIR
//用来实现断点续传，实时记录读取到的位置
a1.sources.r1.positionFile =/opt/module/flume/test/log_position.json//记录日志读取位置
//第一个source监控的目录起名字为f1
a1.sources.r1.filegroups = f1
//配置f1的具体的路径
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.+
//配置拦截器
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type=com.rzf.flume.interceptor.LogInterceptor$Builder

//配置channel1
# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop100:9092,hadoop101:9092,hadoop102:9092
a1.channels.c1.kafka.topic = topic_log//日志类型是start,数据发往c1
a1.channels.c1.parseAsFlumeEvent = false

//绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1
~~~

### flume的elt拦截器和分类型拦截器

拦截器的功能，json数据格式的校验。

1. 创建Maven工程flume-interceptor
2. 创建包名：com.rzf.flume.interceptor
3. 在pom.xml文件中添加如下配置

~~~ java
<dependencies>
    <dependency>
        <groupId>org.apache.flume</groupId>
        <artifactId>flume-ng-core</artifactId>
        <version>1.9.0</version>
        <scope>provided</scope>
    </dependency>

    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.62</version>
    </dependency>
</dependencies>

<build>
    <plugins>
        <plugin>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
~~~

**源码**

~~~ java
在com.atguigu.flume.interceptor包下创建JSONUtils类
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONException;

public class JSONUtils {
    public static boolean isJSONValidate(String log){
        try {
            JSON.parse(log);
            return true;
        }catch (JSONException e){
            return false;
        }
    }
}

在com.atguigu.flume.interceptor包下创建LogInterceptor类
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSON;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }
}
~~~

1. 拦截器打包之后，只需要单独包，不需要将依赖的包上传。打包之后要放入Flume的lib文件夹下面。

   注意：为什么不需要依赖包？因为依赖包在flume的lib目录下面已经存在了。

2. 需要先将打好的包放入到hadoop100的/opt/module/flume/lib文件夹下面

3. 分发Flume到hadoop101、hadoop102

~~~ java
//启动flume，并且作为后台程序
bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf &
~~~

4. 启动kafka消费数据

~~~ java
//任何一台机器上启动kafka消费进程
bin/kafka-console-consumer.sh --bootstrap-server hadoop100:9092 --topic topic_log
//启动flume消费数据
bin/flume-ng agent -n a1 -c conf/ -f conf/file-flume-kafka.conf -Dflume.root.logger=info,console
~~~

### flume启动停止脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop100 hadoop101
do 
echo**********$i启动*****************
    	ssh $i "source /etc/profile;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/dev/null 2>&1 &"
done    	
};;
"stop"){
for i in hadoop100 hadoop101
do 
echo " --------停止 $i 采集flume-------"
    	"source /etc/profile；ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs kill"
done
};;
esac
/*说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。
说明2：/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。
标准输入0：从键盘获得输入 /proc/self/fd/0 
标准输出1：输出到屏幕（即控制台） /proc/self/fd/1 
错误输出2：输出到屏幕（即控制台） /proc/self/fd/2*/
//升级权限
chmod 777 f1.sh
~~~

## 第六章，Kafka

### 集群规划

|       | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ----- | --------------- | --------------- | --------------- |
| Kafka | Kafka           | Kafka           | Kafka           |

[Kafka配置安装]()

### Kafka集群启动停止脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop100 hadoop101 hadoop102
do 
echo"************$i启动********************"
		ssh $i "export JMX_PORT=9988 && /opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties "
};;
"stop"){
for i in hadoop100 hadoop101 hadoop102
do
echo"************kafka停止**************"
    	ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
done
};;
esac
//说明：启动Kafka时要先开启JMX端口，是用于后续KafkaManager监控。
//增加脚本执行权限
chmod 777 kf.sh
//查看kafkatopic列表
bin/kafka-topics.sh --zookeeper hadoop101:2181 --list
~~~

如果启动脚本，相应的进程无法启动，那么修改为如下代码

~~~ java
#! /bin/bash

case $1 in
"start"){
    for i in hadoop100 hadoop101 hadoop102
    do
        echo " --------启动 $i Kafka-------"
        ssh $i " source /etc/profile;nohup /opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in hadoop100 hadoop101 hadoop102
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "s·ource /etc/profile; /opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
~~~

### 创建Kafka的topic 

~~~ java
//创建启动日志主题,副本数是1，topic名字是：topic_start,分区个数也是1
bin/kafka-topics.sh --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181  --create --replication-factor 1 --partitions 1 --topic topic_start

//查看创建的topic
bin/kafka-topics.sh --zookeeper hadoop100:2181/kafka --describe --topic first
~~~

### 创建事件日志主题

~~~ java
//创建主题副本数是1，分区个数是1，主题名字是：topic_event
bin/kafka-topics.sh --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181  --create --replication-factor 1 --partitions 1 --topic topic_event
~~~

### 删除kafka主题操作

~~~ java
bin/kafka-topics.sh --delete --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181 --topic topic_start
//这里连接zookeeper任意一台节点都是可以的
~~~

### Kafka生产消息

~~~ java
bin/kafka-console-producer.sh  --broker-list hadoop101:9092 --topic topic_start
>hello world

//写入数据时候是写在每一个topic 的leader上面，但是上面可以写任意的zookeeper节点的地址，写入数据的时候是如何知道leader的具体的位置？
--zookeer的每一个节点都会维护一个topic的元数据信息，根据元数据信息可以很容易的找到每一个topic的leader地址，然后写入数据即可。也就是通过describe查看到的信息。生产者首先去broker中获取元数据的信息，然后根据元数据的信息获取每一个topic的leaser在哪里，下面展示的就是元数据信息

[rzf@hadoop100 kafka]$ bin/kafka-topics.sh --zookeeper hadoop100:2181/kafka --describe --topic first
Topic:first     PartitionCount:3        ReplicationFactor:2     Configs:
        Topic: first    Partition: 0    Leader: 0       Replicas: 0,1   Isr: 0,1
        Topic: first    Partition: 1    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: first    Partition: 2    Leader: 2       Replicas: 2,0   Isr: 2,0
~~~

### Kafka消费消息

~~~ java
bin/kafka-console-consumer.sh \ --zookeeper hadoop101:2181 --from-beginning --topic topic_start
//--from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。
//在这里消费者也需要知道zookeeper的地址，然后去获取每一个topic的元数据信息，然后去leader哪里获取数据
~~~

### 查看Kafka的topic详情

~~~ java
bin/kafka-topics.sh --zookeeper hadoop101:2181 \ --describe --topic topic_start
~~~

### Kafka Manager的安装

- Kafka Manager是yahoo的一个Kafka监控管理项目。

1）下载地址

~~~ java
https://github.com/yahoo/kafka-manager
~~~

下载之后编译源码，编译完成后，拷贝出：kafka-manager-1.3.3.22.zip

2）拷贝kafka-manager-1.3.3.22.zip到hadoop101的/opt/module目录

3）解压kafka-manager-1.3.3.22.zip到/opt/module目录

4）进入到/opt/module/kafka-manager-1.3.3.22/conf目录，在application.conf文件中修改kafka-manager.zkhosts

~~~ java
kafka-manager.zkhosts="hadoop101:2181,hadoop102:2181,hadoop103:2181"
~~~

5) 启动Kafka Manager

~~~ java
nohup bin/kafka-manager   -Dhttp.port=7456 >/opt/module/kafka-manager-1.3.3.22/start.log 2>&1 &
//在浏览器中打开
http://hadoop101:7456
//在Kafka的/opt/module/kafka-manager-1.3.3.22/application.home_IS_UNDEFINED 目录下面，可以看到Kafka-Manager的日志。
~~~

### Kafka Manager启动停止脚本

~~~ java
#! /bin/bash

case $1 in
"start"){
        echo " -------- 启动 KafkaManager -------"
        nohup /opt/module/kafka-manager-1.3.3.22/bin/kafka-manager   -Dhttp.port=7456 >start.log 2>&1 &
};;
"stop"){
        echo " -------- 停止 KafkaManager -------"
        ps -ef | grep ProdServerStart | grep -v grep |awk '{print $2}' | xargs kill 
};;
esac
~~~

### Kafka的压力测试

**Kafka压测**

用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

~~~ java
kafka-consumer-perf-test.sh

kafka-producer-perf-test.sh
~~~

**Kafka Producer压力测试**

- 在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下

~~~ java
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput 1000 --producer-props bootstrap.servers=hadoop101:9092,hadoop102:9092,hadoop103:9092
//说明：record-size是一条信息有多大，单位是字节。num-records是总共发送多少条信息。throughput 是每秒多少条信息。
//消息打印
5000 records sent, 999.4 records/sec (0.10 MB/sec), 1.9 ms avg latency, 254.0 max latency.
5002 records sent, 1000.4 records/sec (0.10 MB/sec), 0.7 ms avg latency, 12.0 max latency.
5001 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 4.0 max latency.
5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.7 ms avg latency, 3.0 max latency.
5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 5.0 max latency.
//参数解析：本例中一共写入10w条消息，每秒向Kafka写入了0.10MB的数据，平均是1000条消息/秒，每次写入的平均延迟为0.8毫秒，最大的延迟为254毫秒。
~~~

说明：

- record-size是一条信息有多大，单位是字节。

- num-records是总共发送多少条信息。

- throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大**吞吐量**。如果throughput设置为具体的数值，那么测试的就是传输数据的延迟。

**Kafka Consumer压力测试**

Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。

~~~ java
bin/kafka-consumer-perf-test.sh --zookeeper hadoop101:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1
~~~

参数说明：

--zookeeper 指定zookeeper的连接地址

--topic 指定topic的名称

--fetch-size 指定每次fetch的数据的大小

--messages 总共要消费的消息个数

~~~ java
测试结果：start.time, **end.time,** data.consumed.in.MB, **MB.sec,** data.consumed.in.nMsg**, nMsg.sec**

2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
~~~

开始测试时间，测试结束数据，最大吞吐率9.5368MB/s，平均每秒消费2.0714MB/s，最大每秒消费100010条，平均每秒消费21722.4153条。

### kafka机器数量计算

**搭建kafka集群需要考虑的问题**

1. 搭建kafka集群需要多少台节点
2. 每个节点上面创建多少个topic ?
3. 每一个topic种分区数量和副本数量设置多少个？

**经验-集群规模数量计算**

Kafka机器数量（经验公式）=2`*`（峰值生产速度`*`副本数/100）+1

副本数量一般设置2到3个合适。

先要预估一天大概产生多少数据，然后用Kafka自带的生产压测（只测试Kafka的写入速度，保证数据不积压），计算出峰值生产速度。再根据设定的副本数，就能预估出需要部署Kafka的数量。

比如我们采用压力测试测出写入的速度是10M/s一台，峰值的业务数据的速度是50M/s。副本数为2。

Kafka机器数量=2（50*2/100）+ 1=3台

**经验-kafka集群分区数量设置**

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量。
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）

例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；

分区数=100 / 20 =5分区

> 分区数一般设置为：3-10个

## 第七章，消费kafka数据的flume

### 集群配置

|                    | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ------------------ | --------------- | --------------- | --------------- |
| Flume（消费Kafka） |                 |                 | Flume           |

![1623745181313](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202106/15/161942-833545.png)

**配置文件**

~~~ java
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false


a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
~~~

- 启动flume

~~~ java
bin/flume-ng agent -n a1 -c conf/ -f conf/kafka-flume-hdfs.conf -Dflume.root.logger=info,console 
~~~

启停脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop102 
do 
echo**********$i启动*****************
    	ssh $i "source /etc/profile;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/dev/null 2>&1 &"
done    	
};;
"stop"){
for i in hadoop102
do 
echo " --------停止 $i 采集flume-------"
    	"source /etc/profile；ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs kill"
done
};;
esac
~~~

