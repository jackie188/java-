# 离线数仓数据采集模块

## 第二章，数据生成模块

### 目标数据

我们要收集和分析的数据主要包括**页面数据**、**事件数据、曝光数据、启动数据和错误数据。**

**按照日志的内容分类**

#### 页面

页面数据主要记录一个页面的用户访问情况，包括**访问时间、停留时间、页面路径**等信息。

![1619415865700](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154419-165594.png)

**所有页面id如下**

~~~ java
home("首页"),
category("分类页"),
discovery("发现页"),
top_n("热门排行"),
favor("收藏页"),
search("搜索页"),
good_list("商品列表页"),
good_detail("商品详情"),
good_spec("商品规格"),
comment("评价"),
comment_done("评价完成"),
comment_list("评价列表"),
cart("购物车"),
trade("下单结算"),
payment("支付页面"),
payment_done("支付完成"),
orders_all("全部订单"),
orders_unpaid("订单待支付"),
orders_undelivered("订单待发货"),
orders_unreceipted("订单待收货"),
orders_wait_comment("订单待评价"),
mine("我的"),
activity("活动"),
login("登录"),
register("注册");
~~~

**所有页面对象类型如下：**

~~~ java
sku_id("商品skuId"),
keyword("搜索关键词"),
sku_ids("多个商品skuId"),
activity_id("活动id"),
coupon_id("购物券id");
~~~

**所有来源类型如下：**

~~~ java
promotion("商品推广"),
recommend("算法推荐商品"),
query("查询结果商品"),
activity("促销活动");
~~~

#### 事件

事件数据主要记录应用内一个具体操作行为，包括**操作类型、操作对象、操作对象描述**等信息。

![1619416283210](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154421-792500.png)

**所有动作类型如下：**

~~~ java
favor_add("添加收藏"),
favor_canel("取消收藏"),
cart_add("添加购物车"),
cart_remove("删除购物车"),
cart_add_num("增加购物车商品数量"),
cart_minus_num("减少购物车商品数量"),
trade_add_address("增加收货地址"),
get_coupon("领取优惠券");
~~~

注：对于下单、支付等业务数据，可从业务数据库获取。

**所有动作目标类型如下：**

~~~ java
sku_id("商品"),
coupon_id("购物券");
~~~

#### 曝光

曝光数据主要记录页面所曝光的内容，包括**曝光对象，曝光类型**等信息。

![1619416406284](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202104/26/135332-977230.png)

**所有曝光类型如下：**

~~~ java
promotion("商品推广"),
recommend("算法推荐商品"),
query("查询结果商品"),
activity("促销活动");
~~~

**所有曝光对象类型如下：**

~~~ java
sku_id("商品skuId"),
activity_id("活动id");
~~~

#### 启动

启动数据记录应用的启动信息。

![1619416581539](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154427-352497.png)

**所有启动入口类型如下：**

~~~ java
icon("图标"),
notification("通知"),
install("安装后启动");
~~~

#### 错误

错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。

### 数据埋点

#### 主流的埋点方式

目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。

**代码埋点**是通过调用埋点SDK（sdk是一些封装好的库）函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。

**可视化埋点**只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。

**全埋点**是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。

#### 埋点数据的基本格式

我们的日志结构大致可分为两类，**一是普通页面埋点日志（包括曝光日志），二是启动日志。**

普通页面日志结构如下，每条日志包含了：

- 当前页面的**页面信息**，
- 所有事件（**动作**）、
- 所有**曝光信息**
- 以及**错误信息**。

除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。

按照日志的结构划分

- 公共字段：基本所有安卓手机都包含的字段

- 业务字段：埋点上报的字段，有具体的业务类型

~~~ java
{
  "common": {                  -- 公共信息
    "ar": "230000",              -- 地区编码
    "ba": "iPhone",              -- 手机品牌
    "ch": "Appstore",            -- 渠道
    "md": "iPhone 8",            -- 手机型号
    "mid": "YXfhjAYH6As2z9Iq", -- 设备id
    "os": "iOS 13.2.9",          -- 操作系统
    "uid": "485",                 -- 会员id
    "vc": "v2.1.134"             -- app版本号
  },
"actions": [                     --动作(事件)  
    {
      "action_id": "favor_add",   --动作id
      "item": "3",                   --目标id
      "item_type": "sku_id",       --目标类型
      "ts": 1585744376605           --动作时间戳
    }
  ]，
  "displays": [
    {
      "displayType": "query",        -- 曝光类型
      "item": "3",                     -- 曝光对象id
      "item_type": "sku_id",         -- 曝光对象类型
      "order": 1                        --出现顺序
    },
    {
      "displayType": "promotion",
      "item": "6",
      "item_type": "sku_id",
      "order": 2
    },
    {
      "displayType": "promotion",
      "item": "9",
      "item_type": "sku_id",
      "order": 3
    },
    {
      "displayType": "recommend",
      "item": "6",
      "item_type": "sku_id",
      "order": 4
    },
    {
      "displayType": "query ",
      "item": "6",
      "item_type": "sku_id",
      "order": 5
    }
  ],
  "page": {                       --页面信息
    "during_time": 7648,        -- 持续时间毫秒
    "item": "3",                  -- 目标id
    "item_type": "sku_id",      -- 目标类型
    "last_page_id": "login",    -- 上页类型
    "page_id": "good_detail",   -- 页面ID
    "sourceType": "promotion"   -- 来源类型
  },
"err":{                     --错误
"error_code": "1234",      --错误码
    "msg": "***********"       --错误信息
},
  "ts": 1585744374423  --跳入时间戳
}

~~~

**格式展示**

一条完整的页面日志包括的字段，是一个页面为单位，一个页面一条日志，公共字段，行为动作，曝光，页面信息，错误信息，页面跳转时间五部分。

![1619418096740](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202104/26/142138-394480.png)

**公共字段**

![1619418199252](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202104/26/142320-885637.png)

**行为字段**

里面存储的是数组，数组中存储json对象，每一个对象表示一个动作。

![1619418263093](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1619418263093.png)

**曝光字段**

也是一个json数组，里面存储的是json对象，每一个对象表示一个曝光对象。

![1619418354321](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202104/26/142555-449526.png)

**page信息**

json对象，存储的是页面信息

![1619418424720](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1619418424720.png)

**error**

错误信息

![1619418454568](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1619418454568.png)

**ts**

浏览页面的时间

![1619418483164](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1619418483164.png)

> 上面展示的是一个最完整的结构，某一些日志信息可能不完整

**启动日志格式**

启动日志结构相对简单，主要包含**公共信息，启动信息和错误信息。**

~~~ java
{
  "common": {
    "ar": "370000",
    "ba": "Honor",
    "ch": "wandoujia",
    "md": "Honor 20s",
    "mid": "eQF5boERMJFOujcp",
    "os": "Android 11.0",
    "uid": "76",
    "vc": "v2.1.134"
  },
  "start": {   
    "entry": "icon",         --icon手机图标  notice 通知   install 安装后启动
    "loading_time": 18803,  --启动加载时间
    "open_ad_id": 7,        --广告页ID
    "open_ad_ms": 3449,    -- 广告总共播放时间
    "open_ad_skip_ms": 1989   --  用户跳过广告时点
  },
"err":{                     --错误
"error_code": "1234",      --错误码
    "msg": "***********"       --错误信息
},
  "ts": 1585744304000
}

~~~

**字段信息**

![1619418630427](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154440-640393.png)

**common**

公共字段的信息

![1619418657205](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154438-477174.png)

**start**

启动信息

![1619418693874](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154435-171225.png)

**err**

错误信息

![1619418726506](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154436-500888.png)

**时间信息**

![1619418744289](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202104/26/143226-946531.png)

#### 埋点数据上报时机

埋点数据上报时机包括两种方式。

方式一，在离开该页面时，上传在这个页面产生的所有数据（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。

方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。

### Json语法

1. 什么是Json?

- JSON 指的是 JavaScript 对象表示法（*J*ava*S*cript *O*bject *N*otation）
- JSON 是轻量级的文本数据交换格式
- JSON 独立于语言 *
- JSON 具有自我描述性，更易理解

2. Json语法？

   JSON 语法是 JavaScript 对象表示法语法的子集。

   - 数据在名称/值对中
   - 数据由逗号分隔
   - 花括号保存对象
   - 方括号保存数组

~~~ java
//1 json名称/值
//JSON 数据的书写格式是：名称/值对。名称/值对包括字段名称（在双引号中），后面写一个冒号，然后是值：
"firstName" : "John"
//2 json值
JSON 值可以是：

    数字（整数或浮点数）
    字符串（在双引号中）
    逻辑值（true 或 false）
    数组（在方括号中）
    对象（在花括号中）
    null
//3 JSON 对象在花括号中书写：
//对象可以包含多个名称/值对：
{ "firstName":"John" , "lastName":"Doe" }
//4 json数组
JSON 数组在方括号中书写：
数组可包含多个对象：
{
"employees": [
{ "firstName":"John" , "lastName":"Doe" },
{ "firstName":"Anna" , "lastName":"Smith" },
{ "firstName":"Peter" , "lastName":"Jones" }
]
}
// 5 json文件

    JSON 文件的文件类型是 ".json"
    JSON 文本的 MIME 类型是 "application/json"

~~~

### 服务器和jdk安装

安装三台虚拟机或者云主机，然后安装上jdk。

#### 环境变量配置说明

Linux的环境变量可在多个文件中配置，如/etc/profile，/etc/profile.d/*.sh，~/.bashrc等，下面说明上述几个文件之间的关系和区别。

bash的运行模式可分为login shell和non-login shell。

例如，我们通过终端，输入用户名、密码，登录系统之后，得到就是一个login shell，而当我们执行以下命令ssh hadoop103 command，在hadoop103执行command的就是一个non-login shell。

这两种shell的主要区别在于，它们启动时会加载不同的配置文件，login shell启动时会加载/etc/profile，non-login shell启动时会加载~/.bashrc。

而在加载~/.bashrc（实际是~/.bashrc中加载的/etc/bashrc）或/etc/profile时，都会执行如下代码片段，

![1619422099419](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/154447-60167.png)

因此不管是loginshell还是non-login shell，启动时都会加载/etc/profile.d/*.sh中的环境变量。

### 模拟数据

hadoop100和hadoop101存放生成的日志文件。

#### 使用说明

将application.properties、gmall2020-mock-log-2020-05-10.jar、path.json、logback.xml上传到hadoop100的/opt/module/applog目录下

**配置文件**

- application.properteis文件，可以根据需求生成对应日期的用户行为日志。

~~~ java
# 外部配置打开
logging.config=./logback.xml
#业务日期
mock.date=2020-06-14 //可以在这里修改日期

#模拟数据发送模式
mock.type=log
#mock.type=http
#http模式下，发送的地址
mock.url=http://localhost:8080/applog

#启动次数
mock.startup.count=100
#设备最大值
mock.max.mid=50
#会员最大值
mock.max.uid=500
#商品最大值
mock.max.sku-id=10
#页面平均访问时间
mock.page.during-time-ms=20000
#错误概率 百分比
mock.error.rate=3
#每条日志发送延迟 ms
mock.log.sleep=10
#商品详情来源  用户查询，商品推广，智能推荐, 促销活动
mock.detail.source-type-rate=40:25:15:20
~~~

- path.json，该文件用来配置访问路径，根据需求，可以灵活配置用户点击路径。

~~~ java
[
  {"path":["home","good_list","good_detail","cart","trade","payment"],"rate":20 },
  {"path":["home","search","good_list","good_detail","login","good_detail","cart","trade","payment"],"rate":50 },
  {"path":["home","mine","orders_unpaid","trade","payment"],"rate":10 },
  {"path":["home","mine","orders_unpaid","good_detail","good_spec","comments","trade","payment"],"rate":10 },
  {"path":["home","mine","orders_unpaid","good_detail","good_spec","comments","home"],"rate":10 }
]
~~~

- logback配置文件可配置日志生成路径，修改内容如下

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property name="LOG_HOME" value="/opt/module/applog/log" />
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 将某一个包下日志单独打印日志 -->
    <logger name="com.atgugu.gmall2020.mock.log.Mocker"
            level="INFO" additivity="true">
        <appender-ref ref="rollingFile" />
         <appender-ref ref="console" />
    </logger>

    <root level="error" additivity="true">
        <appender-ref ref="console" />
        <!-- <appender-ref ref="async-rollingFile" />  -->
    </root>
</configuration>
~~~

**生成日志**

1. 进入到/opt/module/applog路径，执行以下命令

~~~ java
java -jar gmall2020-mock-log-2020-05-10.jar
~~~

2. 拷贝生成的日志文件到另一台节点

~~~ java
scp -r applog/ hadoop101:/opt/module/
~~~

### 集群日志生成脚本

在hadoop100的/home/rzf目录下创建bin目录，这样脚本可以在服务器的任何目录执行。

在bin目录下面创建lg.sh文件，添加下面的内容。

~~~ java
#!/bin/bash
for i in hadoop100 hadoop101; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2020-05-10.jar >/dev/null 2>&1 &" 
done 
>/dev/null 2>&1 &" //表示将日志信息写入到黑洞文件中
~~~

注：

1. /opt/module/applog/为jar包及配置文件所在路径

2. /dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。

- 标准输入0：从键盘获得输入 /proc/self/fd/0 

- 标准输出1：输出到屏幕（即控制台） /proc/self/fd/1 

- 错误输出2：输出到屏幕（即控制台） /proc/self/fd/2

**修改脚本的执行权限**

~~~ java
chmod u+x lg.sh
//至此，可以随意修改时间内进行日志的生成
~~~

## 第三章，数据采集

### Hadoop安装和配置

1. 集群规划

|      | 服务器hadoop100  | 服务器hadoop102            | 服务器hadoop103           |
| ---- | ---------------- | -------------------------- | ------------------------- |
| HDFS | NameNodeDataNode | DataNode                   | DataNodeSecondaryNameNode |
| Yarn | NodeManager      | ResourcemanagerNodeManager | NodeManager               |

**配置文件的一些说明**

- core-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop100:8020</value> //端口号表示datanode向namenode请求文件时候访问的端口号，既内部交流使用的端口号，我使用的是9000端口号，版本3默认使用的是8020端口
</property>
<!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        //指的是namenode或者datanode存储数据的目录，实际上真正存储数据的是另外两个参数，只不过另外两个参数的默认值是当前这里的参数值
        <value>/opt/module/hadoop-3.1.3/data</value>
</property>

//hadoop3后添加的内容
<!-- 配置HDFS网页登录使用的静态用户为atguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>rzf</value>
</property>
//那个用户启动hdfs，那个用户就是hadfs的超级用户
<!-- 配置该rzf(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.rzf.hosts</name>
        <value>*</value>
</property>
<!-- 配置该rzf(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.rzf.groups</name>
        <value>*</value>
</property>
<!-- 配置该rzf(superUser)允许通过代理的用户-->
    <property>
        <name>hadoop.proxyuser.rzf.groups</name>
        <value>*</value>
</property>
</configuration>
~~~

- hdfs-site.xml

~~~ JAVA
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- nn web端访问地址,也就是输入url时候的地址和端口-->
	<property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop100:9870</value>
    </property>
    
	<!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop102:9868</value>
    </property>
    
    <!-- 测试环境指定HDFS副本的数量1 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
~~~

- 配置yarn-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    
    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop102</value>
    </property>
    
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    
    <!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
    
    <!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>
    
    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
~~~

- 配置mapred-site.xml

~~~ java
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
~~~

- 配置workers

~~~ java
hadoop100
hadoop101
hadoop102
~~~

- 配置历史服务器，在mapred-site.xml添加如下内容

~~~ java
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop101:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop101:19888</value>
</property>
~~~

历史服务器部署在那一台节点上面，以后就要在那一台节点上面启动。

配置日志聚集功能

- 日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。

- 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。

- 注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。

开启日志聚集功能具体步骤如下：

- 配置yarn-site.xml

~~~ java
<!-- 开启日志聚集功能 -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>

<!-- 设置日志聚集服务器地址，也就是历史服务器的地址 -->
<property>  
    <name>yarn.log.server.url</name>  
    <value>http://hadoop101:19888/jobhistory/logs</value>
</property>

<!-- 设置日志保留时间为7天 -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
~~~

### 多目录配置

- 生产环境服务器磁盘情况,在生产环境中，可能有多个磁盘，多个位置可以存储namenode和datanode的数据，如果不配置多目录，那么数据仅仅会存储在一个目录中，所以要进行多目录配置，把数据存储在多个磁盘中。就像下面图中，有多个磁盘，所以需要配置多目录将namenode和datanode的数据存储在多个磁盘上面。

![1621242833109](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1621242833109.png)

- 在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

- HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为file://${hadoop.tmp.dir}/dfs/data，若服务器有多个磁盘，必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。

- 真正决定datanode和namenode数据存储的两个参数是：dfs.datanode.data.dir，dfs.namenode.name.dir，只不过这两个参数的默认值是file://${hadoop.tmp.dir}/dfs/data

```java
<property>
//这个路径才是决定datanode数据存储的路径
    <name>dfs.datanode.data.dir</name>
    file://  ：表示的是协议
    /data1:	：表示的是根目录下的data1目录
<value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
</property>
```

### 集群的数据均衡

**节点间数据均衡**

- 开启数据均衡命令：

~~~ java
start-balancer.sh -threshold 10
~~~

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

- 停止数据均衡命令：

~~~ java
stop-balancer.sh
//注意，停止数据均衡之后，之前已经均衡过的数据不会再变化
~~~

**磁盘间数据均衡**

- 生成均衡计划（**我们只有一块磁盘，不会生成计划**）

~~~ java
hdfs diskbalancer -plan hadoop100
//执行后会生成一个文件
~~~

- 执行均衡计划

~~~ java
hdfs diskbalancer -execute hadoop100.plan.json
~~~

- 查看当前均衡任务的执行情况

~~~ java
hdfs diskbalancer -query hadoop100
~~~

- 取消均衡任务

~~~ java
hdfs diskbalancer -cancel hadoop100.plan.json
~~~

### 支持LZO的压缩配置

hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译，编译步骤如下。

```java
//1,先下载lzo的jar项目
https://github.com/twitter/hadoop-lzo/archive/master.zip
//2,下载后的文件名是hadoop-lzo-master，它是一个zip格式的压缩包，先进行解压，然后用maven编译。生成hadoop-lzo-0.4.20.jar。
//3,将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/
//4，同步hadoop-lzo-0.4.20.jar到hadoop102、hadoop103
//5,core-site.xml增加配置支持LZO压缩
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
<name>io.compression.codecs</name>
<value>
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.BZip2Codec,
org.apache.hadoop.io.compress.SnappyCodec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec
</value>
</property>

<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
</configuration>
//6, 同步core-site.xml到hadoop102、hadoop103
//7,启动及查看集群
//如果以上发生错误，可以进入hadoop-2.7.2/logs目录查看日志
```

lzo压缩的好处是支持切片，这样多个切片之间的数据就可以并行处理。但是要支持切片是有条件的，必须要建立索引。

### 项目经验之LZO创建索引

创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。

~~~ java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
~~~

**测试**

1. 将bigtable.lzo（200M）上传到集群的根目录

~~~ java
hadoop fs -mkdir /input
hadoop fs -put bigtable.lzo /input
~~~

2. 执行wordcount程序

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output1
~~~

![1621245492651](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1621245492651.png)

3. 对上传的LZO文件建索引

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo
~~~

4. 再次执行WordCount程序

~~~ java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output2
~~~

![1621245556752](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/17/175918-350510.png)

注意：如果以上任务，在运行过程中报如下异常

~~~ java
Container [pid=8468,containerID=container_1594198338753_0001_01_000002] is running 318740992B beyond the 'VIRTUAL' memory limit. Current usage: 111.5 MB of 1 GB physical memory used; 2.4 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree for container_1594198338753_0001_01_000002 :
~~~

解决办法：在hadoop102的/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml文件中增加如下配置，然后分发到hadoop103、hadoop104服务器上，并重新启动集群。

~~~ java
<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.pmem-check-enabled</name>
   <value>false</value>
</property>

<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
<property>
   <name>yarn.nodemanager.vmem-check-enabled</name>
   <value>false</value>
</property>
~~~

### 基准测试

1. 测试HDFS写性能

   测试内容：向HDFS集群写10个128M的文件，写数据时候，写入的速度可能与网络有关。

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

//参数说明
-write:表示测试写性能
-nFiles 10 :表示写入10个文件，一般情况下一个文件一个maptask任务，所以一般把maptask设置为最大来测试写性能，一般设置为电脑的核心数-1
-fileSize 128m:表示每一个文件的大小

19/05/02 11:45:23 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
19/05/02 11:45:23 INFO fs.TestDFSIO:            Date & time: Thu May 02 11:45:23 CST 2019
19/05/02 11:45:23 INFO fs.TestDFSIO:        Number of files: 10
19/05/02 11:45:23 INFO fs.TestDFSIO: Total MBytes processed: 1280.0 //写的文件大小
19/05/02 11:45:23 INFO fs.TestDFSIO:      Throughput mb/sec: 10.69751115716984//衡量每一个节点的吞吐量
19/05/02 11:45:23 INFO fs.TestDFSIO: Average IO rate mb/sec: 14.91699504852295//求每一个文件写速率的平均值
19/05/02 11:45:23 INFO fs.TestDFSIO:  IO rate std deviation: 11.160882132355928//反应平均值波动情况的标准差
19/05/02 11:45:23 INFO fs.TestDFSIO:     Test exec time sec: 52.315
```

2, 测试HDFS读性能

测试内容：读取HDFS集群10个128M的文件，读取数据，副本不会产生影响，但是磁盘会产生影响。

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
//重点参数
同写文件时候一样

19/05/02 11:56:36 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
19/05/02 11:56:36 INFO fs.TestDFSIO:            Date & time: Thu May 02 11:56:36 CST 2019
19/05/02 11:56:36 INFO fs.TestDFSIO:        Number of files: 10
19/05/02 11:56:36 INFO fs.TestDFSIO: Total MBytes processed: 1280.0
19/05/02 11:56:36 INFO fs.TestDFSIO:      Throughput mb/sec: 16.001000062503905
19/05/02 11:56:36 INFO fs.TestDFSIO: Average IO rate mb/sec: 17.202795028686523
19/05/02 11:56:36 INFO fs.TestDFSIO:  IO rate std deviation: 4.881590515873911
19/05/02 11:56:36 INFO fs.TestDFSIO:     Test exec time sec: 49.116
19/05/02 11:56:36 INFO fs.TestDFSIO:
```

3. 删除测试生成数据

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean
```

4. 使用Sort程序评测MapReduce

   （1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar randomwriter random-data
```

​	(2) 执行sort程序

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar sort random-data sorted-data
```

​	(3) 验证数据是否真正排好序了

```java
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

### Hadoop参数调优

#### HDFS参数调优hdfs-site.xml

1. dfs.namenode.handler.count

~~~ java
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。
对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
~~~

dfs.namenode.handler.count=20 * loge(Cluster Size)，比如集群规模为8台时，此参数设置为60

> NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数，dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。

2. 编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟

#### YARN参数调优yarn-site.xml

1. 情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

- 解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。

~~~ java
yarn.nodemanager.resource.memory-mb
~~~

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

~~~ java
yarn.scheduler.maximum-allocation-mb
~~~

单个任务可申请的最多物理内存量，默认是8192（MB）。

#### Hadoop宕机

- 如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）

- 如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。

## 第四章，zookeeper

### 集群规划

|           | 服务器hadoop100 | 服务器hadoop101 | 服务器hadoop102 |
| --------- | --------------- | --------------- | --------------- |
| Zookeeper | Zookeeper       | Zookeeper       | Zookeeper       |

- 安装zookeeper请参考另一篇文章
- zookeeper集群启动和停止脚本

```java
#! /bin/bash
case $1 in
"start"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop101 hadoop102 hadoop103
	do
		ssh $i "/opt/module/zookeeper/bin/zkServer.sh status"
	done
};;
esac
//增加执行的权限
chmod 777 zk.sh
```

### 集群日志生成脚本

1）将生成的jar包log-collector-0.0.1-SNAPSHOT-jar-with-dependencies.jar拷贝到hadoop101、服务器上，并同步到hadoop102的/opt/module路径下，

2）在hadoop102上执行jar程序

~~~ java
java -classpath log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.atguigu.appclient.AppMain  >/opt/module/test.log//这是日志生成的路径
//最后可以再此路径下面查看生成的日志
~~~

- 集群日志生成启动脚本

~~~ java
#! /bin/bash

	for i in hadoop101 hadoop102 
	do
		ssh $i "java -classpath /opt/module/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.atguigu.appclient.AppMain $1 $2 >/opt/module/test.log &"
	done
//升级权限
hmod 777 lg.sh
~~~

- 集群时间同步脚本

~~~ jabva
#!/bin/bash

log_date=$1 //获取输入的时间

for i in hadoop101 hadoop102 hadoop103
do
	ssh -t $i "sudo date -s $log_date"
done
~~~

在这里为什么要使用-t参数：需要登录线上的一台目标机器A,但是不能直接登录（没有登录权限），需要先登录B机器，然后从B机器跳转到A机器。需要增加-t -t参数来强制伪终端分配，即使标准输入不是终端

~~~ java
https://www.cnblogs.com/kevingrace/p/6110842.html//博文很详细
~~~

- 集群所有进程查看脚本

~~~ java
#!/bin/bash

for i in hadoop101 hadoop102 hadoop103
do
echo***********$i***********************
	ssh $i "$*"
done
~~~

## 第五章，日志采集之Flume

### 集群规划

|                 | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| --------------- | --------------- | --------------- | --------------- |
| Flume(采集日志) | Flume           | Flume           |                 |

### 项目经验之Flume组件（重要）

**Source**

- Taildir Source相比Exec Source、Spooling Directory Source的优势
  - TailDir Source：断点续传、多目录，继承了Exec Source 和Spooling Directory Source的优点。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。
  - Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。假设source传输一段时间的数据之后，source挂掉，如果重新启动source(相当于执行exec source命令），只会重新传输新追加进来的数据，在source挂掉的那一段时间，还会重新追加数据，这些数据在source重新启动后就会丢失。
  - Spooling Directory Source监控目录，不支持断点续传。实时性比较差，只能把一个文件写完之后放入文件夹内，不可以对文件进行追加操作。

- batchSize大小如何设置？
  - Event 1K左右时，500-1000合适（默认为100）

**Channel**

采用Kafka Channel，省去了Sink，提高了效率。

### 日志采集之Flume配置

![1621323833856](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202105/18/154355-129791.png)

Flume直接读log日志的数据，log日志的格式是app-yyyy-mm-dd.log。

**flume具体配置**

写flume配置文件分为三步骤：

- 给每一个组件命名
- 配置source
- 配置channel
- 配置seek
- 绑定seek和channel和source之间的关系

~~~ java
//（1）在/opt/module/flume/conf目录下创建file-flume-kafka.conf文件

//agent相当于flume中的一个进程，a1是第一个agent的名字
a1.sources=r1 //第一个agent的第一个source命名为r1,可以命名多个source，中间使用空格隔开
a1.channels=c1 //第一个agent的第一个channel命名为c1，也可以有多个channel
  
//配置sources
# configure source
//sources类型,第一个agent的第一个source名字为r1的类型
a1.sources.r1.type = TAILDIR
//用来实现断点续传，实时记录读取到的位置
a1.sources.r1.positionFile =/opt/module/flume/test/log_position.json//记录日志读取位置
//第一个source监控的目录起名字为f1
a1.sources.r1.filegroups = f1
//配置f1的具体的路径
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.+
//配置拦截器
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type=com.rzf.flume.interceptor.LogInterceptor$Builder

//配置channel1
# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop100:9092,hadoop101:9092,hadoop102:9092
a1.channels.c1.kafka.topic = topic_log//日志类型是start,数据发往c1
a1.channels.c1.parseAsFlumeEvent = false

//绑定source和channel以及sink和channel的关系
a1.sources.r1.channels = c1
~~~

### flume的elt拦截器和分类型拦截器

拦截器的功能，json数据格式的校验。

1. 创建Maven工程flume-interceptor
2. 创建包名：com.rzf.flume.interceptor
3. 在pom.xml文件中添加如下配置

~~~ java
<dependencies>
    <dependency>
        <groupId>org.apache.flume</groupId>
        <artifactId>flume-ng-core</artifactId>
        <version>1.9.0</version>
        <scope>provided</scope>
    </dependency>

    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.62</version>
    </dependency>
</dependencies>

<build>
    <plugins>
        <plugin>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>2.3.2</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
        <plugin>
            <artifactId>maven-assembly-plugin</artifactId>
            <configuration>
                <descriptorRefs>
                    <descriptorRef>jar-with-dependencies</descriptorRef>
                </descriptorRefs>
            </configuration>
            <executions>
                <execution>
                    <id>make-assembly</id>
                    <phase>package</phase>
                    <goals>
                        <goal>single</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
~~~

**源码**

~~~ java
在com.atguigu.flume.interceptor包下创建JSONUtils类
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONException;

public class JSONUtils {
    public static boolean isJSONValidate(String log){
        try {
            JSON.parse(log);
            return true;
        }catch (JSONException e){
            return false;
        }
    }
}

在com.atguigu.flume.interceptor包下创建LogInterceptor类
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSON;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }
}
~~~

1. 拦截器打包之后，只需要单独包，不需要将依赖的包上传。打包之后要放入Flume的lib文件夹下面。

   注意：为什么不需要依赖包？因为依赖包在flume的lib目录下面已经存在了。

2. 需要先将打好的包放入到hadoop100的/opt/module/flume/lib文件夹下面

3. 分发Flume到hadoop101、hadoop102

~~~ java
//启动flume，并且作为后台程序
bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf &
~~~

4. 启动kafka消费数据

~~~ java
//任何一台机器上启动kafka消费进程
bin/kafka-console-consumer.sh --bootstrap-server hadoop100:9092 --topic topic_log
//启动flume消费数据
bin/flume-ng agent -n a1 -c conf/ -f conf/file-flume-kafka.conf -Dflume.root.logger=info,console
~~~

### flume启动停止脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop100 hadoop101
do 
echo**********$i启动*****************
    	ssh $i "source /etc/profile;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/dev/null 2>&1 &"
done    	
};;
"stop"){
for i in hadoop100 hadoop101
do 
echo " --------停止 $i 采集flume-------"
    	"source /etc/profile；ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs kill"
done
};;
esac
/*说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。
说明2：/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。
标准输入0：从键盘获得输入 /proc/self/fd/0 
标准输出1：输出到屏幕（即控制台） /proc/self/fd/1 
错误输出2：输出到屏幕（即控制台） /proc/self/fd/2*/
//升级权限
chmod 777 f1.sh
~~~

## 第六章，Kafka

### 集群规划

|       | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ----- | --------------- | --------------- | --------------- |
| Kafka | Kafka           | Kafka           | Kafka           |

[Kafka配置安装]()

### Kafka集群启动停止脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop100 hadoop101 hadoop102
do 
echo"************$i启动********************"
		ssh $i "export JMX_PORT=9988 && /opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties "
};;
"stop"){
for i in hadoop100 hadoop101 hadoop102
do
echo"************kafka停止**************"
    	ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
done
};;
esac
//说明：启动Kafka时要先开启JMX端口，是用于后续KafkaManager监控。
//增加脚本执行权限
chmod 777 kf.sh
//查看kafkatopic列表
bin/kafka-topics.sh --zookeeper hadoop101:2181 --list
~~~

如果启动脚本，相应的进程无法启动，那么修改为如下代码

~~~ java
#! /bin/bash

case $1 in
"start"){
    for i in hadoop100 hadoop101 hadoop102
    do
        echo " --------启动 $i Kafka-------"
        ssh $i " source /etc/profile;nohup /opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in hadoop100 hadoop101 hadoop102
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "s·ource /etc/profile; /opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
~~~

### 创建Kafka的topic 

~~~ java
//创建启动日志主题,副本数是1，topic名字是：topic_start,分区个数也是1
bin/kafka-topics.sh --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181  --create --replication-factor 1 --partitions 1 --topic topic_start

//查看创建的topic
bin/kafka-topics.sh --zookeeper hadoop100:2181/kafka --describe --topic first
~~~

### 创建事件日志主题

~~~ java
//创建主题副本数是1，分区个数是1，主题名字是：topic_event
bin/kafka-topics.sh --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181  --create --replication-factor 1 --partitions 1 --topic topic_event
~~~

### 删除kafka主题操作

~~~ java
bin/kafka-topics.sh --delete --zookeeper hadoop101:2181,hadoop102:2181,hadoop103:2181 --topic topic_start
//这里连接zookeeper任意一台节点都是可以的
~~~

### Kafka生产消息

~~~ java
bin/kafka-console-producer.sh  --broker-list hadoop101:9092 --topic topic_start
>hello world

//写入数据时候是写在每一个topic 的leader上面，但是上面可以写任意的zookeeper节点的地址，写入数据的时候是如何知道leader的具体的位置？
--zookeer的每一个节点都会维护一个topic的元数据信息，根据元数据信息可以很容易的找到每一个topic的leader地址，然后写入数据即可。也就是通过describe查看到的信息。生产者首先去broker中获取元数据的信息，然后根据元数据的信息获取每一个topic的leaser在哪里，下面展示的就是元数据信息

[rzf@hadoop100 kafka]$ bin/kafka-topics.sh --zookeeper hadoop100:2181/kafka --describe --topic first
Topic:first     PartitionCount:3        ReplicationFactor:2     Configs:
        Topic: first    Partition: 0    Leader: 0       Replicas: 0,1   Isr: 0,1
        Topic: first    Partition: 1    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: first    Partition: 2    Leader: 2       Replicas: 2,0   Isr: 2,0
~~~

### Kafka消费消息

~~~ java
bin/kafka-console-consumer.sh \ --zookeeper hadoop101:2181 --from-beginning --topic topic_start
//--from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。
//在这里消费者也需要知道zookeeper的地址，然后去获取每一个topic的元数据信息，然后去leader哪里获取数据
~~~

### 查看Kafka的topic详情

~~~ java
bin/kafka-topics.sh --zookeeper hadoop101:2181 \ --describe --topic topic_start
~~~

### Kafka Manager的安装

- Kafka Manager是yahoo的一个Kafka监控管理项目。

1）下载地址

~~~ java
https://github.com/yahoo/kafka-manager
~~~

下载之后编译源码，编译完成后，拷贝出：kafka-manager-1.3.3.22.zip

2）拷贝kafka-manager-1.3.3.22.zip到hadoop101的/opt/module目录

3）解压kafka-manager-1.3.3.22.zip到/opt/module目录

4）进入到/opt/module/kafka-manager-1.3.3.22/conf目录，在application.conf文件中修改kafka-manager.zkhosts

~~~ java
kafka-manager.zkhosts="hadoop101:2181,hadoop102:2181,hadoop103:2181"
~~~

5) 启动Kafka Manager

~~~ java
nohup bin/kafka-manager   -Dhttp.port=7456 >/opt/module/kafka-manager-1.3.3.22/start.log 2>&1 &
//在浏览器中打开
http://hadoop101:7456
//在Kafka的/opt/module/kafka-manager-1.3.3.22/application.home_IS_UNDEFINED 目录下面，可以看到Kafka-Manager的日志。
~~~

### Kafka Manager启动停止脚本

~~~ java
#! /bin/bash

case $1 in
"start"){
        echo " -------- 启动 KafkaManager -------"
        nohup /opt/module/kafka-manager-1.3.3.22/bin/kafka-manager   -Dhttp.port=7456 >start.log 2>&1 &
};;
"stop"){
        echo " -------- 停止 KafkaManager -------"
        ps -ef | grep ProdServerStart | grep -v grep |awk '{print $2}' | xargs kill 
};;
esac
~~~

### Kafka的压力测试

**Kafka压测**

用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

~~~ java
kafka-consumer-perf-test.sh

kafka-producer-perf-test.sh
~~~

**Kafka Producer压力测试**

- 在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下

~~~ java
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput 1000 --producer-props bootstrap.servers=hadoop101:9092,hadoop102:9092,hadoop103:9092
//说明：record-size是一条信息有多大，单位是字节。num-records是总共发送多少条信息。throughput 是每秒多少条信息。
//消息打印
5000 records sent, 999.4 records/sec (0.10 MB/sec), 1.9 ms avg latency, 254.0 max latency.
5002 records sent, 1000.4 records/sec (0.10 MB/sec), 0.7 ms avg latency, 12.0 max latency.
5001 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 4.0 max latency.
5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.7 ms avg latency, 3.0 max latency.
5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 5.0 max latency.
//参数解析：本例中一共写入10w条消息，每秒向Kafka写入了0.10MB的数据，平均是1000条消息/秒，每次写入的平均延迟为0.8毫秒，最大的延迟为254毫秒。
~~~

说明：

- record-size是一条信息有多大，单位是字节。

- num-records是总共发送多少条信息。

- throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大**吞吐量**。如果throughput设置为具体的数值，那么测试的就是传输数据的延迟。

**Kafka Consumer压力测试**

Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。

~~~ java
bin/kafka-consumer-perf-test.sh --zookeeper hadoop101:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1
~~~

参数说明：

--zookeeper 指定zookeeper的连接地址

--topic 指定topic的名称

--fetch-size 指定每次fetch的数据的大小

--messages 总共要消费的消息个数

~~~ java
测试结果：start.time, **end.time,** data.consumed.in.MB, **MB.sec,** data.consumed.in.nMsg**, nMsg.sec**

2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
~~~

开始测试时间，测试结束数据，最大吞吐率9.5368MB/s，平均每秒消费2.0714MB/s，最大每秒消费100010条，平均每秒消费21722.4153条。

### kafka机器数量计算

**搭建kafka集群需要考虑的问题**

1. 搭建kafka集群需要多少台节点
2. 每个节点上面创建多少个topic ?
3. 每一个topic种分区数量和副本数量设置多少个？

**经验-集群规模数量计算**

Kafka机器数量（经验公式）=2`*`（峰值生产速度`*`副本数/100）+1

副本数量一般设置2到3个合适。

先要预估一天大概产生多少数据，然后用Kafka自带的生产压测（只测试Kafka的写入速度，保证数据不积压），计算出峰值生产速度。再根据设定的副本数，就能预估出需要部署Kafka的数量。

比如我们采用压力测试测出写入的速度是10M/s一台，峰值的业务数据的速度是50M/s。副本数为2。

Kafka机器数量=2（50*2/100）+ 1=3台

**经验-kafka集群分区数量设置**

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量。
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）

例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；

分区数=100 / 20 =5分区

> 分区数一般设置为：3-10个

## 第七章，消费kafka数据的flume

### 集群配置

|                    | 服务器hadoop102 | 服务器hadoop103 | 服务器hadoop104 |
| ------------------ | --------------- | --------------- | --------------- |
| Flume（消费Kafka） |                 |                 | Flume           |

![1623745181313](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202106/15/161942-833545.png)

**配置文件**

~~~ java
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false


a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
~~~

- 启动flume

~~~ java
bin/flume-ng agent -n a1 -c conf/ -f conf/kafka-flume-hdfs.conf -Dflume.root.logger=info,console 
~~~

启停脚本

~~~ java
#!/bin/bash

case $1 in
"start"){
for i in hadoop102 
do 
echo**********$i启动*****************
    	ssh $i "source /etc/profile;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/dev/null 2>&1 &"
done    	
};;
"stop"){
for i in hadoop102
do 
echo " --------停止 $i 采集flume-------"
    	"source /etc/profile；ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs kill"
done
};;
esac
~~~

