
<!-- TOC -->

- [离线数仓项目总结](#离线数仓项目总结)
  - [如何向别人说明你的项目](#如何向别人说明你的项目)
  - [你对数据仓库的理解](#你对数据仓库的理解)
  - [数据仓库为什么要分层](#数据仓库为什么要分层)
  - [如何考虑系统架构的设计或者技术选型](#如何考虑系统架构的设计或者技术选型)
  - [架构设计](#架构设计)
  - [Hadoop项目经验](#hadoop项目经验)
    - [hdfs多目录存储](#hdfs多目录存储)
    - [集群数据均衡](#集群数据均衡)
    - [Hadoop支持LZO压缩配置](#hadoop支持lzo压缩配置)
    - [Hadoop基准测试](#hadoop基准测试)
    - [Hadoop参数调优](#hadoop参数调优)
      - [HDFS参数调优hdfs-site.xml](#hdfs参数调优hdfs-sitexml)
      - [YARN参数调优yarn-site.xml](#yarn参数调优yarn-sitexml)
  - [Kafka](#kafka)
    - [Kafka常用命令](#kafka常用命令)
    - [kafka压力测试](#kafka压力测试)
      - [Kafka Producer压力测试](#kafka-producer压力测试)
      - [Kafka Consumer压力测试](#kafka-consumer压力测试)
    - [Kafka机器数量计算](#kafka机器数量计算)
    - [Kafka分区数计算](#kafka分区数计算)
  - [Flume](#flume)
    - [方案对比](#方案对比)
    - [为什么需要kafka channel?](#为什么需要kafka-channel)
    - [flume组件选型](#flume组件选型)
    - [flume结构](#flume结构)
    - [Flume拦截器](#flume拦截器)
  - [消费kafka的Flume](#消费kafka的flume)
    - [方案选型](#方案选型)
    - [组件选型](#组件选型)
      - [FileChannel和MemoryChannel区别](#filechannel和memorychannel区别)
      - [选型](#选型)
      - [FileChannel优化](#filechannel优化)
      - [Sink](#sink)
    - [数据压缩](#数据压缩)
  - [数据采集小结](#数据采集小结)
  - [业务数据采集平台](#业务数据采集平台)
    - [两个重要概念](#两个重要概念)
    - [表ER模型图](#表er模型图)
    - [使用sqoop导入数据](#使用sqoop导入数据)
      - [sqoop基础操作](#sqoop基础操作)
    - [同步策略](#同步策略)
      - [全量同步](#全量同步)
      - [增量同步](#增量同步)
      - [新增及变化策略](#新增及变化策略)
      - [特殊策略](#特殊策略)

<!-- /TOC -->

## 离线数仓项目总结

### 如何向别人说明你的项目

首先说明项目背景：做的是一个电商项目，后台的数据，使用的是电商系统的**业务数据和日志数据**。

### 你对数据仓库的理解

数据仓库是为企业提供决策支持的数据集合，数据仓库不像我们平时所使用的mysql业务数据库，业务数据库中的数据不是永久存储的，有固定的寿命，但是我们数据仓库中的数据是保存很长时间的，至少保存半年之一年的历史数据，所以有了历史数据，我们就可以多一条分析数据的维度，**时间**，我们可以通过分析随着时间的推移，用户的行为，用户的喜好，等等很多信息，为我们的推广，决策提供支持。

并且通过这些数据的分析，我们可以分析用户画像，报表信息，或者进行机器学习等模型的训练。

### 数据仓库为什么要分层

如果要我一句话说明的话，我会说：复杂的问题简单化，如何理解呢？

![20211212181158](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212181158.png)

**清晰的数据结构：**

什么是清晰的数据结构，简单来说就是每一层的数据都有他自己的作用域，这样我们再使用数据的时候，可以更加方便的去定位。

**数据血缘关系的追踪：**

如果那我们的业务数据来举例，可能我们一张订单详情表中是由很多其他的维度表或者是事实表join得到的，如果我们不进行分层，那么随着数据量增长，我们很难分析表和表之间的关系，所以为了建立表之间的血缘关系，使用分层。

**减少重复开发：**

规范的数据分层，不但可以重用我们的中间数据，还可以减少计算，重用计算。

**屏蔽原始数据的异常**

这个很容易想到，采集到的数据不一定拿来就可以使用，再分层的过程中，我们会主键清洗掉异常的数据，有利于保护我们的数据安全性。

### 如何考虑系统架构的设计或者技术选型

**对于离线数仓**

通常采集日志数据使用flume.

采集业务数据使用sqoop。

使用消息队列kafka作为缓冲的组件。

数据的存储通常用分布式文件系统hdfs，可以存储大容量的数据。

计算引擎，通常使用hadoop或者spark。

**实时数仓**

再实际中，企业通常使用一套采集系统来采集日志数据，中间通过kafaka组件对应离线系统和实时系统。

采集业务数据通常使用Flink cdc组件

计算组件使用Flink或者spark streamming

### 架构设计

![20211212181351](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212181351.png)

### Hadoop项目经验

#### hdfs多目录存储

在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为`file://${hadoop.tmp.dir}/dfs/data`，这个路径可以决定namenode和datanode存储数据的位置，若服务器有多个磁盘，必须对该参数进行修改。参数应修改为如下的值。

```java
<property>
    <name>dfs.datanode.data.dir</name>
  <value>
    file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4
  </value>
</property>
file://代表是一种协议
```
注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。

**使用多目录可以将数据的写操作分散到多个磁盘上，提高吞吐量。**

#### 集群数据均衡

**节点间数据均衡**

节点之间数据不均衡指的是，有的节点空间占用率达到80%，而有的节点空间占用率才30%，所以需要均衡节点之间的数据。

开启数据均衡命令：
```java
start-balancer.sh -threshold 10
```

开启这一个进程，那么默认就会对数据进行跨界点数据的转移，直到数据均衡为止。

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

停止数据均衡命令：`stop-balancer.sh`

停止数据均衡后，已经均衡的数据不会恢复。

**磁盘间数据均衡**

这个是hadoop3.x之后的新特性，在这之前是没有的。

比如新加一块磁盘，那么就需要这个命令，让各个磁盘的数据均衡。

（1）生成均衡计划（我们只有一块磁盘，不会生成计划）

hdfs diskbalancer -plan hadoop103(新加磁盘的节点，这个计划其实就是一个json文件)

（2）执行均衡计划

hdfs diskbalancer -execute hadoop103.plan.json(生成的执行计划)

（3）查看当前均衡任务的执行情况

hdfs diskbalancer -query hadoop103

（4）取消均衡任务

hdfs diskbalancer -cancel hadoop103.plan.json

如果是取消执行计划，那么已经均衡的数据，是不会再恢复的。

#### Hadoop支持LZO压缩配置

1. hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。
2. 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/
3. 同步hadoop-lzo-0.4.20.jar到集群中的所有节点。
4. core-site.xml增加配置支持LZO压缩

```java
<configuration>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```
同步配置文件到集群中其他的节点。

lzo压缩的长处是可以**支持切片**，但是是有条件的，需要建立索引。

建立索引

```java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
```

`com.hadoop.compression.lzo.DistributedLzoIndexer`这个类是lzo中专门用来建立索引的类。

#### Hadoop基准测试

1） 测试HDFS写性能

```java
测试内容：向HDFS集群写10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:             Date & time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:       Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:      Test exec time sec: 67.61

```

2）测试HDFS读性能

```java
测试内容：读取HDFS集群10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB

2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date & time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test exec time sec: 53.61
```
3）删除测试生成数据
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

4）使用Sort程序评测MapReduce

（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
```
（2）执行Sort程序
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
```
（3）验证数据是否真正排好序了
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

> hadoop的基准测试：读，写，删除，性能

#### Hadoop参数调优

##### HDFS参数调优hdfs-site.xml

```
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
```

NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。当Datanode启动之后，会一直和namenode保持心跳，并且发送元数据信息。

对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
```xml
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```
dfs.namenode.handler.count=20×log_e^(Cluster Size)，比如集群规模为8台时，此参数设置为41。

##### YARN参数调优yarn-site.xml

（1）情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

（2）解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，**单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小**。调节这两个参数能提高系统内存的利用率。

（a）yarn.nodemanager.resource.memory-mby

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

（b）yarn.scheduler.maximum-allocation-mb

单个任务可申请的最多物理内存量，默认是8192（MB）。

### Kafka

#### Kafka常用命令

**查看Kafka Topic列表**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
```
**创建Kafka Topic**

进入到/opt/module/kafka/目录下创建日志主题

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka  --create --replication-factor 1 --partitions 1 --topic topic_log
```

**删除Kafka Topic**

```java
bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log
```

**Kafka生产消息**

```java
bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
>hello world
>rzf  rzf
```

**Kafka消费消息**
```java
bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 
--from-beginning 
--topic topic_log
--from-beginning：
```
会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。

**查看Kafka Topic详情**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic topic_log
```

#### kafka压力测试

用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

```java
kafka-consumer-perf-test.sh
kafka-producer-perf-test.sh
```
##### Kafka Producer压力测试

1. 在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下

```java
bin/kafka-producer-perf-test.sh  
--topic test 
--record-size 100 
--num-records 100000 
--throughput -1 
--producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
```
- record-size是一条信息有多大，单位是字节。
- num-records是总共发送多少条信息。
- throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。

> throughput设置为具体的值，测量的因为消息总数确定，所以测出的是延迟，但是如果值为-1，那么测出的是最高的速率。

kafka会打印下面消息

```java
100000 records sent, 95877.277085 records/sec (9.14 MB/sec), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.
```
参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。

##### Kafka Consumer压力测试

Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。

```java
bin/kafka-consumer-perf-test.sh 
--broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 
--topic test 
--fetch-size 10000 
--messages 10000000 
--threads 1
```
参数说明：

--zookeeper 指定zookeeper的链接信息

--topic 指定topic的名称

--fetch-size 指定每次fetch的数据的大小

--messages 总共要消费的消息个数

测试结果说明：
```java
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
```
开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。


#### Kafka机器数量计算

Kafka机器数量（经验公式）=2`*`（峰值生产速度`*`副本数/100）+1

先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。

比如我们的峰值生产速度是50M/s。副本数为2。

Kafka机器数量=2*（50*2/100）+ 1=3台

> 峰值速度一般需要公司提供。

#### Kafka分区数计算

topic一般不需要计算，一个topic一般是一类数据。

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量。
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）


例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；

分区数=100 / 20 =5分区

`https://blog.csdn.net/weixin_42641909/article/details/89294698`

> 分区数一般设置为：3-10个


### Flume

#### 方案对比

本项目中，日志采集结构设计是：

flume--->kafka--->flume

**第一种方案**

第一个flume:
- source:Taildir Source采集日志文件
- channel:kafka channel,将数据直接写入kafka集群中，省掉了sink组件。

第二个flume:
- source:kafka source消费kafka中的数据
- channel:memory channel。
- sink:hdfs sink写入hdfs中。


**第二种方案：**

采用一个flume:

- source:采用Taildir Source采集日志文件。
- channle:采用的是kafka channel,将数据写入kafka集群当中。
- sink:采用hdfs sink方式，将数据写入hdfs中。

> 向kafka channel中写入数据其实是向kafka 集群中写。然后flume的sink在将数据从kafka集群中将数据读出来，写入hdfs中，这个过程需要经过网络的传输。

**第三种方案**

和第一种方案差不多，只不过再第一个flume使用kafka channel将数据写入kafka集群之后，第二个flume直接使用kafka channel消费kafka中的数据，然后将数据写入外部系统即可。此时的kafka channel相当于一个consumer。

> flume source-->kafka 集群---> flume sink---> hdfs

**三种方案对比分析：**

第一种方案简洁，向kafka集群中写入一次数据，然后再读取一次，很方便。

第二种设计结构很不好，压力比较大，首先我们的一个flume需要部署到日志服务器上面。日志服务器需要接收客户端埋点发送过来的日志数据，也就是所有数据先要写入日志服务器上，然后flume还需要读取日志向kafka集群写入一次，这个过程需要经过网络的传输，然后flume sink还需要去kafka集群读取一次数据，也就是数据还要从kafka集群写回flume一次，还要经过网络传输，最后再写入hdfs中，数据是两斤两处，对于日志服务器节点，压力比较大。

而第一种方案，把flume和kafka集群分散开，对日志服务器压力很小。

第三种方案和第一种方案都可以。

#### 为什么需要kafka channel?

在使用flume对接Kafka时，我们往往使用TailFileSource–>MemoryChannel–>KafkaSink的这种方式，然后将数据输送到Kafka集群中。

![20211212184253](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212184253.png)

但是这种方式有弊端：

1. TailFileSource只能监听一个文件
2. MemoryChannel数据会有堆积，内存可能溢出（而FileChannel又比较慢）
3. 这种方式经历多个组件，效率变低，出现问题的概率也变大。

**新的思路：**

使用TailFileSource–>KafkaChannel这种方式，将KafkaChannel作为缓冲，效率变高，而且数据不会丢失。

![20211212184226](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212184226.png)

这种方法就是使用TailFileSource来读取日志文件，然后将数据输送到KafkaChannel种，然后KafkaChannel直接将数据输送到Kafka集群中，此时不需要Sink，KafkaChannel相当于Kafka的生产者，这样就充分利用了Kafka集群的优点，当数据量很大的时候，也能hold得住。

如果要将数据写入到HDFS或者ES中，要再创建一个flume集群，这个flume中只要有KafkaChannel和HDFSSink就可以了，此时的KafkaChannel相当于Kafka的消费者。但是要注意，为了避免多个flume消费同样的数据，要将多个flume实例放在同一个组内。


#### flume组件选型

**Source**

1. Taildir Source相比Exec Source、Spooling Directory Source的优势,这几个都是采集文件的source.


TailDir Source：**断点续传、保证数据不丢失，还可以监控数据**。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传，继承了Exec Source实时传输数据和Spooling Directory Source断点续传的优点。

![20211212185046](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212185046.png)

Exec Source：**可以实时监控数据，并且向文件中追加数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失**，也就是相当于重新执行了命令，但是再脚本挂掉期间，产生的数据会丢失。可以执行一个脚本，去事实的监控一个数据文件。

Spooling Directory Source：能够保证数据的不丢失，支持断点续传。原理是将写好的文件放到这个监控目录中，然后可以将文件中的内容一次性收集完。但是缺点是只要这个文件放到这个监控目录中之后，就不能修改文件的名字和追加内容，这是其局限性，所以不能事实的采集数据，所以再事实性监控，我们使用dlik cdc。

如果真要使用Spooling Directory Source实时监控数据，那么只能让文件小一点，比如几秒钟文东一个文件，尽可能的去模拟实时的场景，但是延迟高。

2. batchSize大小如何设置？


答：Event 1K左右时，500-1000合适（默认为100），也即是向channel中放数据的时候，一次性放多少个。

> 这里的批指的是什么，再put事务的时候，会首先将数据存储再一个putlist中，当批次满的时候，会提交一个事务，这个就是批次的大小。

**Channel**

采用Kafka Channel，直接将数据写入kafka集群，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

> bug：注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。

#### flume结构

![20211212130648](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212130648.png)


#### Flume拦截器

flume拦截器属于flume的source中一个组件。使用拦截器，可以做日志的过滤或者分类。在这里做日志的清晰，清除不合法的日志。

1. 自定义类实现Interceptor 接口。
2. 实现initialize()初始化方法。
3. 实现Event intercept(Event event)拦截方法，判断一个事件是否是合法的。
4. 实现List<Event> intercept(List<Event> list)方法，这个方法是对上面第三个方法的封装，

在这里，对字符串的判断，使用的是阿里巴巴的fastJson对json字符串进行校验。

在这里创建拦截器对象使用的是builder创建者模式，自定义创建者实现Interceptor.Builder类，重写下面方法：

1. Interceptor build()创建一个拦截器对象。
2. void configure(Context context)配置信息。

### 消费kafka的Flume

#### 方案选型

消费kafka中数据的Flume有两种配置方案：
1. 配置一个完整的flume，包括kafka source，channel和hdfs sink。
2. 配置一个kafka channel省去了source组件，然后再配置一个hdfs sink，这两种方案都可以。

但是这里也存在问题，flume sink将数据写入hdfs的时候，如何去写，因为我们离线数仓是一种批处理，那么这个一批指的是一天的数据，一次性需要计算一天的数据，所以再这里存储数据的时候，最好是按照分天进行存储，所以路径应该以天为单位。

但是实现这种方案，需要我们对event添加一个时间戳，由于flume默认会用linux系统时间（如果和flink类比的话，应该是摄入事件），作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间（也就是事件事件），发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间。

在这里我们依然使用一个拦截器解决：

拦截器属于source端的组件，所以再上面的两种方案中，我们就只能使用第一种方案了。自定义拦截器，提取日志中的事件事件，然后把事件事件添加到event的header中，根据header中的事件时间创建文件夹写出数据。

在这里我么也可以使用第二种方案，我们把提取时间戳的拦截器放到第一个flume中，形成一个拦截器链，这样做的话，再写入kafka的时候，我们就需要保留evevt的头部信息，不能过滤掉。

自定义拦截器步骤请参考上文。

#### 组件选型

采用kafka source，再底层就是一个kafka的消费者，

##### FileChannel和MemoryChannel区别

MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。

FileChannel传输速度相对于Memory慢，但数据安全保障高,Agent进程挂掉也可以从失败中恢复数据。

为什么FileChannel不会发生数据的丢失：

![20211212140412](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212140412.png)

![20211212185801](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212185801.png)

可以看到底层的数据结构，真实的数据存储再磁盘文件中，但是还是有一个内存队列的，这个内存队列相当于是对磁盘上的文件建立一个索引，记录了消费到哪里。

虽然磁盘上的数据不会丢失，但是内存队列中的数据断电就会丢失，所以再内存队列之外，还有一个快照文件，这个快照文件可以看作是内存队列的快照，只要内存队列发生变化，这个快照文件就会跟着变化。这样就保证数据的安全性。

如果消费者挂点，那么就可以兄快照恢复，然后恢复读取的位置，重新消费。

##### 选型

金融类公司、对钱要求非常准确的公司通常会选择FileChannel
传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。

##### FileChannel优化

通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。

checkpointDir(做快照的文件路径，保存内存索引)和backupCheckpointDir（检查点备份文件路径）也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据

批量指的是再将数据写入putlist事务列表的时候，事务列表的大小，再提交事务的时候，put事务会先检查一些channel是否有足够的空间存放数据，如果没有的话，就会立刻回滚数据，然后source会清空putlist列表，再次去拉去数据写入putlist中，但是如果立刻回滚事务，很消耗性能，所以就产生了keep_alive参数，等待几秒钟，再次提交事务，如果还没有成功，就进行回滚。

##### Sink

HDFS Sink
1. HDFS存入大量小文件，有什么影响？

元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命

计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。

2. HDFS小文件处理

官方默认的这三个参数配置写入HDFS后会产生小文件，
`hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount`

基于以上：

hdfs.rollInterval=3600：1小时生成一个新文件

hdfs.rollSize=134217728：128m生成一个新文件

hdfs.rollCount =0：多少个event生成一个新文件

几个参数综合作用，效果如下：
1. 文件在达到128M时会滚动生成新文件
2. 文件创建超3600秒时会滚动生成新文件

#### 数据压缩

再写入hdfs上的文件，我么也可以进行压缩处理，使用lzo压缩。

flume目前支持三种：

二进制文件：SequenceFile。

DataStream:不做任何处理。

Compress:压缩文件，目前支持：gzip,bzip2,lzo,snappy。

### 数据采集小结

web/app---> 日志服务器--->Flume--->kafka--->flume--->hdfs

再实际中，日志服务器可能部署再大数据集群中，也可能不在。

再第一个Flume中，有三种实现方案。

第二个flume中，有两种实现方案。

> 思考为什么中间需要一个kafka集群？

### 业务数据采集平台

业务数据是使用sqoop将数据导入到hdfs上面，对于业务数据，重要的是数据业务表之间的联系。

#### 两个重要概念

电商常识（SKU、SPU）

SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。

SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。

例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。

SPU表示一类商品。好处就是：可以共用商品图片，海报、销售属性等。

spu描述的粒度更粗，sku描述一件具体商品，粒度细。

#### 表ER模型图

本项目中设计24张表结构。

![20211212144449](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212144449.png)

拿到表的脚本建立表之后，使用EZDML工具建立各个表之间的联系。

#### 使用sqoop导入数据

sqoop可以实现双向传输，也就是从sqoop到mysql和从mysql到sqoop的双向传输。

sqoop的底层就是mapreduce任务，所以延迟很高，底层的mr任务只有map任务，没有reduce任务，这个可以从sqoop功能来看，sqoop的定位只是用来传输数据，不用来分析数据，所以只需要使用map将数据读出来然后写入hdfs即可。

那么sqoop做的工作就是自定义inoutFormat和outputFormat组件。自定义读取数据可以从数据库中读取。

##### sqoop基础操作

**连接mysql**

```java
bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 000000
```
再导入的时候，sqoop支持将mysql数据库中的一张表导入到hdfs中的一个路径，或者是一张表或者是hbase中的一张表，但是到处的时候，只支持将hdfs上一个路径下的文件导出到mysql中的一张表中。

**导入数据**

```java
bin/sqoop import --connect jdbc:mysql://hadoop102:3306/gmall 
--user root 
--password root
--table user_info  //全量表
--columns id,login_name//导入具体的列
--where "id >=10 and id<=30" //导入数据的过滤条件
--target-dir /test // 上传文件的路径
--delete-target-dir //如果目标路径存在就删除，这个参数可以保证数据的幂等性，因为如果任务失败，再次导入的话，会删除文件夹重新上传数据
//优化相关参数
--num-mappers 2//指的是到数据的时候map任务个数
--fields-terminated-by //字段之间分隔符
--splits-by id //按照哪一个字段进行切片
```

关于这里的切片机制，自定义的inoutFormat会根据我们输入的配置进行切片，也就是根据我们输入的--num-mappers参数，将数据根据id进行切片，然后交给map处理。

**使用sql方式导入数据**

```java
bin/sqoop import --connect jdbc:mysql://hadoop102:3306/gmall 
--user root 
--password root
--query "select id,login_name from user_info where id >=10 and id<=30"
--target-dir /test // 上传文件的路径
--delete-target-dir //如果目标路径存在就删除，这个参数可以保证数据的幂等性，因为如果任务失败，再次导入的话，会删除文件夹重新上传数据
//优化相关参数
--num-mappers 2//指的是到数据的时候map任务个数
--fields-terminated-by //字段之间分隔符
--splits-by id //按照哪一个字段进行切片
```

#### 同步策略

数据同步策略的类型包括：**全量同步、增量同步、新增及变化同步、特殊情况**

- 全量表：存储完整的数据。
- 增量表：存储新增加的数据。
- 新增及变化表：存储新增加的数据和变化的数据。
- 特殊表：只需要存储一次。

##### 全量同步

每日全量，就是每天存储一份完整数据，作为一个分区，所以说全量表也是一个分区表，里面存储的是每天从mysql中导出来的全部数据。

**适用于表数据量不大，且每天既会有新数据插入，也会有旧数据的修改的场景。**

例如：编码字典表、品牌表、商品三级分类、商品二级分类、商品一级分类、优惠规则表、活动表、活动参与商品表、加购表、商品收藏表、优惠卷表、SKU商品表、SPU商品表

![20211212154656](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212154656.png)

##### 增量同步

每日增量，就是每天存储一份**增量数据**，作为一个分区，增量数据通常存储再**增量表**中，可以按照天使用一个**分区存储**。

增量同步适用于mysql中每天只会新增数据，不会发生修改的表，比流水表，支付流水，订单状态流水表。

适用于表数据量大，且每天只会有新数据插入的场景。例如：退单表、订单状态表、支付流水表、订单详情表、活动与订单关联表、商品评论表。

![20211212154853](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212154853.png)

##### 新增及变化策略

每日新增及变化，就是存储**创建时间和操作时间**都是今天的数据，这里使用的不是分区表。

查询数据主要有两种，获取最新数据，获取历史上某一天数据，但是再新增及变化表中查询上面两种数据很不方便。

**适用场景为，表的数据量大，既会有新增，又会有变化。例如：用户表、订单表、优惠卷领用表。**

![20211212155015](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212155015.png)

##### 特殊策略

某些特殊的维度表，可不必遵循上述同步策略。

1）客观世界维度

没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。

2）日期维度

日期维度可以一次性导入一年或若干年的数据。

