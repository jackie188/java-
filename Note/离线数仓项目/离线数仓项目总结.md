
<!-- TOC -->

- [离线数仓项目总结](#离线数仓项目总结)
  - [如何向别人说明你的项目](#如何向别人说明你的项目)
  - [你对数据仓库的理解](#你对数据仓库的理解)
  - [数据仓库为什么要分层](#数据仓库为什么要分层)
  - [如何考虑系统架构的设计或者技术选型](#如何考虑系统架构的设计或者技术选型)
  - [架构设计](#架构设计)
  - [Hadoop项目经验](#hadoop项目经验)
    - [hdfs多目录存储](#hdfs多目录存储)
    - [集群数据均衡](#集群数据均衡)
    - [Hadoop支持LZO压缩配置](#hadoop支持lzo压缩配置)
    - [Hadoop基准测试](#hadoop基准测试)
    - [Hadoop参数调优](#hadoop参数调优)
      - [HDFS参数调优hdfs-site.xml](#hdfs参数调优hdfs-sitexml)
      - [YARN参数调优yarn-site.xml](#yarn参数调优yarn-sitexml)
  - [Kafka](#kafka)
    - [Kafka常用命令](#kafka常用命令)
    - [kafka压力测试](#kafka压力测试)
      - [Kafka Producer压力测试](#kafka-producer压力测试)
      - [Kafka Consumer压力测试](#kafka-consumer压力测试)
    - [Kafka机器数量计算](#kafka机器数量计算)
    - [Kafka分区数计算](#kafka分区数计算)
  - [Flume](#flume)
    - [方案对比](#方案对比)
    - [为什么需要kafka channel?](#为什么需要kafka-channel)
    - [flume组件选型](#flume组件选型)
    - [flume结构](#flume结构)
    - [Flume拦截器](#flume拦截器)
  - [消费kafka的Flume](#消费kafka的flume)
    - [方案选型](#方案选型)
    - [组件选型](#组件选型)
      - [FileChannel和MemoryChannel区别](#filechannel和memorychannel区别)
      - [选型](#选型)
      - [FileChannel优化](#filechannel优化)
      - [Sink](#sink)
    - [数据压缩](#数据压缩)
  - [数据采集小结](#数据采集小结)
  - [业务数据采集平台](#业务数据采集平台)
    - [两个重要概念](#两个重要概念)
    - [表ER模型图](#表er模型图)
    - [使用sqoop导入数据](#使用sqoop导入数据)
      - [sqoop基础操作](#sqoop基础操作)
    - [同步策略](#同步策略)
      - [全量同步](#全量同步)
      - [增量同步](#增量同步)
      - [新增及变化策略](#新增及变化策略)
      - [特殊策略](#特殊策略)
      - [本项目中表的导入策略](#本项目中表的导入策略)
      - [sqoop导入数据](#sqoop导入数据)
  - [数仓理论](#数仓理论)
    - [离线数仓分层](#离线数仓分层)
    - [数据集市和数据仓库区别](#数据集市和数据仓库区别)
    - [范式理论](#范式理论)
      - [第一范式：](#第一范式)
      - [第二范式](#第二范式)
      - [第三范式](#第三范式)
    - [关系建模与维度建模](#关系建模与维度建模)
      - [关系建模](#关系建模)
      - [维度建模](#维度建模)
    - [事实表和维度表](#事实表和维度表)
      - [维度表](#维度表)
      - [事实表](#事实表)
    - [维度模型分类](#维度模型分类)
  - [数据仓库建模](#数据仓库建模)
    - [ODS层](#ods层)
    - [DWD层](#dwd层)
    - [DWS层与DWT层](#dws层与dwt层)
    - [ADS层](#ads层)
  - [Hive计算引擎](#hive计算引擎)
  - [配置yarn容量调度器多队列](#配置yarn容量调度器多队列)
  - [搭建ODS层数据仓库](#搭建ods层数据仓库)
    - [日志数据](#日志数据)
    - [业务数据](#业务数据)
    - [ods层日志数据和业务数据](#ods层日志数据和业务数据)
  - [搭建DWD层数据](#搭建dwd层数据)
    - [DWD层的任务](#dwd层的任务)
    - [日志数据的解析](#日志数据的解析)
      - [启动日志表](#启动日志表)
      - [页面日志表](#页面日志表)
      - [动作日志表](#动作日志表)
      - [曝光日志表](#曝光日志表)
      - [错误日志表](#错误日志表)
    - [业务数据的解析](#业务数据的解析)
      - [商品维度表（全量）](#商品维度表全量)
      - [优惠券维度表（全量）](#优惠券维度表全量)
      - [活动维度表（全量）](#活动维度表全量)
      - [地区维度表（特殊）](#地区维度表特殊)
      - [时间维度表（特殊）](#时间维度表特殊)
      - [支付事实表（事务性事实表）](#支付事实表事务性事实表)
      - [退款事实表（事务性事实表）](#退款事实表事务性事实表)
      - [评价事实表(事务性事实表)](#评价事实表事务性事实表)
      - [订单明细事实表（事务型事实表）](#订单明细事实表事务型事实表)
      - [加购事实表（周期型快照事实表，每日快照）](#加购事实表周期型快照事实表每日快照)
      - [收藏事实表（周期型快照事实表，每日快照）](#收藏事实表周期型快照事实表每日快照)
      - [优惠券领用事实表（累积型快照事实表）](#优惠券领用事实表累积型快照事实表)
      - [订单事实表](#订单事实表)
      - [用户维度表（拉链表）](#用户维度表拉链表)
  - [DWS层](#dws层)
    - [业务术语](#业务术语)
    - [每日设备行为](#每日设备行为)
    - [每日会员行为](#每日会员行为)
    - [每日商品行为](#每日商品行为)
    - [每日活动行为](#每日活动行为)
  - [DWT层宽表数据](#dwt层宽表数据)
    - [设备主题宽表](#设备主题宽表)
    - [会员主题宽表](#会员主题宽表)
    - [商品主题宽表](#商品主题宽表)
    - [活动主题宽表](#活动主题宽表)

<!-- /TOC -->

## 离线数仓项目总结

### 如何向别人说明你的项目

首先说明项目背景：做的是一个电商项目，后台的数据，使用的是电商系统的**业务数据和日志数据**。

### 你对数据仓库的理解

数据仓库是为企业提供决策支持的数据集合，数据仓库不像我们平时所使用的mysql业务数据库，业务数据库中的数据不是永久存储的，有固定的寿命，但是我们数据仓库中的数据是保存很长时间的，至少保存半年之一年的历史数据，所以有了历史数据，我们就可以多一条分析数据的维度，**时间**，我们可以通过分析随着时间的推移，用户的行为，用户的喜好，等等很多信息，为我们的推广，决策提供支持。

并且通过这些数据的分析，我们可以分析用户画像，报表信息，或者进行机器学习等模型的训练。

### 数据仓库为什么要分层

如果要我一句话说明的话，我会说：复杂的问题简单化，如何理解呢？

![20211212181158](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212181158.png)

**清晰的数据结构：**

什么是清晰的数据结构，简单来说就是每一层的数据都有他自己的作用域，这样我们再使用数据的时候，可以更加方便的去定位。

**数据血缘关系的追踪：**

如果那我们的业务数据来举例，可能我们一张订单详情表中是由很多其他的维度表或者是事实表join得到的，如果我们不进行分层，那么随着数据量增长，我们很难分析表和表之间的关系，所以为了建立表之间的血缘关系，使用分层。

**减少重复开发：**

规范的数据分层，不但可以重用我们的中间数据，还可以减少计算，重用计算。

**屏蔽原始数据的异常**

这个很容易想到，采集到的数据不一定拿来就可以使用，再分层的过程中，我们会主键清洗掉异常的数据，有利于保护我们的数据安全性。

### 如何考虑系统架构的设计或者技术选型

**对于离线数仓**

通常采集日志数据使用flume.

采集业务数据使用sqoop。

使用消息队列kafka作为缓冲的组件。

数据的存储通常用分布式文件系统hdfs，可以存储大容量的数据。

计算引擎，通常使用hadoop或者spark。

**实时数仓**

再实际中，企业通常使用一套采集系统来采集日志数据，中间通过kafaka组件对应离线系统和实时系统。

采集业务数据通常使用Flink cdc组件

计算组件使用Flink或者spark streamming

### 架构设计

![20211212181351](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212181351.png)

### Hadoop项目经验

#### hdfs多目录存储

在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为`file://${hadoop.tmp.dir}/dfs/data`，这个路径可以决定namenode和datanode存储数据的位置，若服务器有多个磁盘，必须对该参数进行修改。参数应修改为如下的值。

```java
<property>
    <name>dfs.datanode.data.dir</name>
  <value>
    file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4
  </value>
</property>
file://代表是一种协议
```
注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。

**使用多目录可以将数据的写操作分散到多个磁盘上，提高吞吐量。**

#### 集群数据均衡

**节点间数据均衡**

节点之间数据不均衡指的是，有的节点空间占用率达到80%，而有的节点空间占用率才30%，所以需要均衡节点之间的数据。

开启数据均衡命令：
```java
start-balancer.sh -threshold 10
```

开启这一个进程，那么默认就会对数据进行跨界点数据的转移，直到数据均衡为止。

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

停止数据均衡命令：`stop-balancer.sh`

停止数据均衡后，已经均衡的数据不会恢复。

**磁盘间数据均衡**

这个是hadoop3.x之后的新特性，在这之前是没有的。

比如新加一块磁盘，那么就需要这个命令，让各个磁盘的数据均衡。

（1）生成均衡计划（我们只有一块磁盘，不会生成计划）

hdfs diskbalancer -plan hadoop103(新加磁盘的节点，这个计划其实就是一个json文件)

（2）执行均衡计划

hdfs diskbalancer -execute hadoop103.plan.json(生成的执行计划)

（3）查看当前均衡任务的执行情况

hdfs diskbalancer -query hadoop103

（4）取消均衡任务

hdfs diskbalancer -cancel hadoop103.plan.json

如果是取消执行计划，那么已经均衡的数据，是不会再恢复的。

#### Hadoop支持LZO压缩配置

1. hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。
2. 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/
3. 同步hadoop-lzo-0.4.20.jar到集群中的所有节点。
4. core-site.xml增加配置支持LZO压缩

```java
<configuration>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```
同步配置文件到集群中其他的节点。

lzo压缩的长处是可以**支持切片**，但是是有条件的，需要建立索引。

建立索引

```java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
```

`com.hadoop.compression.lzo.DistributedLzoIndexer`这个类是lzo中专门用来建立索引的类。

#### Hadoop基准测试

1） 测试HDFS写性能

```java
测试内容：向HDFS集群写10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:             Date & time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:       Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:      Test exec time sec: 67.61

```

2）测试HDFS读性能

```java
测试内容：读取HDFS集群10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB

2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date & time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test exec time sec: 53.61
```
3）删除测试生成数据
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

4）使用Sort程序评测MapReduce

（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
```
（2）执行Sort程序
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
```
（3）验证数据是否真正排好序了
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

> hadoop的基准测试：读，写，删除，性能

#### Hadoop参数调优

##### HDFS参数调优hdfs-site.xml

```
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
```

NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。当Datanode启动之后，会一直和namenode保持心跳，并且发送元数据信息。

对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
```xml
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```
dfs.namenode.handler.count=20×log_e^(Cluster Size)，比如集群规模为8台时，此参数设置为41。

##### YARN参数调优yarn-site.xml

（1）情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

（2）解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，**单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小**。调节这两个参数能提高系统内存的利用率。

（a）yarn.nodemanager.resource.memory-mby

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

（b）yarn.scheduler.maximum-allocation-mb

单个任务可申请的最多物理内存量，默认是8192（MB）。

### Kafka

#### Kafka常用命令

**查看Kafka Topic列表**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
```
**创建Kafka Topic**

进入到/opt/module/kafka/目录下创建日志主题

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka  --create --replication-factor 1 --partitions 1 --topic topic_log
```

**删除Kafka Topic**

```java
bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log
```

**Kafka生产消息**

```java
bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
>hello world
>rzf  rzf
```

**Kafka消费消息**
```java
bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 
--from-beginning 
--topic topic_log
--from-beginning：
```
会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。

**查看Kafka Topic详情**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic topic_log
```

#### kafka压力测试

用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

```java
kafka-consumer-perf-test.sh
kafka-producer-perf-test.sh
```
##### Kafka Producer压力测试

1. 在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下

```java
bin/kafka-producer-perf-test.sh  
--topic test 
--record-size 100 
--num-records 100000 
--throughput -1 
--producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
```
- record-size是一条信息有多大，单位是字节。
- num-records是总共发送多少条信息。
- throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。

> throughput设置为具体的值，测量的因为消息总数确定，所以测出的是延迟，但是如果值为-1，那么测出的是最高的速率。

kafka会打印下面消息

```java
100000 records sent, 95877.277085 records/sec (9.14 MB/sec), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.
```
参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。

##### Kafka Consumer压力测试

Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。

```java
bin/kafka-consumer-perf-test.sh 
--broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 
--topic test 
--fetch-size 10000 
--messages 10000000 
--threads 1
```
参数说明：

--zookeeper 指定zookeeper的链接信息

--topic 指定topic的名称

--fetch-size 指定每次fetch的数据的大小

--messages 总共要消费的消息个数

测试结果说明：
```java
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
```
开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。


#### Kafka机器数量计算

Kafka机器数量（经验公式）=2`*`（峰值生产速度`*`副本数/100）+1

先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。

比如我们的峰值生产速度是50M/s。副本数为2。

Kafka机器数量=2*（50*2/100）+ 1=3台

> 峰值速度一般需要公司提供。

#### Kafka分区数计算

topic一般不需要计算，一个topic一般是一类数据。

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量。
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）


例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；

分区数=100 / 20 =5分区

`https://blog.csdn.net/weixin_42641909/article/details/89294698`

> 分区数一般设置为：3-10个


### Flume

#### 方案对比

本项目中，日志采集结构设计是：

flume--->kafka--->flume

**第一种方案**

第一个flume:
- source:Taildir Source采集日志文件
- channel:kafka channel,将数据直接写入kafka集群中，省掉了sink组件。

第二个flume:
- source:kafka source消费kafka中的数据
- channel:memory channel。
- sink:hdfs sink写入hdfs中。


**第二种方案：**

采用一个flume:

- source:采用Taildir Source采集日志文件。
- channle:采用的是kafka channel,将数据写入kafka集群当中。
- sink:采用hdfs sink方式，将数据写入hdfs中。

> 向kafka channel中写入数据其实是向kafka 集群中写。然后flume的sink在将数据从kafka集群中将数据读出来，写入hdfs中，这个过程需要经过网络的传输。

**第三种方案**

和第一种方案差不多，只不过再第一个flume使用kafka channel将数据写入kafka集群之后，第二个flume直接使用kafka channel消费kafka中的数据，然后将数据写入外部系统即可。此时的kafka channel相当于一个consumer。

> flume source-->kafka 集群---> flume sink---> hdfs

**三种方案对比分析：**

第一种方案简洁，向kafka集群中写入一次数据，然后再读取一次，很方便。

第二种设计结构很不好，压力比较大，首先我们的一个flume需要部署到日志服务器上面。日志服务器需要接收客户端埋点发送过来的日志数据，也就是所有数据先要写入日志服务器上，然后flume还需要读取日志向kafka集群写入一次，这个过程需要经过网络的传输，然后flume sink还需要去kafka集群读取一次数据，也就是数据还要从kafka集群写回flume一次，还要经过网络传输，最后再写入hdfs中，数据是两斤两处，对于日志服务器节点，压力比较大。

而第一种方案，把flume和kafka集群分散开，对日志服务器压力很小。

第三种方案和第一种方案都可以。

#### 为什么需要kafka channel?

在使用flume对接Kafka时，我们往往使用TailFileSource–>MemoryChannel–>KafkaSink的这种方式，然后将数据输送到Kafka集群中。

![20211212184253](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212184253.png)

但是这种方式有弊端：

1. TailFileSource只能监听一个文件
2. MemoryChannel数据会有堆积，内存可能溢出（而FileChannel又比较慢）
3. 这种方式经历多个组件，效率变低，出现问题的概率也变大。

**新的思路：**

使用TailFileSource–>KafkaChannel这种方式，将KafkaChannel作为缓冲，效率变高，而且数据不会丢失。

![20211212184226](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212184226.png)

这种方法就是使用TailFileSource来读取日志文件，然后将数据输送到KafkaChannel种，然后KafkaChannel直接将数据输送到Kafka集群中，此时不需要Sink，KafkaChannel相当于Kafka的生产者，这样就充分利用了Kafka集群的优点，当数据量很大的时候，也能hold得住。

如果要将数据写入到HDFS或者ES中，要再创建一个flume集群，这个flume中只要有KafkaChannel和HDFSSink就可以了，此时的KafkaChannel相当于Kafka的消费者。但是要注意，为了避免多个flume消费同样的数据，要将多个flume实例放在同一个组内。


#### flume组件选型

**Source**

1. Taildir Source相比Exec Source、Spooling Directory Source的优势,这几个都是采集文件的source.


TailDir Source：**断点续传、保证数据不丢失，还可以监控数据**。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传，继承了Exec Source实时传输数据和Spooling Directory Source断点续传的优点。

![20211212185046](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212185046.png)

Exec Source：**可以实时监控数据，并且向文件中追加数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失**，也就是相当于重新执行了命令，但是再脚本挂掉期间，产生的数据会丢失。可以执行一个脚本，去事实的监控一个数据文件。

Spooling Directory Source：能够保证数据的不丢失，支持断点续传。原理是将写好的文件放到这个监控目录中，然后可以将文件中的内容一次性收集完。但是缺点是只要这个文件放到这个监控目录中之后，就不能修改文件的名字和追加内容，这是其局限性，所以不能事实的采集数据，所以再事实性监控，我们使用dlik cdc。

如果真要使用Spooling Directory Source实时监控数据，那么只能让文件小一点，比如几秒钟文东一个文件，尽可能的去模拟实时的场景，但是延迟高。

2. batchSize大小如何设置？


答：Event 1K左右时，500-1000合适（默认为100），也即是向channel中放数据的时候，一次性放多少个。

> 这里的批指的是什么，再put事务的时候，会首先将数据存储再一个putlist中，当批次满的时候，会提交一个事务，这个就是批次的大小。

**Channel**

采用Kafka Channel，直接将数据写入kafka集群，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

> bug：注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。

#### flume结构

![20211212130648](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212130648.png)


#### Flume拦截器

flume拦截器属于flume的source中一个组件。使用拦截器，可以做日志的过滤或者分类。在这里做日志的清晰，清除不合法的日志。

1. 自定义类实现Interceptor 接口。
2. 实现initialize()初始化方法。
3. 实现Event intercept(Event event)拦截方法，判断一个事件是否是合法的。
4. 实现List<Event> intercept(List<Event> list)方法，这个方法是对上面第三个方法的封装，

在这里，对字符串的判断，使用的是阿里巴巴的fastJson对json字符串进行校验。

在这里创建拦截器对象使用的是builder创建者模式，自定义创建者实现Interceptor.Builder类，重写下面方法：

1. Interceptor build()创建一个拦截器对象。
2. void configure(Context context)配置信息。

### 消费kafka的Flume

#### 方案选型

消费kafka中数据的Flume有两种配置方案：
1. 配置一个完整的flume，包括kafka source，channel和hdfs sink。
2. 配置一个kafka channel省去了source组件，然后再配置一个hdfs sink，这两种方案都可以。

但是这里也存在问题，flume sink将数据写入hdfs的时候，如何去写，因为我们离线数仓是一种批处理，那么这个一批指的是一天的数据，一次性需要计算一天的数据，所以再这里存储数据的时候，最好是按照分天进行存储，所以路径应该以天为单位。

但是实现这种方案，需要我们对event添加一个时间戳，由于flume默认会用linux系统时间（如果和flink类比的话，应该是摄入事件），作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间（也就是事件事件），发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间。

在这里我们依然使用一个拦截器解决：

拦截器属于source端的组件，所以再上面的两种方案中，我们就只能使用第一种方案了。自定义拦截器，提取日志中的事件事件，然后把事件事件添加到event的header中，根据header中的事件时间创建文件夹写出数据。

在这里我么也可以使用第二种方案，我们把提取时间戳的拦截器放到第一个flume中，形成一个拦截器链，这样做的话，再写入kafka的时候，我们就需要保留evevt的头部信息，不能过滤掉。

自定义拦截器步骤请参考上文。

#### 组件选型

采用kafka source，再底层就是一个kafka的消费者，

##### FileChannel和MemoryChannel区别

MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。

FileChannel传输速度相对于Memory慢，但数据安全保障高,Agent进程挂掉也可以从失败中恢复数据。

为什么FileChannel不会发生数据的丢失：

![20211212140412](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212140412.png)

![20211212185801](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212185801.png)

可以看到底层的数据结构，真实的数据存储再磁盘文件中，但是还是有一个内存队列的，这个内存队列相当于是对磁盘上的文件建立一个索引，记录了消费到哪里。

虽然磁盘上的数据不会丢失，但是内存队列中的数据断电就会丢失，所以再内存队列之外，还有一个快照文件，这个快照文件可以看作是内存队列的快照，只要内存队列发生变化，这个快照文件就会跟着变化。这样就保证数据的安全性。

如果消费者挂点，那么就可以兄快照恢复，然后恢复读取的位置，重新消费。

##### 选型

金融类公司、对钱要求非常准确的公司通常会选择FileChannel
传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。

##### FileChannel优化

通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。

checkpointDir(做快照的文件路径，保存内存索引)和backupCheckpointDir（检查点备份文件路径）也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据

批量指的是再将数据写入putlist事务列表的时候，事务列表的大小，再提交事务的时候，put事务会先检查一些channel是否有足够的空间存放数据，如果没有的话，就会立刻回滚数据，然后source会清空putlist列表，再次去拉去数据写入putlist中，但是如果立刻回滚事务，很消耗性能，所以就产生了keep_alive参数，等待几秒钟，再次提交事务，如果还没有成功，就进行回滚。

##### Sink

HDFS Sink
1. HDFS存入大量小文件，有什么影响？

元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命

计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。

2. HDFS小文件处理

官方默认的这三个参数配置写入HDFS后会产生小文件，
`hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount`

基于以上：

hdfs.rollInterval=3600：1小时生成一个新文件

hdfs.rollSize=134217728：128m生成一个新文件

hdfs.rollCount =0：多少个event生成一个新文件

几个参数综合作用，效果如下：
1. 文件在达到128M时会滚动生成新文件
2. 文件创建超3600秒时会滚动生成新文件

#### 数据压缩

再写入hdfs上的文件，我么也可以进行压缩处理，使用lzo压缩。

flume目前支持三种：

二进制文件：SequenceFile。

DataStream:不做任何处理。

Compress:压缩文件，目前支持：gzip,bzip2,lzo,snappy。

### 数据采集小结

web/app---> 日志服务器--->Flume--->kafka--->flume--->hdfs

再实际中，日志服务器可能部署再大数据集群中，也可能不在。

再第一个Flume中，有三种实现方案。

第二个flume中，有两种实现方案。

> 思考为什么中间需要一个kafka集群？

### 业务数据采集平台

业务数据是使用sqoop将数据导入到hdfs上面，对于业务数据，重要的是数据业务表之间的联系。

#### 两个重要概念

电商常识（SKU、SPU）

SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。

SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。

例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。

SPU表示一类商品。好处就是：可以共用商品图片，海报、销售属性等。

spu描述的粒度更粗，sku描述一件具体商品，粒度细。

#### 表ER模型图

本项目中设计24张表结构。

![20211212144449](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212144449.png)

拿到表的脚本建立表之后，使用EZDML工具建立各个表之间的联系。

#### 使用sqoop导入数据

sqoop可以实现双向传输，也就是从sqoop到mysql和从mysql到sqoop的双向传输。

sqoop的底层就是mapreduce任务，所以延迟很高，底层的mr任务只有map任务，没有reduce任务，这个可以从sqoop功能来看，sqoop的定位只是用来传输数据，不用来分析数据，所以只需要使用map将数据读出来然后写入hdfs即可。

那么sqoop做的工作就是自定义inoutFormat和outputFormat组件。自定义读取数据可以从数据库中读取。

##### sqoop基础操作

**连接mysql**

```java
bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password 000000
```
再导入的时候，sqoop支持将mysql数据库中的一张表导入到hdfs中的一个路径，或者是一张表或者是hbase中的一张表，但是到处的时候，只支持将hdfs上一个路径下的文件导出到mysql中的一张表中。

**导入数据**

```java
bin/sqoop import --connect jdbc:mysql://hadoop102:3306/gmall 
--user root 
--password root
--table user_info  //全量表
--columns id,login_name//导入具体的列
--where "id >=10 and id<=30" //导入数据的过滤条件
--target-dir /test // 上传文件的路径
--delete-target-dir //如果目标路径存在就删除，这个参数可以保证数据的幂等性，因为如果任务失败，再次导入的话，会删除文件夹重新上传数据
//优化相关参数
--num-mappers 2//指的是到数据的时候map任务个数
--fields-terminated-by //字段之间分隔符
--splits-by id //按照哪一个字段进行切片
```

关于这里的切片机制，自定义的inoutFormat会根据我们输入的配置进行切片，也就是根据我们输入的--num-mappers参数，将数据根据id进行切片，然后交给map处理。

**使用sql方式导入数据**

```java
bin/sqoop import --connect jdbc:mysql://hadoop102:3306/gmall 
--user root 
--password root
--query "select id,login_name from user_info where id >=10 and id<=30"
--target-dir /test // 上传文件的路径
--delete-target-dir //如果目标路径存在就删除，这个参数可以保证数据的幂等性，因为如果任务失败，再次导入的话，会删除文件夹重新上传数据
//优化相关参数
--num-mappers 2//指的是到数据的时候map任务个数
--fields-terminated-by //字段之间分隔符
--splits-by id //按照哪一个字段进行切片
```

#### 同步策略

数据同步策略的类型包括：**全量同步、增量同步、新增及变化同步、特殊情况**

- 全量表：存储完整的数据。
- 增量表：存储新增加的数据。
- 新增及变化表：存储新增加的数据和变化的数据。
- 特殊表：只需要存储一次。

##### 全量同步

每日全量，就是每天存储一份完整数据，作为一个分区，所以说全量表也是一个分区表，里面存储的是每天从mysql中导出来的全部数据。

**适用于表数据量不大，且每天既会有新数据插入，也会有旧数据的修改的场景。**

例如：编码字典表、品牌表、商品三级分类、商品二级分类、商品一级分类、优惠规则表、活动表、活动参与商品表、加购表、商品收藏表、优惠卷表、SKU商品表、SPU商品表

![20211212154656](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212154656.png)

##### 增量同步

每日增量，就是每天存储一份**增量数据**，作为一个分区，增量数据通常存储再**增量表**中，可以按照天使用一个**分区存储**。

增量同步适用于mysql中每天只会新增数据，不会发生修改的表，比流水表，支付流水，订单状态流水表。

适用于表数据量大，且每天只会有新数据插入的场景。例如：退单表、订单状态表、支付流水表、订单详情表、活动与订单关联表、商品评论表。

![20211212154853](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212154853.png)

##### 新增及变化策略

每日新增及变化，就是存储**创建时间和操作时间**都是今天的数据，这里使用的不是分区表。

查询数据主要有两种，获取最新数据，获取历史上某一天数据，但是再新增及变化表中查询上面两种数据很不方便。

**适用场景为，表的数据量大，既会有新增，又会有变化。例如：用户表、订单表、优惠卷领用表。**

![20211212155015](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212155015.png)

##### 特殊策略

某些特殊的维度表，可不必遵循上述同步策略。

1）客观世界维度

没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。

2）日期维度

日期维度可以一次性导入一年或若干年的数据。

##### 本项目中表的导入策略


![20211213141749](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213141749.png)

本项目中，全量表一般是一些描述性信息，有变化，但是数据量很小。

##### sqoop导入数据

使用脚本导入数据，一张表需要写一个sql命令。

这里注意一点就是再mysql中存储空值使用的是null，但是再HIve中空值存储再hdfs上面使用的是\N，

所以使用sqoop从mysql中导入数据到hive中需要将空值转换为\N，所以再sqoop的脚本中需要添加下面两个参数：

```java
--null-string '\\N' \     表示字符串类型的空值存储为什么
--null-non-string '\\N'  非字符串类型空值存储为什么
```
同步脚本请参考文档。

**如何同步增量数据：**

```sql
import_data order_detail "select 
                              id,
                              order_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              create_time,
                              source_type,
                              source_id,
                              split_total_amount,
                              split_activity_amount,
                              split_coupon_amount
                            from order_detail 
                            where DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date'"
}
```
增量数据，比如说订单详情表，如果我们需要获取前一天的新增数据，那么就是订单的支付时间==前一天的时间即可。

**全量数据：**

全量数据全部需要导入，所以where条件直接写为true即可，也就是1=1.

**新增及变化**

```sql
import_data "user_info" "select 
                            id,
                            login_name,
                            nick_name,
                            name,
                            phone_num,
                            email,
                            user_level, 
                            birthday,
                            gender,
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}
```

需要两个条件：
1. 创建时间等于前一天，可以找到新增的数据。
2. 操作时间等于前一天，可以找出变化数据。

### 数仓理论

#### 离线数仓分层

![20211213151648](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213151648.png)

明细数据和汇总是一个相反的概念，明细表示最原始，最详细的数据，汇总，比如统计今天的支付金额，一条数据由多条数据汇总而来。

dwd层存储的数据是最明细的数据，比较重要的一层，是基础。对于业务数据来说，本来就是结构化的，但是对于日志数据来说，是一个json字符串，需要解析为一个一个的字段。

dws和dwt都是汇总的数据，不同的是dws是按照**天**进行汇总的，而dwt是按照**主题**进行汇总，汇总多少天或者某一个地区的数据，以主题为单位。汇总的粒度不一样。

ads是聚合好的数据，比如需要报表数据，那么ads层就把报表数据处理好，别人直接使用即可。

#### 数据集市和数据仓库区别

数据集市则是一种微型的数据仓库，它通常有更少的数据，更少的主题区域，以及更少的历史数据，因此是部门级的，一般只能为某个局部范围内的管理人员服务。

数据仓库是企业级的，能为整个企业各个部门的运行提供决策支持手段。

![20211213153517](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213153517.png)

#### 范式理论

**定义**

范式可以理解为设计一张数据表的表结构，符合的标准级别、规范和要求。

**优点**

采用范式，可以降低数据的冗余性。

为什么要降低数据冗余性？

1. 十几年前，磁盘很贵，为了减少磁盘存储。
2. 以前没有分布式系统，都是单机，只能增加磁盘，磁盘个数也是有限的
3. 一次修改，需要修改多个表，很难保证数据一致性

**缺点**

范式的缺点是获取数据时，需要通过Join拼接出最后的数据。数据规范化，那么数据的粒度越细，但是会影响查询性能。

**分类**

目前业界范式有：
1. 第一范式(1NF)：消除完全函数依赖
2. 第二范式(2NF)：消除部分函数依赖
3. 第三范式(3NF)：消除传递依赖
4. 巴斯-科德范式(BCNF)
5. 第四范式(4NF)
6. 第五范式(5NF)

级别越高，数据的冗余度越小。

##### 第一范式：

![20211213154247](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213154247.png)

比如通过，(学号，课程) 推出分数 ，但是单独用学号推断不出来分数，那么就可以说：分数 完全依赖于(学号，课程) 。

即：通过AB能得出C，但是AB单独得不出C，那么说C完全依赖于AB。

第一范式实际上要求所有的属性不可分割，实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。

##### 第二范式

![20211213154400](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213154400.png)

比如通过，(学号，课程) 推出姓名，因为其实直接可以通过，学号推出姓名，所以：姓名部分依赖(学号，课程)

即：通过AB能得出C，通过A也能得出C，或者通过B也能得出C，那么说C部分依赖于AB。

如果存在部分函数依赖，我们可以把主键拆开，分为两张表即可。

##### 第三范式

![20211213154448](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213154448.png)

比如：学号推出系名，系名推出系主任， 但是，系主任推不出学号，系主任主要依赖于系名。这种情况可以说：系主任传递依赖学号。

通过A得到B，通过B得到C，但是C得不到A，那么说C传递依赖于A。

同样，如果存在传递函数依赖，也需要对表进行拆分操作。

#### 关系建模与维度建模

当今的数据处理大致可以分成两大类：**联机事务处理OLTP（on-line transaction processing）、联机分析处理OLAP（On-Line Analytical Processing）**。OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的**事务处理**，例如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。二者的主要区别对比如下表所示。

![20211213155001](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213155001.png)

通常对于范式建模，我们使用er图建模，范式理论建立额模型表的数量比较多。

##### 关系建模

![20211213155217](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213155217.png)

关系模型如图所示，严格遵循第三范式（3NF），从图中可以看出，较为松散、零碎，物理表数量多，数据的粒度比较细，而数据冗余程度低。由于数据分布于众多的表中，这些数据可以更为灵活地被应用，功能性较强。关系模型主要应用与OLTP系统中，为了保证数据的一致性以及避免冗余，所以大部分业务系统的表都是遵循第三范式的。缺点是查询数据的时候比较麻烦，需要进行很多的join操作，影响查询的性能。

因为范式建模需要去除数据的冗余性，所以需要对表进行切割，导致表比较多，比较分散。

##### 维度建模

![20211213155259](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213155259.png)

维度模型如图所示，主要应用于OLAP系统中，通常以某一个**事实表**（事实表一般在中心，并且有多张事实表，主要是一些操作）为中心进行表的组织，主要面向业务，特征是可能存在数据的冗余，但是能方便的得到数据。以业务为驱动，很方便理解。更适合做数据分析，更适合做聚合的数据分析。维度表一般是一些描述性的信息，而事实表中的属性就是维度表的外键组成的。维度表的属性一般有某一个事务的属性以及其他和该事物相关的属性组成的。

**关系模型虽然冗余少，但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率**。所以通常我们采用维度模型建模，把相关各种表整理成两种：事实表（中间的表）和维度表（两边的表）两种。维度表中存储的是对事实表的描述信息。事实表一般存储的是业务信息，动词。

维度模型通常是以一个主题为单位，通常围绕一个主题进行建模。这样可以完整的描述用户的一个动作，而如果使用范式理论建模，那多个表之间join才可以描述清除。

比如再上面的模型中，中间的事实表是销售订单，如果我想看各个地区的销售情况，那么我就可以将销售表和Location表进行关联，然后以LocationId进行分区聚合，类似的，还可以以Gender进行聚合操作。

#### 事实表和维度表

##### 维度表

维度表本质上是我们分析数据的角度，根据每一个维度去聚合分析事实表数据，也可以理解为我们后期写sql分组的字段。

维度表：一般是对事实的描述信息。每一张维表对应现实世界中的一个对象或者概念。    

例如：用户、商品、日期、地区等。可以看做是关系型数据库er图中的对象信息。

维表的特征：

1. 维表的范围很宽（具有多个属性、列比较多）对象本身的属性以及和对象相关联的其他对象的属性。
2. 跟事实表相比，行数相对较小：通常< 10万条，因为数对对象的描述性信息，不需要存储太多的数据
3. 内容相对固定：编码表

**时间维度表**

![20211213161758](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213161758.png)

##### 事实表

整个业务系统中，有很多的业务，那么每一个业务都有一张事实表和其对应，比如下单事实表，支付事实表中一行表示一个支付事件，订单事实表中一行表示一个订单事件。

事实表中的每行数据代表一个业务事件（下单、支付、退款、评价等）。**“事实”这个术语表示的是业务事件的度量值（可统计次数、个数、金额等）**，例如，2020年5月21日，宋宋老师在京东花了250块钱买了一瓶海狗人参丸。维度表：时间、用户、商品、商家。事实表：250块钱、一瓶。

每一个事实表的行包括：具有可加性的数值型的度量值、与维表相连接的外键，通常具有两个和两个以上的外键。

事实表的特征：

1. **非常的大，每天都会增加很多数据**
2. **内容相对的窄：列数较少（主要是外键id和度量值）**
3. **经常发生变化，每天会新增加很多。**

**事务型事实表**

以每个事务或事件为单位，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。**一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新**。一行数据就是一个具体的事件。

事务性事实表对应mysql中的那张表是不会发生变化的，每一天只会新增数据，不会发生修改。

> 事务性事实表保留所有的数据。

**周期型快照事实表**

周期型快照事实表中**不会保留所有数据，只保留固定时间间隔的数据**，例如每天或者每月的销售额，或每月的账户余额等。离线数仓中一般周期是一天，也就是把mysql中一天的数据做一个快照。**周期型快照事实表就是一个全量表。**

例如购物车，有加减商品，随时都有可能变化，但是我们更关心每天结束时这里面有多少商品，方便我们后期统计分析。

**累积型快照事实表**

**累计快照事实表用于跟踪业务事实的变化**。

例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。

![20211213164412](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211213164412.png)

将事实表和同步策略对比：

事务性事实表：增量同步，分区表。事务性事实表存储的就是增量同步数据，按照分区表存储，每一天存储当天新增的数据。

周期性快照事实表：对应全量同步，也是一个分区表，相当于每天做一个快照，每一个分区中存储mysql中一张表的快照。

累积性快照事实表：新增及变化同步，这一部分比较麻烦，因为要从Mysql中获取新增及变化然后和累积性快照事实表中的数据做一个整合。

#### 维度模型分类

在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。

**星型模型**

![20211215123449](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211215123449.png)

雪花模型与星型模型的区别主要在于维度的层级，标准的星型模型维度只有一层，而雪花模型可能会涉及多级。


**雪花模型**

![20211215123556](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211215123556.png)

雪花模型，比较靠近3NF，但是无法完全遵守，因为遵循3NF的性能成本太高。

**星座模型**

![20211215123652](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211215123652.png)

星座模型与前两种情况的区别是事实表的数量，星座模型是基于多个事实表。也就是说多个事实表可以公用一个维度表。

**模型欸都选择**

首先就是星座不星座这个只跟数据和需求有关系，跟设计没关系，不用选择。

基本上是很多数据仓库的常态,因为很多数据仓库都是多个
事实表的。所以星座不星座只反映是否有多个事实表,他们之间
是否共享一些维度表所以星座模型并不和前两个模型冲突

**星型还是雪花，取决于性能优先，还是灵活更优先**。目前实际企业开发中，不会绝对选择一种，根据情况灵活组合，甚至并存（一层维度和多层维度都保存）。但是整体来看，更倾向于维度更少的星型模型。尤其是Hadoop体系，减少Join就是减少Shuffle，性能差距很大。（关系型数据可以依靠强大的主键索引）


### 数据仓库建模

#### ODS层

**HDFS用户行为数据**

就是将hdfs上面的文件和表进行映射，中间做一个缓冲。对于日志，一般只建立**一张表**，将所有的日志放在一张表中(因为我们采集所有类型的日志都在一张表中)，**表只有一个字段**。在DWD层对日志进行解析。

**HDFS业务数据**

因为mysql业务数据本身就是结构化的数据，所以我们只需将mysql数据库中的表数据导入hdfs中即可，数据原封不动。

**针对HDFS上的用户行为数据和业务数据，我们如何规划处理？**

1. 保持数据原貌不做任何修改，起到备份数据的作用。
2. 数据采用压缩，减少磁盘存储空间（例如：原始数据100G，可以压缩到10G左右）
3. 创建分区表，防止后续的全表扫描

对于业务数据，hdfs上面有哪些数据文件，就建立那几张表，因为业务数据是从mysql中导入的，所以建立的表根据mysql表即可。

> ods层的数据也需要进行分区，不管是日志还是业务数据，每一天需要导入一次，所以每天按照日期进行分区。

#### DWD层

DWD层需构建**维度模型**，一般采用星型模型，呈现的状态一般为星座模型。是最重要的一层。

维度建模一般按照以下四个步骤：

> 选择业务过程→声明粒度→确认维度→确认事实

1. 选择业务过程

在业务系统中，挑选我们感兴趣的业务线，比如**下单业务，支付业务，退款业务，物流业务**，一条业务线对应一张**事实表**。

如果是中小公司，尽量把所有业务过程都选择。

如果是大公司（1000多张表），选择和需求相关的业务线。

2. 声明粒度

数据粒度指数据仓库的数据中保存数据的**细化程度或综合程度**的级别。

**声明粒度意味着精确定义事实表中的一行数据表示什么**，应该尽可能选择最小粒度，以此来应各种各样的需求。

典型的粒度声明如下：

订单事实表中一行数据表示的是一个订单中的一个商品项。

支付事实表中一行数据表示的是一个支付记录。

> 声明粒度，一般是声明事实表的粒度，这个粒度要选择最小的粒度，也就是最明细的数据。

**确定维度**

维度的主要作用是描述业务是**事实**，主要表示的是“谁，何处，何时”等信息。我们后期就是根据维度进行主题的统计。

确定维度的原则是：后续需求中是否要分析相关维度的指标。例如，需要统计，什么时间下的订单多，哪个地区下的订单多，哪个用户下的订单多。需要确定的维度就包括：**时间维度、地区维度、用户维度**。（也就是确认每一张事实表和哪一张维度表有关系）

**确定事实**

此处的“事实”一词，指的是业务中的**度量值**（**次数、个数、件数、金额，可以进行累加**），例如订单金额、下单次数等。（事实表中的字段有两类，第一类是维度表的外键，另一类是度量值，也就是说每一张事实表都有一个度量值，维度外键在第三步中确定，不同的业务，有不同的度量值）

在DWD层，以业务过程为建模驱动，基于每个具体业务过程的特点，构建最细粒度的明细层事实表。事实表可做适当的宽表化处理。

事实表和维度表的关联比较灵活，但是为了应对更复杂的业务需求，可以将能关联上的表尽量关联上。如何判断是否能够关联上呢？在业务表关系图中，只要两张表能通过中间表能够关联上，就说明能关联上。

![20211215131048](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211215131048.png)

> 在这里有了订单明细，为什么还要订单表，这里是处于性能的考虑，假如有一些需求既可以从订单表中得到，还可以从订单明细中得到，那么选择哪种方法呢？应该选择订单表，因为订单明细表数据量更大，计算量就越大，订单表相对来说数据量少。


横向描述的是事实表，纵向描述的是维度表。

至此，数据仓库的维度建模已经完毕，DWD层是以**业务过程**为驱动。

DWS层、DWT层和ADS层都是以**需求**为驱动，和维度建模已经没有关系了。

DWS和DWT都是建**宽表**，按照**主题**去建表。主题相当于观察问题的角度。对应着维度表。

![20211215131227](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211215131227.png)


对于日志数据，根据内容对日志文件进行解析。按照内容解析，每一类日志文件的字段类型一致，方便进行查询。（我们的日志一般分为，普通页面日志，启动日志，曝光日志等）

维度表一般是**名词**，可以从关系型数据库的er图中去选择。一般都是描述性的表。

日志数据一般是按照内容进行解析，相同类型的日志放在一起。

#### DWS层与DWT层

DWS层和DWT层统称**宽表层**，这两层的设计思想大致相同，都是建立宽表，通过以下案例进行阐述。

1. 问题引出：两个需求，统计每个省份订单的个数、统计每个省份订单的总金额
2. 处理办法：都是将省份表和订单表进行join，group by省份，然后计算。同样数据被计算了两次，实际上类似的场景还会更多。那怎么设计能避免重复计算呢？
3. 针对上述场景，可以设计一张地区宽表，其主键为地区ID，字段包含为：下单次数、下单金额、支付次数、支付金额等。上述所有指标都统一进行计算，并将结果保存在该宽表中，这样就能有效避免数据的重复计算。在宽表中，以维度为核心。
4. 总结：
   1. 需要建哪些宽表：以维度为基准。有哪些维度，就建立那些宽表
   2. 宽表里面的字段：是站在不同维度的角度去看事实表，重点关注事实表聚合后的**度量值**。
   3. DWS和DWT层的区别：DWS层存放的所有主题对象**当天**的汇总行为，例如每个地区当天的下单次数，下单金额等，DWT层存放的是所有主题对象的**累积行为**，例如每个地区最近７天（１５天、３０天、６０天）的下单次数、下单金额等。

> 宽表中的度量值是很重要的一点。

#### ADS层

对电商系统各大主题指标分别进行分析。

### Hive计算引擎

Hive引擎包括：默认MR、tez、spark。

Hive on Spark：Hive既作为存储元数据又负责SQL的解析优化，语法是HQL语法，执行引擎变成了Spark，Spark负责采用RDD执行。使用的是hive的客户端，也就是写sql的时候，使用的语法还是Hive语法。

Spark on Hive : Hive只作为存储元数据，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。使用的是spark sql的客户端，这里的spark只得是spark sql，使用的是spark sql的客户端。

> hive on spark 使用的是hive客户端和语法。
> spark on hive使用spark sql客户端和语法。

到底什么是兼容性？

比如我们需要配置hive on spark，那么我们就要在集群中安装两个框架。所以我们需要在hive配置文件中引入spark的依赖，假如现在使用hive3.12进行开发，那么引入的spark框架是2.4.5版本，那如果引入的spark版本不是2.4.5的化，可能就会出现兼容性问题，也有可能没有兼容性问题。假如2.4.5中有一些过时的api，那么在spark3.0版本中可能将这些过时的api进行去掉，那么如果引入spark3.0版本的话，那些过时的api就无法使用，就出现了不兼容问题。

那么如何解决不兼容问题，需要重新编译源码，将hive源码下载下来，然后从中找到pom依赖，然后修改spark版本的依赖，然后重新编译项目，不兼容会发生报错。然后找到不兼容的类发生的地方，然后找到哪个方法或者类过时了，就重新修改。


### 配置yarn容量调度器多队列

在企业里面如何配置多队列：

- 按照计算引擎创建队列hive、spark、flink

- 按照业务创建队列：下单、支付、点赞、评论、收藏（用户、活动、优惠相关）
- 
多队列有什么好处？

- 假如公司来了一个菜鸟，写了一个递归死循环，公司集群资源耗尽，大数据全部瘫痪。
- 可以使用队列统一管理任务优先级，保证重要的任务优先完成。
- 多个队列，可以对集群的资源进行限制和隔离，防止因为资源问题而导致集群瘫痪。

**增加容量调度器队列**

1. 修改容量调度器配置文件

默认Yarn的配置下，容量调度器只有一条default队列。在capacity-scheduler.xml中可以配置多条队列，修改以下属性，增加hive队列。

```java
<property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,hive</value>
    <description>
     再增加一个hive队列
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
<value>50</value>
    <description>
      default队列的容量为50%
    </description>
</property>
```

为新添加的队列增加属性信息：

```java
<property>
    <name>yarn.scheduler.capacity.root.hive.capacity</name>
<value>50</value>
    <description>
      hive队列的容量为50%
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
<value>1</value>
    <description>
      一个用户最多能够获取该队列资源容量的比例，取值0-1
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
<value>80</value>
    <description>
      hive队列的最大容量（自己队列资源不够，可以使用其他队列资源上限）,表示可以借到的资源的最大值
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.state</name>
    <value>RUNNING</value>
    <description>
      开启hive队列运行，不设置队列不能使用
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
<value>*</value>
    <description>
      访问控制，控制谁可以将任务提交到该队列,*表示任何人
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
<value>*</value>
    <description>
      访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
<value>*</value>
<description>
      指定哪个用户可以提交配置任务优先级
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
<value>-1</value>
    <description>
      hive队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。
</description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
<value>-1</value>
    <description>
      hive队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。
</description>
</property>
```
提交任务指定使用的队列：

```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=hive 1 1
```
> 可以提交任务的只能是根节点的子节点。

在hive客户端中进行设定：

```java
set mapreduce.job.queuename=hive
```

### 搭建ODS层数据仓库

#### 日志数据

对于日志数据，我们建立一张表，表中只有一个字段，就是json格式的日志字符串。这张表也是一张分区表，每天会根据日期写入一个分区。

在这里使用的是lzo进行数据的压缩，所以上传到hdfs上面的文件，需要我们手动建立lzo索引文件，当然也可以把建立索引文件的语句写入脚本中，在导入数据的时候，直接建立索引。只要保证我们去ods层查询数据的时候，有索引即可，索引的存放位置需要和hive表放在一起。

![20211217101236](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217101236.png)

上图中是存储在ods层的日志数据格式，只有一个字段line存储的是一个json字符串，后面的dt字段是分区字段。

#### 业务数据

![1639980406913](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1639980406913.png)

根据我们mysql业务数据库中的表结构，再hive中建立相同的表结构，然后使用sqoop讲数据导入到hive表中。

- 订单表：增量及更新，每一天会向hive表中同步增量及更新数据。
- 订单详情表：增量表，每天同步增量数据。
- sku商品表，全量表，每一天同步所有的数据。
- 用户表：增量及更新，每天同步增量及更新数据。
- 商品一级分类：全量表。
- 商品二级分类：全量表；
- 商品三级分类：全量表。
- 支付流水表：同步增量数据。
- 省份表：特殊表
- 地区表：特殊表；
- 品牌表：全量表。
- 订单状态表：增量表
- spu商品表：全量
- 商品评论表：增量表
- 退单表：增量表
- 加购表：全量表
- 商品收藏表：全量表
- 优惠券领用表：新增及变化表
- 优惠券表：全量表
- 活动表：全量表
- 活动订单关联表：增量表
- 优惠规则表：全量
- 编码字典表：全量

#### ods层日志数据和业务数据

![20211217114341](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217114341.png)


### 搭建DWD层数据

#### DWD层的任务

1. 对用户行为数据解析。
2. 对核心数据进行判空过滤。
3. 对业务数据采用维度模型重新建模。

日志按照内容进行划分，一共有五类：
- 页面数据
- 事件日志
- 曝光日志
- 启动日志
- 错误日志

按照日志结构分类：

- 页面埋点日志
- 启动日志

> 本项目中，对数据的解析式按照内容进行解析。

#### 日志数据的解析

get_json_object是hive中专门用来解析json对象的工具。

##### 启动日志表 

启动日志解析思路：启动日志表中每行数据对应**一个启动记录**，一个启动记录应该包含日志中的**公共信息和启动信息**。先将所有包含start字段的日志过滤出来，然后使用get_json_object函数解析每个字段。启动日志表中的数据来自于启动日志。

> dwd,dws,dwt三层都是采用parquet列式存储格式，然后使用lzo进行数据的压缩。采用列式存储，对于数据的查询有好处，因为后面我们都是做聚合操作，需要对某一列做聚合操作，不需要查询其他的列，而列式存储刚好有利于对某一列进行查询。

> parquet存储+lzo压缩式什么格式呢？文件的格式本质上还式parquet的格式，压缩指的是parquet内部每一个列压缩采用的格式。

**启动日志数据格式**

- 公共字段
- start启动字段
- 时间戳

![20211217145428](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217145428.png)

在这里所有表插入数据的时候，使用的式overwrite，为了就是保证幂等性，如果任务失败，重试写入数据的时候，不会发生数据的重复。

##### 页面日志表

**页面日志解析思路：**页面日志表中每行数据对应**一个页面访问记录**，一个页面访问记录应该包含日志中的公共信息和页面信息。先将所有包含page字段的日志过滤出来，然后使用get_json_object函数解析每个字段。数据来自于页面埋点日志数据。

- 公共字段
- 页面字段

**数据格式**

![20211217150203](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217150203.png)

页面日志中，用改包含common字段，page信息和ts时间戳字段信息。

##### 动作日志表

动作日志解析思路：动作日志表中每行数据对应用户的**一个动作记录**，一个动作记录应当包含公共信息、页面信息以及动作信息。先将包含action字段的日志过滤出来，然后通过UDTF函数，将action数组“炸开”（类似于explode函数的效果），然后使用get_json_object函数解析每个字段。

为什么这里需要使用自定义udtf函数，因为explode()函数接收的式一个数组或者map才可以炸裂，但是ods层存储的json以字符串形式存储，所以我们需要自定义udtf函数将字符串转换为json数组。

- 公共字段
- action动作字段是一个数组。

**udtf函数设计思路**

![20211217180316](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217180316.png)

**自定义udtf函数**

initialize():
- 限定自定义函数的输入类型，这样就可以对数据做校验。
- 限定自定义函数的输出类型。

process():
- 对每一行数据的处理逻辑

forward():
- 将解析完成的数据进行输出。

在本项目中，initialize()方法中需要完成的功能：
1. 完成输入数据类型的校验。
2. 封装输出数据的类型

在process()中，完成字符串到数组的转换。

在hive中，临时函数只在本次的会话中有效，自定义函数的包放在hive所在的节点的本地即可，永久函数需要将自定义函数的jar包放在hdfs上面。

永久函数也有库的概念，如果在gmall库下面创建的函数，那么如果在其他数据库下面使用函数，需要使用gmall.进行引用。

**在hive中创建函数的语法**

下面创建的是永久函数，

```java
create function explode_json_array as 'com.rzf.hive.udtf.ExplodeJSONArray' using jar 'hdfs://hadoop102:8020/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar'
//usering jar指向hdfs上面jar包的路径;
```

经过炸裂函数，我们建立的虚表中有三个字段，line字段，dt分区字段和action字段。

**炸裂后虚表的结构**

![20211217175253](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217175253.png)

```sql
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert overwrite table dwd_action_log partition(dt='2020-06-14')
select
    get_json_object(line,'$.common.ar'),
    get_json_object(line,'$.common.ba'),
    get_json_object(line,'$.common.ch'),
    get_json_object(line,'$.common.md'),
    get_json_object(line,'$.common.mid'),
    get_json_object(line,'$.common.os'),
    get_json_object(line,'$.common.uid'),
    get_json_object(line,'$.common.vc'),
    get_json_object(line,'$.page.during_time'),
    get_json_object(line,'$.page.item'),
    get_json_object(line,'$.page.item_type'),
    get_json_object(line,'$.page.last_page_id'),
    get_json_object(line,'$.page.page_id'),
    get_json_object(line,'$.page.sourceType'),
    get_json_object(action,'$.action_id'),
    get_json_object(action,'$.item'),
    get_json_object(action,'$.item_type'),
    get_json_object(action,'$.ts')
from ods_log lateral view explode_json_array(get_json_object(line,'$.actions')) tmp as action
where dt='2020-06-14'
and get_json_object(line,'$.actions') is not null;
```

可以看到，曝光字段是从action的json对象中解析，这里没有使用ts时间戳字段，因为每一个动作中都自带时间戳字段。

需要注意的是，上面自定义函数传入的是一个json数组，但是这个数组是以字符串的形式存在的，我们自定义udtf函数的目的就是将字符串的数组转换为数组的格式，然后再炸裂开，因为hive中的炸裂函数需要使用数组参数或者map参数。

##### 曝光日志表

曝光日志解析思路：曝光日志表中每行数据对应**一个曝光记录**，一个曝光记录应当包含**公共信息、页面信息以及曝光信息**。先将包含display字段的日志过滤出来，然后通过UDTF函数，将display数组“炸开”（类似于explode函数的效果），然后使用get_json_object函数解析每个字段。

- 公共字段
- 页面信息
- 曝光信息

**过程**

![20211217180656](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217180656.png)

> 但是在这里不需要再自定义一个udtf函数，因为上面写的解析动作日志的udtf函数很通用，我们可以将曝光字符串数组传入自定义的udtf函数，然后返回一个解析好的数组，最后使用炸裂函数。

##### 错误日志表

错误日志解析思路：错误日志表中每行数据对应**一个错误记录**，为方便定位错误，一个错误记录应当包含与之对应的**公共信息、页面信息、曝光信息、动作信息、启动信息以及错误信息**。先将包含err字段的日志过滤出来，然后使用get_json_object函数解析所有字段。

为什么错误字段包含这么多信息？

因为再启动，曝光和点击的时候，都有可能发生页面的错误。

再错误日志中，存在一对多的现象，因为再动作日志和曝光日志中可能存在多个动作和曝光，也就是一个错误对应一个页面，但是一个页面又对应者多个动作或者多个曝光。

> 再动作日志和曝光日志中都存在一对多的情况。

> 再本项目中，一个错误对应一个页面，但是并没有对动作信息和曝光信息进行解析。实际项目中，可以根据需求进行解析。

解决lzo索引失效问题：

```sql
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
```
再向表中插入数据的时候，首先需要设置以下上面的参数。

#### 业务数据的解析

业务数据方面DWD层的搭建主要注意点在于维度建模，减少后续大量Join操作。

> 一共有8张事实表，6张维度表。事务性事实表很简单，拿到新增的数据然后追加到表中即可。加购和收藏是周期性事实表，因为我们并不是很关心数据的增删操作，做起来相对简单，每日快照，累积性快照事实表，需要修改表中的数据，一条数据分多次写入表，等获取到新数据之后，还会修改数据，所以比较麻烦。

维度表的特点，数据量不是很大并且数据稳定，所以一般都是**全量导入**，还有一些采用**增量及变化**导入，使用增量导入方式很少，因为本来数据就少，变化的不多，所以一般都是用全量，另外对于一些特殊的维度表，只需要导入一次即可。

如果采用新增及变化，那么就需要考虑数据仓库中的数据和数据库中的数据同步问题，所以需要将新增及变化数据和前一天的数据进行一个整和，需要使用拉链表，这种维度表一般对应的数据量会很大，不适合使用全量同步。

业务数据方面DWD层的搭建主要注意点在于维度建模，减少后续大量Join操作。

再本项目中，事实表和维度表之间的关系：

![20211217211858](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211217211858.png)

事实表和维度表是根据需要的业务，总结出来的。

##### 商品维度表（全量）

商品维度表主要是将商品表SKU表、商品一级分类、商品二级分类、商品三级分类、商品品牌表和商品SPU表联接为商品表。

##### 优惠券维度表（全量）

再ods层，和优惠券有关的表是coupon_info表，所以再dwd层，优惠券维度表和ods层的表字段相同，只需要阿静ods层关于收回全表的数据稍作清洗然后导入优惠券维度表即可。

##### 活动维度表（全量）

与活动有关的表有activity_info,activity_rule,activity_sku表，正常情况下需要将这三张表的字段合并在一起组成我们的活动维度表，但是活动规则中，一个商品可能参与多种活动，比如满减活动，也就是一个活动有多个优惠级别，所以将activity_info和activity_rule表进行join的话，一个活动会有多行数据。所以之后再将事实表订单表和活动表关联的时候，关联的字段是actovity_id，所以再关联的时候，会找到多个一样的活动id，因为上面已经说了，一个活动id有多个优惠级别。正常情况下，一个订单只能参与一个活动id的某一个优惠级别。所以我们应该再业务系统中获取订单以及订单的优惠级别，然后再关联。这个时候，订单和活动维度表再关联的时候，关联的id就有两个，活动id和优惠级别。

但是再本项目中，无法获取订单和优惠级别，所以就不在进行activity_info和activity_rule表进行关联，再活动维度表中只保留activity_info字段。

> 在做事实表和维度表进行关联的时候，只能是一对一关系。


##### 地区维度表（特殊）

地区维度表是一张特殊的表，因为变化很少，所以我们只需要加载一次即可。地区维度表中的数据字段来自ods_base_province和ods_base_region表，省份和省份中的地区。

##### 时间维度表（特殊）

前面的所有的维度表都是从业务系统中导入过来的数据，但是时间维度表并不是从业务系统中倒过来的，业务系统中并没有这张表，这张表属于数据仓库中的表，因为业务系统中只需要保留最新数据，并不涉及历史数据，所以并不需要时间维度，而数据仓库中需要是因为保存有历史数据，需要时间维度去解释。

时间维度表每一年只需要导入一次，所以并不会将导入数据语句写入到脚本中。之所以需要时间维度，就是我们需要根据时间去分析数据仓库中的数据。

##### 支付事实表（事务性事实表）

支付事实表一行数据所代表的含义：代表依次支付事件。

![20211219125110](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219125110.png)

支付事实表中的维度外键有：用户id,地区id，分区字段就代表时间id，度量值是payment_amount。

支付事实表的主要数据来源是payment_info,所以主要字段页来自于支付事实表，主要数据来源是payment_info，payment_type也可以作为一个度量字段。，

- ods_payment_info
- ods_order_info

**度量值是支付金额**

但是province_id需要从ods_order_info表中去查询，所以需要使用到join。

##### 退款事实表（事务性事实表）

把ODS层ods_order_refund_info表数据导入到DWD层退款事实表，在导入过程中可以做适当的清洗。

![20211219132332](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219132332.png)

退款事实表和用户维度，时间维度，商品维度有关，分区字段作为时间维度。

度量值有：**退款的件数，退款的金额**。

退款事实表中数据主要来自ods_order_refund_info，因为退款事实表中的字段和ods_order_refund_info一样，所以不需要进行join操作。

- ods_order_refund_info

##### 评价事实表(事务性事实表)

把ODS层ods_comment_info表数据导入到DWD层评价事实表，在导入过程中可以做适当的清洗。

![20211219132358](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219132358.png)

评价事实表中，一行数据指的是一条评价信息。涉及的维度有时间，用户，地区维度，商品维度等等。

**度量值是好评的条数。**

评价事实表中的数据和字段基本上来自于ods_comment_info表，所以可以直接将数据倒过来即可,我们只关心好评数，并不关心每天增加了多少条评论。

- ods_comment_info

##### 订单明细事实表（事务型事实表）

![20211219134804](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219134804.png)

订单明细事实表中一条数据表示一个订单中的一个商品的明细。

维度因该有时间，用户，地区和商品。

**度量值有商品的件数，订单中总金额，分摊优惠，分摊运费，分摊最终。**

订单明细表中的数据主要来自ods_order_detail和ods_order_info表。

- 原始价格分摊：商品的单价乘以商品的数量。
- 优惠分摊：按照原价的比例进行分摊，比如说商品a,b,c原始价格20，30，50。然后满100元优惠20元，所以优惠20最终付费80元，那么分摊到每一个商品中，优惠的价格是4，6，10元，但是这里也会产生问题，会产生误差，比如说优惠10元，每一个商品分摊3.33元，但是最终分摊结果的总和相加没有10元，所以需要补偿这个误差，这里额做法是使用10减去分摊的金额，然后补充在某一个商品分摊的金额上，需要使用开创函数：

![20211219142823](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219142823.png)

- ods_order_detail
- ods_order_info


##### 加购事实表（周期型快照事实表，每日快照）

每日全量表，因为我们关心的是购物车中的商品的件数，并不关系购物车中的商品每天新增或者减少。

由于购物车的数量是**会发生变化**，所以导增量不合适。

每天做一次快照，导入的数据是**全量**，区别于事务型事实表是每天导入**新增**。

周期型快照事实表劣势：存储的数据量会比较大。

解决方案：周期型快照事实表存储的数据比较讲究**时效性**，时间太久了的意义不大，可以删除以前的数据。

数据组要来源于：

- ods_cart_info

![20211219144352](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219144352.png)

一行数据代表，一个用户购物车中的一个商品。

度量值，一个用户的购物车中某一个商品一共有多少件，购物车中商品的总金额是多少（加入购物车商品的数量乘以单价）。

##### 收藏事实表（周期型快照事实表，每日快照）

收藏的标记，是否取消，会发生变化，做增量不合适。

每天做一次快照，导入的数据是全量，区别于事务型事实表是每天导入新增。

度量值，某个商品收藏的个数，通过累计行数确定。

数据主要来源：

- ods_favor_info

##### 优惠券领用事实表（累积型快照事实表）

![20211219150224](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219150224.png)

优惠卷的生命周期：领取优惠卷-》用优惠卷下单-》优惠卷参与支付
累积型快照事实表使用：统计优惠卷领取次数、优惠卷下单次数、优惠卷参与支付次数

维度字段，用户id，优惠券id，订单id，时间维度等，度量值是行数，表示一个优惠券被多少人领取。

对于事务性事实表，就是增量表，只需要将mysql中的增量数据存放到hive中当天的分区中。

周期性事实表也是将mysql中的全量数据放到hive表中当天的分区中。

累积性事实表在每一天向hive表中导入数据的时候，需要修改之前的数据，比如优惠券有领取时间，使用时间和过期时间，但是这几个时间不是一次性就直接可以写满，需要根据下单的过程进行去修改时间，那么mysql中修改过的时间需要同步到hive表中，所以需要修改历史数据。做法就是根据修改的数据，去hive表中查询数据然后进行更新操作，相当于修改数仓中的历史数据。

hive中也可以支持修改或者删除操作，只不过只有分桶表支持，因为如果是普通的表，因为hive支持一张表中数据量非常大，如果需要删除或者修改，会逐条判断，性能不高。所以hive采用分桶，将一张表中的数据分到多个桶中，也就是多个文件当中，首先定位文件，然后再小文件中修改操作。

但是我们一般不会进行分桶操作，而是采用另一种方法，先把hive表中的所有数据全部查询出来。然后对每一个字段逐个判断，使用if或者case when语句，判断哪一个字段需要修改，哪一个不需要修改，直接在select的过程中直接修改，然后再把查询出的结果写回去。

> 这种方式是便查询便修改，如果修改了，再把数据写会原表即可。

这一张表必须分区，如果不分区，那么比如修改表中100跳数据，就需要以整个表为单位进行查询然后修改，如果进行分区，那么就是以分区为单位进行查询和修改。

这里按照时间进行分区，但是每一个分区里面存储什么内容？

分区里面存储的是每一天新增的优惠券，如果是修改，那么就去之前的分区中，找到领取优惠券的那条记录，然后进行修改。

> 这个需求比较难

需要用到动态分区。

##### 订单事实表

![20211219190217](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219190217.png)

订单生命周期：创建时间=》支付时间=》取消时间=》完成时间=》退款时间=》退款完成时间。

由于ODS层订单表只有创建时间和操作时间两个状态，不能表达所有时间含义，所以需要关联订单状态表。订单事实表里面增加了活动id，所以需要关联活动订单表。

表中的一条数据表示一个订单，用户每下一个单，表中就会产生一条数据与之对应。

维度外键，用户，省份，时间，活动id维度等等，度量值：原价金额，优惠金额，运费和订单金额。

这个表是一个典型的累积性宽窄事实表，从字段中可以看出，下一个订单之后，订单有很多的中间状态，没完成一部，都需要修改订单的状态。

用户下单之后，数据会放入数据仓库，但是下单这个操作也不是一次性完成操作，可能过两天后才完成下单这个操作，所以我们需要跟踪用户下单的操作，及时修改数据仓库中的数据。

数据按照分区存放，因为也需要修改数据，然后分区中存放当天下的订单信息，不存放修改数据，如果有修改数据，就去创建订单的那个分区中修改数据。

**更新操作**

从当天新增及变化的数据中找到变化的订单是那一天下的单，根据create_time进行判断，然后根据创建时间我们就可以找到数据所在的分区，因为订单存储再当天创建订单的时间分区里面，然后获取整个分区中的数据，但是我们可能只是修改整个分区中很少的数据，然后我们把拿到的分区中的数据和今天的新增及变化的数据做一个对比，也就是进行join操作，join之后，数据的对应关系也就很清晰了，因为有的数据可以join到，有的数据是新增数据，没有左边的数据，无法关联，有的是变化数据，右边没有无法关联。

![20211219192335](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219192335.png)

就像上图中所示，最上边是老的数据，但是没有发生变化，中间的是关联上的，也就是老数据发生了变化，最下面的是新增数据，那么我们需要找到的就是中间的老数据并且发生变化的，所以关联的时候，我们使用full outer join。

> 这里也需要使用动态分区。

##### 用户维度表（拉链表）

用户表中的数据每日既有可能新增，也有可能修改，但修改频率并不高，属于缓慢变化维度，此处采用拉链表存储用户维度数据。

**什么是拉链表**

> 维度表的特点一般是数据量不是很大，并且数据相对的稳定，基于这样的特点，我们维度表一般都采用全量的同步法。那么再数仓里面这张表就叫做每日全量表。
> 
> 但是有些维度表，数据量很大，如果这样的表也做每日全量表，存储空间占用大，并且效率低，如果每天做全量，那么维度表本身数据变化很少，所以导过来了很多重复或者没用的数据。
> 所以当我们的维度表数据量很大，并且不方便做每日全量同步的话，这个时候，我们可以使用每日新增及变化，但是使用每日新增及变化，拿到的数据需要和数仓维度表中已经存在的数据做一个整合，所谓的整合就是整合为拉链表。
> 
> 在这里使用拉链表是为了解决数据的存储问题，因为全量的话，重复数据很多。

![20211219194347](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219194347.png)

**为什么需要做拉链表**

拉链表适合于：**数据会发生变化，但是大部分是不变的。（即：缓慢变化维）**

比如：用户信息会发生变化，但是每天变化的比例不高。如果数据量有一定规模，按照每日全量的方式保存效率很低。 比如：1亿用户*365天，每天一份用户信息。(做每日全量效率低)

![20211219194603](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219194603.png)

拉链表中一行数据是一个用户的一个状态，除了普通的字段之外，拉链表中还有两个时间的字段，一个是开始日期，一个是结束日期，代表的是状态的开始日期和结束日期。


拉链表可以降低存储是因为只对状态做一个开始时间和结束时间，因为始终保存的是状态，只要状态不发生改变，那么只会存储一份数据，这样再表中保存一份数据就可以了，如果使用全量同步，那么每天都需要存储一份。

全量表可以实现的功能，拉链表都可以实现，只是拉链表写起来比较麻烦。

**如何使用拉链表**

![20211219201028](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219201028.png)


比如需要获取全量的最新数据，可以使用结束时间进行过滤即可。因为一个用户只有一个状态是9999.

获取历史上某一天的全量数据，通过开始日期大于等于某一个日期，结束日期小于等于某一个日期的进行过滤。

同样使用事实表也是获取全量最新数据和历史某一天的数据。

**拉链表的制作过程**

1. 制作初始化导入，初始化起始时间。把mysql业务数据库中的数据一次性全部导入hive表中，做一次全量同步，结束时间都是9999，其实时间一般是根据实际情况指定一个时间。
2. 后续mysql业务数据库中的数据会发生新增和变化，需要把mysql中新增和变化的数据导入Mysql数据仓库中，通过crerate_time和operator_time两个字段获取新增和变化的数据。
3. 接下来将新增和变化的数据整合到拉链表中，首先将获取到的新增及变化数据补充上字段，开始时间就是今天，结束时间是9999，表示新增或者变化的数据是从今天开始，状态发生了变化。
4. 新增及变化数据处理好了，还需要处理原来拉链表中的历史数据，比如有些数据更新了，那么需要把过期的数据的结束日期修改下，一般修改为昨天的日期，必须保证一个用户所有状态之间的时间不会发生相互重合。
5. 最后将新增和变化的数据和处理过的历史数据进行unioin上下合并即可。
6. 再修改历史数据的过程中有涉及到修改数据的操作，在这里也是先查出数据，然后边查询边修改，最后再插入表中。

整个制作过程如下：

![20211219214012](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219214012.png)

![20211219214042](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211219214042.png)


在这里中间使用了一个临时表，是为了数据安全问题，因为我们再使用insert into或者overwrite的时候，是先删除原始表中的数据，然后再插入数据，但是如果再插入数据时候，任务失败了，那么原始数据也就丢失了。所以中间使用了一个临时表，原始数据并没有丢失。

> 但是实际上上面的考虑是多余的，因为Hive不是按照上面的做法做的，因为hive再插入数据的时候，也是先写入临时表，也就是先写入临时路径下的一个文件，等任务成功后，修改路径和文件名即可。


**第一步：初始化拉链表（首次独立执行）**

用户拉链表和ods层用户信息表字段基本一致，但是需要再添加两个字段，一个是开始时间，一个是结束时间。

拉链表为什么没有进行分区，因为我们再查询数据的时候，通常是根据时间进行查询的，也就是开始时间大于某一个时间，结束时间小于某一个时间，这样查询数据的话，如果分区，不能提高查询效率。

**步骤二：制作当日变动数据（包括新增，修改）每日执行**

因为本项目中只有2020-6-14号数据，所以基本上全是新增数据，没有变化数据，所以就从ods层中加载全部数据，然后添加上两个时间字段即可。

1. 如何获得每日变动表
   1. 最好表内有创建时间和变动时间（Lucky!）
   2. 如果没有，可以利用第三方工具监控比如canal，监控MySQL的实时变化进行记录（麻烦）。
   3. 逐行对比前后两天的数据，检查md5(concat(全部有可能变化的字段))是否相同(low)
   4. 要求业务数据库提供变动流水（人品，颜值）


**步骤三：先合并变动信息，再追加新增信息，插入到临时表中**

1. 先从ods层根据分区获取最新的的数据，然后对获取到的最新数据添加一个开始时间和结束时间，开始时间是就是获取数据那天的时间，结束时间是9999.
2. 处理老数据就是修改老数据的结束时间为9999，获取老数据是从dwd层的用户维度表。
3. 如何获取老数据，首先获取今天数据的新增和变化的数据，然后和原始表中的数据做一个对比，也就是进行full join，就会产生三部分数据，老数据没有修改，老数据并且修改，新增数据，我们只获取老数据并且修改的数据。

> 再hive中，可以进行union的条件是两边都必须是子查询。不管是几个select子查询union到一起，最终只当作一个select整体额子查询即可。

**步骤四：把临时表覆盖给拉链表**

先把查询的数据放到临时表中，然后整体再插入拉链表中。


### DWS层

dwd层建模是以业务为驱动，有什么业务线就建立什么事实表，业务和那些维度有关，就和维度关联即可，维度建模不需要考虑需求。

dws和dwt层数据宽表层，建模的时候有需求驱动，再dws或dwt层建立宽表的时候，我们是根据维度建立宽表，因为我们后续再做统计分析的时候，更多额时候是根据维度进行统计聚合，分组字段往往是维度id。

#### 业务术语

**用户**

用户以设备为判断标准，在移动统计中，每个独立设备认为是一个独立用户。Android系统根据IMEI号，IOS系统根据OpenUDID来标识一个独立用户，每部手机一个用户，我们以设备id为标准。因为如果用户不登陆，我们无法获取用户的id，这个也可以表示访客，也就是用户没有登录，但是访问了网站，所以可以用来记录访客数量。

**新增用户**

首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。

**活跃用户**

打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户，有日活跃，周活跃，月活跃。

**周（月）活跃用户**

某个自然周（月）内启动过应用的用户，该周（月）内的多次启动只记一个活跃用户。

**月活跃率**

月活跃用户与截止到该月累计的用户总和之间的比例。

**沉默用户**

用户仅在安装当天（次日）启动一次，后续时间无再启动行为。该指标可以反映新增用户质量和用户与APP的匹配程度。

**版本分布**

不同版本的周内各天新增用户数，活跃用户数和启动次数。利于判断APP各个版本之间的优劣和用户行为习惯。

**本周回流用户**

上周未启动过应用，本周启动了应用的用户。也就是一段时间没有使用产品，但是一段时间之后，又开始使用。

**连续n周活跃用户**

连续n周，每周至少启动一次。

**忠诚用户**

连续活跃5周以上的用户

**连续活跃用户**

连续2周及以上活跃的用户

**近期流失用户**

连续n（2<= n <= 4）周没有启动应用的用户。（第n+1周没有启动过）

**留存用户**

某段时间内的新增用户，经过一段时间后，仍然使用应用的被认作是留存用户；这部分用户占当时新增用户的比例即是留存率。
例如，5月份新增用户200，这200人在6月份启动过应用的有100人，7月份启动过应用的有80人，8月份启动过应用的有50人；则5月份新增用户一个月后的留存率是50%，二个月后的留存率是40%，三个月后的留存率是25%。

**用户新鲜度**

每天启动应用的新老用户比例，即新增用户数占活跃用户数的比例。

**单次使用时长**

每次启动使用的时间长度。

**日使用时长**

累计一天内的使用时间长度。

**启动次数计算标准**

IOS平台应用退到后台就算一次独立的启动；Android平台我们规定，两次启动之间的间隔小于30秒，被计算一次启动。用户在使用过程中，若因收发短信或接电话等退出应用30秒又再次返回应用中，那这两次行为应该是延续而非独立的，所以可以被算作一次使用行为，即一次启动。业内大多使用30秒这个标准，但用户还是可以自定义此时间间隔。


再dws中和dwt层的主题宽表对应的是dwd层的维度，一个维度对应一个主题。

#### 每日设备行为

每日设备行为，主要按照设备id统计，但是并没有设备维度表。

设备信息直接融入到宽表当中。

```sql
drop table if exists dws_uv_detail_daycount;
create external table dws_uv_detail_daycount
(
    `mid_id`      string COMMENT '设备id',
    `brand`       string COMMENT '手机品牌',
    `model`       string COMMENT '手机型号',
    `login_count` bigint COMMENT '活跃次数',
    `page_stats`  array<struct<page_id:string,page_count:bigint>> COMMENT '页面访问统计'
) COMMENT '每日设备行为表'
partitioned by(dt string)
stored as parquet
location '/warehouse/gmall/dws/dws_uv_detail_daycount'
tblproperties ("parquet.compression"="lzo");
```
每一行数据代表一个设备累计的一天的行为。

汇总值：活跃次数和页面访问统计。

每一天每一个分区存储的是当天活跃的设备。

活跃次数来自于启动日志表。

页面访问统计来自于页面日志表。

> hive中只要进行分组，那么下面可以选择的字段有以下暗中情况：分组的字段，聚合函数聚合的字段，常量值三种。

dwd_pag_log中的数据，一行数据表示一条浏览记录。我们需要求出每一个设备浏览每一个页面的次数。所以首先按照设备id+page id进行分组。

#### 每日会员行为

再dws层是每日行为，以天为单位进行聚合操作。

再dwt层是累计值。

```sql
drop table if exists dws_user_action_daycount;
create external table dws_user_action_daycount
(   
    user_id string comment '用户 id',
    login_count bigint comment '登录次数',
    cart_count bigint comment '加入购物车次数',
    order_count bigint comment '下单次数',
    order_amount    decimal(16,2)  comment '下单金额',
    payment_count   bigint      comment '支付次数',
    payment_amount  decimal(16,2) comment '支付金额',
    order_detail_stats array<struct<sku_id:string,sku_num:bigint,order_count:bigint,order_amount:decimal(20,2)>> comment '下单明细统计'
) COMMENT '每日会员行为'
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dws/dws_user_action_daycount/'
tblproperties ("parquet.compression"="lzo");
```
表中的一条数据表示一个用户再当天的所有行为的聚合值，一个用户一行数据，唯一。

表中所有字段是和维度表user相关的事实表的度量值字段。

**字段数据来源：**

用户id:

登录次数：来自于启动日志表，再dwd层的启动日志中，每一行代表一个启动记录，所以对数据按照用户id进行分组，就可以得到每一个用户的登录次数。

加入购物车次数：去action_log中获取数据。，因为没有事实表可以计算该参数。先把加购物车的数据过滤出来，然后按照用户id进行分组count()即可。

下单次数：来自于订单表

下单金额：订单表中的最终下单金额。

支付次数：来自于支付事实表

支付金额：来自于支付事实表

下单明细统计：一个用户每天可能下多个订单，订单中多个商品，所以字段中一个结构体表示一个商品，数组中存在多个商品。

> 表按照天进行分区，分区中存储的是当天活跃用户的各种行为。

**求登录次数**

按照userId进行分区，这个值可能是null,也就是有的用户没有登录就直接使用了，如果直接按照userid进行分区，那么所有null都会分到一个分区中，可能产生数据倾斜。所以直接过滤掉null。

#### 每日商品行为

每一条数据表示一个关于商品的行为，表中的字段全部是关于商品的度量值。

向表中插入数据需要join几个子查询，我们可以使用别的方法进行插入，提高效率，但是中优化仅仅适用于表中的所有字段全部都是数字类型的表。

我们可以首先对子查询中的字段补齐，也就是原始表中有几个字段，我们就跟着补齐为几个字段，如果一个子查询中没有计算的字段，我们可以使用0代替，这样所有子查询中字段的个数是一样的，我们就可以使用union进行上下合并，然后对这张合并过的大表进行分组操作，然后对每一个字段累加，和我们使用join的效果是一样的。

#### 每日活动行为

每一行数据表示一个活动当天的汇总行为，也就是和活动维度表相关的事实。

```sql
drop table if exists dws_activity_info_daycount;
create external table dws_activity_info_daycount(
    `id` string COMMENT '编号',
    `activity_name` string  COMMENT '活动名称',
    `activity_type` string  COMMENT '活动类型',
    `start_time` string  COMMENT '开始时间',
    `end_time` string  COMMENT '结束时间',
    `create_time` string  COMMENT '创建时间',
    `display_count` bigint COMMENT '曝光次数',
    `order_count` bigint COMMENT '下单次数',
    `order_amount` decimal(20,2) COMMENT '下单金额',
    `payment_count` bigint COMMENT '支付次数',
    `payment_amount` decimal(20,2) COMMENT '支付金额'
) COMMENT '每日活动统计'
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dws/dws_activity_info_daycount/'
tblproperties ("parquet.compression"="lzo");
```

字段来自活动维度表中的维度字段。

曝光次数也是和活动相关的一个事实。

下单次数和下单金额是参与该活动的商品的下单次数和下单金额。

支付次数和支付金额：参与该活动并且支付的订单。

曝光次数来自于曝光日志。

下单次数和下单金额来自于订单事实表。

支付次数和支付金额可以根据订单表得出，统计支付了的订单，然后再统计支付金额和次数。

### DWT层宽表数据

累计值，dws是按照天进行汇总的，而dwt是对历史数据的累计。

#### 设备主题宽表

```sql
drop table if exists dwt_uv_topic;
create external table dwt_uv_topic
(
    `mid_id` string comment '设备id',
    `brand` string comment '手机品牌',
    `model` string comment '手机型号',
    `login_date_first` string  comment '首次活跃时间',
    `login_date_last` string  comment '末次活跃时间',
    `login_day_count` bigint comment '当日活跃次数',
    `login_count` bigint comment '累积活跃天数'
) COMMENT '设备主题宽表'
stored as parquet
location '/warehouse/gmall/dwt/dwt_uv_topic'
tblproperties ("parquet.compression"="lzo");
```

设备主题宽表不是一个分区表，也是记录累计值，记录的是全量数据信息，指的是包含所有的设备id。一行数据就是一个访客记录或者设备。

前三个字段是设备信息。

累计值是：

- 首次活跃时间：
- 末次活跃时间：
- 当日活跃次数：
- 累计活跃次数：

更新表里面的数据思路：首先获取到表中的所有历史数据，然后再获取到今天的活跃设备，然后进行一个全外连接，更新历史数据中活跃的设备和新增设备即可。

数据来自于前端的埋点数据，如果前端没有进行埋点的话，就无法获取数据进行初始化操作，主要会影响首次活跃时间字段，累计活跃天数如果没有历史数据，无法计算，我们可以从数仓搭建起开始那天计算。

首次活跃时间可以这样做，和数仓中历史数据进行对比，如果没有关联上，就算作新增数据。


#### 会员主题宽表

宽表字段怎么来？维度关联的事实表度量值+开头、结尾+累积+累积一个时间段。

**建表语句**

```sql
drop table if exists dwt_user_topic;
create external table dwt_user_topic
(
    user_id string  comment '用户id',
    login_date_first string  comment '首次登录时间',
    login_date_last string  comment '末次登录时间',
    login_count bigint comment '累积登录天数',
    login_last_30d_count bigint comment '最近30日登录天数',
    order_date_first string  comment '首次下单时间',
    order_date_last string  comment '末次下单时间',
    order_count bigint comment '累积下单次数',
    order_amount decimal(16,2) comment '累积下单金额',
    order_last_30d_count bigint comment '最近30日下单次数',
    order_last_30d_amount bigint comment '最近30日下单金额',
    payment_date_first string  comment '首次支付时间',
    payment_date_last string  comment '末次支付时间',
    payment_count decimal(16,2) comment '累积支付次数',
    payment_amount decimal(16,2) comment '累积支付金额',
    payment_last_30d_count decimal(16,2) comment '最近30日支付次数',
    payment_last_30d_amount decimal(16,2) comment '最近30日支付金额'
)COMMENT '会员主题宽表'
stored as parquet
location '/warehouse/gmall/dwt/dwt_user_topic/'
tblproperties ("parquet.compression"="lzo");
```

dwt层的数据是一个宽表，一行数据是一个用户，并且表中的字段是对用户各种操作的累计值，比如累计登录天数，累计30天登录天数。

涉及的表有：dwd_start_log，dws层订单事实表，dws用户行为表。

这张表没有进行分区，因为记录的都是历史数据的聚合，每天需要进行依次聚合，然后再写入原表中，所以表每一天都需要更新数据。

表中的字段计算方法来自于不同的表，比如登录日志，每一天都会有增加，如果我们每一天都进行计算，随着时间增长，数据量也会大大的增加。比如目前有100个分区，计算了依次，第二天增加3个分区，还要对103个分区重新计算，计算量大大增加不说，还有很多重复计算。

所以可以保存前面100个分区计算的结果，然后比如今天又增加三个分区，那么就计算者三个分区，然后去更新表中的值。所以最终的做法是先从表中查询记录，然后根据今天的值去更新操作。

> dwt层所有的表都是全量表，比如user_topic就应该包含所有的user。需要对所有的dwt层表进行一个初始化。

> 全量表一般不用做初始化，事实表一般也不用做初始化，拉链表第一天使用需要做初始化，因为拉链表记录的是新增和变化的数据。dwt层的表第一次使用需要做初始化。

需要拿到每一天的活跃用户，然后和原始表中的数据进行join比较操作，join之后，数据又join上的部分，也有没有join上的部分，代表的数据是：老用户并且今天活跃，老用户今天没有活跃，新增用户。

- 首次登录时间：对于老用户，这个字段不哟个改变，对于新用户，记录为今天的时间。
- 末次登录时间：对于老用户并且今天活跃和新用户的末次登录时间，修改为今天即可。
- 累计登录天数：原始用户并且没有活跃，不改变，原始用户并且今天活跃和新增用户，累计登录天数+1；
- 累计30天登录天数：需要获取30天之前的登录时间，如果30天之前登录了，数据需要-1，然后今天登录了+1，简单粗暴的就是直接重新计算最近30天登录数量。

其他字段更新思路一致。
![20211222091219](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211222091219.png)

#### 商品主题宽表

商品主题宽表中需要有全量的sku。

表的初始化，我们根据字段，去原始表中获取所有的数据，然后使用子查询全量导入查询。

表中每一天的更新逻辑，首先获取所有的历史数据，然后获取最近一天商品的各种行为，然后通过join两张表，更新累计值。

#### 活动主题宽表

活动主题宽表拥有全量的维度数据。

一行数据表示一个活动的各种累计值，每个活动一行数据。字段来自活动维度表的字段，，当日的字段，再dws层可以直接统计，累计等字段，更新思路是，先获取表中的历史数据，然后获取到当天的各种累计值，然后相加。