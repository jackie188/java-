
<!-- TOC -->

- [离线数仓项目总结](#离线数仓项目总结)
  - [如何向别人说明你的项目](#如何向别人说明你的项目)
  - [你对数据仓库的理解](#你对数据仓库的理解)
  - [数据仓库为什么要分层](#数据仓库为什么要分层)
  - [如何考虑系统架构的设计或者技术选型](#如何考虑系统架构的设计或者技术选型)
  - [Hadoop项目经验](#hadoop项目经验)
    - [hdfs多目录存储](#hdfs多目录存储)
    - [集群数据均衡](#集群数据均衡)
    - [Hadoop支持LZO压缩配置](#hadoop支持lzo压缩配置)
    - [Hadoop基准测试](#hadoop基准测试)
    - [Hadoop参数调优](#hadoop参数调优)

<!-- /TOC -->

## 离线数仓项目总结

### 如何向别人说明你的项目

首先说明项目背景：做的是一个电商项目，后台的数据，使用的是电商系统的业务数据和日志数据。

### 你对数据仓库的理解

数据仓库是为企业提供决策支持的数据集合，数据仓库不像我们平时所使用的mysql业务数据库，业务数据库中的数据不是永久存储的，有固定的寿命，但是我们数据仓库中的数据是保存很长时间的，至少保存半年之一年的历史数据，所以有了历史数据，我们就可以多一条分析数据的维度，**时间**，我们可以通过分析随着时间的推移，用户的行为，用户的喜好，等等很多信息，为我们的推广，决策提供支持。

并且通过这些数据的分析，我们可以分析用户画像，报表信息，或者进行机器学习等模型的训练。

### 数据仓库为什么要分层

如果要我一句话说明的话，我会说：复杂的问题简单化，如何理解呢？

**清晰的数据结构：**

什么是清晰的数据结构，简单来说就是每一层的数据都有他自己的作用域，这样我们再使用数据的时候，可以更加方便的去定位。

**数据血缘关系的追踪：**

如果那我们的业务数据来举例，可能我们一张订单详情表中是由很多其他的维度表或者是事实表join得到的，如果我们不进行分层，那么随着数据量增长，我们很难分析表和表之间的关系，所以为了建立表之间的血缘关系，使用分层。

**减少重复开发：**

规范的数据分层，不但可以重用我们的中间数据，还可以减少计算，重用计算。

**屏蔽原始数据的异常**

这个很容易想到，采集到的数据不一定拿来就可以使用，再分层的过程中，我们会主键清洗掉异常的数据，有利于保护我们的数据安全性。

### 如何考虑系统架构的设计或者技术选型

**对于离线数仓**

通常采集日志数据使用flume.

采集业务数据使用sqoop。

使用消息队列kafka作为缓冲的组件。

数据的存储通常用分布式文件系统hdfs，可以存储大容量的数据。

计算引擎，通常使用hadoop或者spark。

**实时数仓**

再实际中，企业通常使用一套采集系统来采集日志数据和业务数据，中间通过kafaka组件对应离线系统和实时系统。

采集业务数据通常使用Flink cdc组件

计算组件使用Flink或者spark streamming


### Hadoop项目经验

#### hdfs多目录存储

在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为`file://${hadoop.tmp.dir}/dfs/data`，这个路径可以决定namenode和datanode存储数据的位置，若服务器有多个磁盘，必须对该参数进行修改。参数应修改为如下的值。

```java
<property>
    <name>dfs.datanode.data.dir</name>
  <value>
    file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4
  </value>
</property>
file://代表是一种协议
```
注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。

#### 集群数据均衡

**节点间数据均衡**

节点之间数据不均衡指的是，有的节点空间占用率达到80%，而有的节点空间占用率才30%，所以需要均衡节点之间的数据。

开启数据均衡命令：
```java
start-balancer.sh -threshold 10
```

开启这一个进程，那么默认就会对数据进行跨界点数据的转移，直到数据均衡为止。

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

停止数据均衡命令：`stop-balancer.sh`

停止数据均衡后，已经均衡的数据不会恢复。

**磁盘间数据均衡**

这个是hadoop3.x之后的新特性，在这之前是没有的。

比如新加一块磁盘，那么就需要这个命令，让各个磁盘的数据均衡。

（1）生成均衡计划（我们只有一块磁盘，不会生成计划）

hdfs diskbalancer -plan hadoop103(新加磁盘的节点，这个计划其实就是一个json文件)

（2）执行均衡计划

hdfs diskbalancer -execute hadoop103.plan.json(生成的执行计划)

（3）查看当前均衡任务的执行情况

hdfs diskbalancer -query hadoop103

（4）取消均衡任务

hdfs diskbalancer -cancel hadoop103.plan.json

如果是取消执行计划，那么已经均衡的数据，是不会再恢复的。

#### Hadoop支持LZO压缩配置

1. hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。
2. 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/
3. 同步hadoop-lzo-0.4.20.jar到集群中的所有节点。
4. core-site.xml增加配置支持LZO压缩

```java
<configuration>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```
同步配置文件到集群中其他的节点。

lzo压缩的长处是可以支持切片，但是是有条件的，需要建立索引。

建立索引

```java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
```

`com.hadoop.compression.lzo.DistributedLzoIndexer`这个类是lzo中专门用来建立索引的类。

#### Hadoop基准测试

1） 测试HDFS写性能

```java
测试内容：向HDFS集群写10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:             Date & time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:       Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:      Test exec time sec: 67.61

```

2）测试HDFS读性能

```java
测试内容：读取HDFS集群10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB

2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date & time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test exec time sec: 53.61
```
3）删除测试生成数据
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

4）使用Sort程序评测MapReduce

（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
```
（2）执行Sort程序
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
```
（3）验证数据是否真正排好序了
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

> hadoop的基准测试：读，写，删除，性能

#### Hadoop参数调优

**HDFS参数调优hdfs-site.xml**

```
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
```

NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。当Datanode启动之后，会一直和namenode保持心跳，并且发送元数据信息。

对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
```xml
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```
dfs.namenode.handler.count=20×log_e^(Cluster Size)，比如集群规模为8台时，此参数设置为41。

**YARN参数调优yarn-site.xml**

（1）情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

（2）解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，**单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小**。调节这两个参数能提高系统内存的利用率。

（a）yarn.nodemanager.resource.memory-mby

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

（b）yarn.scheduler.maximum-allocation-mb

单个任务可申请的最多物理内存量，默认是8192（MB）。

