
<!-- TOC -->

- [离线数仓项目总结](#离线数仓项目总结)
  - [如何向别人说明你的项目](#如何向别人说明你的项目)
  - [你对数据仓库的理解](#你对数据仓库的理解)
  - [数据仓库为什么要分层](#数据仓库为什么要分层)
  - [如何考虑系统架构的设计或者技术选型](#如何考虑系统架构的设计或者技术选型)
  - [Hadoop项目经验](#hadoop项目经验)
    - [hdfs多目录存储](#hdfs多目录存储)
    - [集群数据均衡](#集群数据均衡)
    - [Hadoop支持LZO压缩配置](#hadoop支持lzo压缩配置)
    - [Hadoop基准测试](#hadoop基准测试)
    - [Hadoop参数调优](#hadoop参数调优)
  - [Kafka](#kafka)
    - [Kafka常用命令](#kafka常用命令)
    - [kafka压力测试](#kafka压力测试)
      - [Kafka Producer压力测试](#kafka-producer压力测试)
      - [Kafka Consumer压力测试](#kafka-consumer压力测试)
    - [Kafka机器数量计算](#kafka机器数量计算)
    - [Kafka分区数计算](#kafka分区数计算)
  - [Flume](#flume)
    - [方案对比](#方案对比)
    - [flume组件选型](#flume组件选型)
    - [flume结构](#flume结构)
    - [Flume拦截器](#flume拦截器)
  - [消费kafka的Flume](#消费kafka的flume)
    - [方案选型](#方案选型)
    - [组件选型](#组件选型)
      - [FileChannel和MemoryChannel区别](#filechannel和memorychannel区别)
      - [选型](#选型)
      - [FileChannel优化](#filechannel优化)
      - [Sink](#sink)
    - [数据压缩](#数据压缩)
  - [数据采集小结](#数据采集小结)
  - [业务数据采集平台](#业务数据采集平台)
    - [两个重要概念](#两个重要概念)
    - [表ER模型图](#表er模型图)

<!-- /TOC -->

## 离线数仓项目总结

### 如何向别人说明你的项目

首先说明项目背景：做的是一个电商项目，后台的数据，使用的是电商系统的业务数据和日志数据。

### 你对数据仓库的理解

数据仓库是为企业提供决策支持的数据集合，数据仓库不像我们平时所使用的mysql业务数据库，业务数据库中的数据不是永久存储的，有固定的寿命，但是我们数据仓库中的数据是保存很长时间的，至少保存半年之一年的历史数据，所以有了历史数据，我们就可以多一条分析数据的维度，**时间**，我们可以通过分析随着时间的推移，用户的行为，用户的喜好，等等很多信息，为我们的推广，决策提供支持。

并且通过这些数据的分析，我们可以分析用户画像，报表信息，或者进行机器学习等模型的训练。

### 数据仓库为什么要分层

如果要我一句话说明的话，我会说：复杂的问题简单化，如何理解呢？

**清晰的数据结构：**

什么是清晰的数据结构，简单来说就是每一层的数据都有他自己的作用域，这样我们再使用数据的时候，可以更加方便的去定位。

**数据血缘关系的追踪：**

如果那我们的业务数据来举例，可能我们一张订单详情表中是由很多其他的维度表或者是事实表join得到的，如果我们不进行分层，那么随着数据量增长，我们很难分析表和表之间的关系，所以为了建立表之间的血缘关系，使用分层。

**减少重复开发：**

规范的数据分层，不但可以重用我们的中间数据，还可以减少计算，重用计算。

**屏蔽原始数据的异常**

这个很容易想到，采集到的数据不一定拿来就可以使用，再分层的过程中，我们会主键清洗掉异常的数据，有利于保护我们的数据安全性。

### 如何考虑系统架构的设计或者技术选型

**对于离线数仓**

通常采集日志数据使用flume.

采集业务数据使用sqoop。

使用消息队列kafka作为缓冲的组件。

数据的存储通常用分布式文件系统hdfs，可以存储大容量的数据。

计算引擎，通常使用hadoop或者spark。

**实时数仓**

再实际中，企业通常使用一套采集系统来采集日志数据和业务数据，中间通过kafaka组件对应离线系统和实时系统。

采集业务数据通常使用Flink cdc组件

计算组件使用Flink或者spark streamming


### Hadoop项目经验

#### hdfs多目录存储

在hdfs-site.xml文件中配置多目录，注意新挂载磁盘的访问权限问题。

HDFS的DataNode节点保存数据的路径由dfs.datanode.data.dir参数决定，其默认值为`file://${hadoop.tmp.dir}/dfs/data`，这个路径可以决定namenode和datanode存储数据的位置，若服务器有多个磁盘，必须对该参数进行修改。参数应修改为如下的值。

```java
<property>
    <name>dfs.datanode.data.dir</name>
  <value>
    file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4
  </value>
</property>
file://代表是一种协议
```
注意：每台服务器挂载的磁盘不一样，所以每个节点的多目录配置可以不一致。单独配置即可。

#### 集群数据均衡

**节点间数据均衡**

节点之间数据不均衡指的是，有的节点空间占用率达到80%，而有的节点空间占用率才30%，所以需要均衡节点之间的数据。

开启数据均衡命令：
```java
start-balancer.sh -threshold 10
```

开启这一个进程，那么默认就会对数据进行跨界点数据的转移，直到数据均衡为止。

对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

停止数据均衡命令：`stop-balancer.sh`

停止数据均衡后，已经均衡的数据不会恢复。

**磁盘间数据均衡**

这个是hadoop3.x之后的新特性，在这之前是没有的。

比如新加一块磁盘，那么就需要这个命令，让各个磁盘的数据均衡。

（1）生成均衡计划（我们只有一块磁盘，不会生成计划）

hdfs diskbalancer -plan hadoop103(新加磁盘的节点，这个计划其实就是一个json文件)

（2）执行均衡计划

hdfs diskbalancer -execute hadoop103.plan.json(生成的执行计划)

（3）查看当前均衡任务的执行情况

hdfs diskbalancer -query hadoop103

（4）取消均衡任务

hdfs diskbalancer -cancel hadoop103.plan.json

如果是取消执行计划，那么已经均衡的数据，是不会再恢复的。

#### Hadoop支持LZO压缩配置

1. hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。
2. 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/
3. 同步hadoop-lzo-0.4.20.jar到集群中的所有节点。
4. core-site.xml增加配置支持LZO压缩

```java
<configuration>
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```
同步配置文件到集群中其他的节点。

lzo压缩的长处是可以支持切片，但是是有条件的，需要建立索引。

建立索引

```java
hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
```

`com.hadoop.compression.lzo.DistributedLzoIndexer`这个类是lzo中专门用来建立索引的类。

#### Hadoop基准测试

1） 测试HDFS写性能

```java
测试内容：向HDFS集群写10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:             Date & time: Thu Apr 16 13:41:24 CST 2020
2020-04-16 13:41:24,724 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:       Throughput mb/sec: 8.88
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:  Average IO rate mb/sec: 8.96
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:   IO rate std deviation: 0.87
2020-04-16 13:41:24,725 INFO fs.TestDFSIO:      Test exec time sec: 67.61

```

2）测试HDFS读性能

```java
测试内容：读取HDFS集群10个128M的文件
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB

2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read
2020-04-16 13:43:38,858 INFO fs.TestDFSIO:   Date & time: Thu Apr 16 13:43:38 CST 2020
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:         Number of files: 10
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:  Total MBytes processed: 1280
2020-04-16 13:43:38,859 INFO fs.TestDFSIO:       Throughput mb/sec: 85.54
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:  Average IO rate mb/sec: 100.21
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:   IO rate std deviation: 44.37
2020-04-16 13:43:38,860 INFO fs.TestDFSIO:      Test exec time sec: 53.61
```
3）删除测试生成数据
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

4）使用Sort程序评测MapReduce

（1）使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
```
（2）执行Sort程序
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
```
（3）验证数据是否真正排好序了
```java
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```

> hadoop的基准测试：读，写，删除，性能

#### Hadoop参数调优

**HDFS参数调优hdfs-site.xml**

```
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.
```

NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。当Datanode启动之后，会一直和namenode保持心跳，并且发送元数据信息。

对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。
```xml
<property>
    <name>dfs.namenode.handler.count</name>
    <value>10</value>
</property>
```
dfs.namenode.handler.count=20×log_e^(Cluster Size)，比如集群规模为8台时，此参数设置为41。

**YARN参数调优yarn-site.xml**

（1）情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

（2）解决办法：

内存利用率不够。这个一般是Yarn的2个配置造成的，**单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小**。调节这两个参数能提高系统内存的利用率。

（a）yarn.nodemanager.resource.memory-mby

表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

（b）yarn.scheduler.maximum-allocation-mb

单个任务可申请的最多物理内存量，默认是8192（MB）。

### Kafka

#### Kafka常用命令

**查看Kafka Topic列表**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
```
**创建Kafka Topic**

进入到/opt/module/kafka/目录下创建日志主题

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka  --create --replication-factor 1 --partitions 1 --topic topic_log
```

**删除Kafka Topic**

```java
bin/kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log
```

**Kafka生产消息**

```java
bin/kafka-console-producer.sh \
--broker-list hadoop102:9092 --topic topic_log
>hello world
>rzf  rzf
```

**Kafka消费消息**
```java
bin/kafka-console-consumer.sh \
--bootstrap-server hadoop102:9092 
--from-beginning 
--topic topic_log
--from-beginning：
```
会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。

**查看Kafka Topic详情**

```java
bin/kafka-topics.sh --zookeeper hadoop102:2181/kafka \
--describe --topic topic_log
```

#### kafka压力测试

用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 

```java
kafka-consumer-perf-test.sh
kafka-producer-perf-test.sh
```
##### Kafka Producer压力测试

1. 在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下

```java
bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
```
- record-size是一条信息有多大，单位是字节。
- num-records是总共发送多少条信息。
- throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。

> throughput设置为具体的值，测量的因为消息总数确定，所以测出的是延迟，但是如果值为-1，那么测出的是最高的速率。

kafka会打印下面消息

```java
100000 records sent, 95877.277085 records/sec (9.14 MB/sec), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.
```
参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。

##### Kafka Consumer压力测试

Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。

```java
bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```
参数说明：

--zookeeper 指定zookeeper的链接信息

--topic 指定topic的名称

--fetch-size 指定每次fetch的数据的大小

--messages 总共要消费的消息个数

测试结果说明：
```java
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153
```
开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。


#### Kafka机器数量计算

Kafka机器数量（经验公式）=2`*`（峰值生产速度`*`副本数/100）+1先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。

比如我们的峰值生产速度是50M/s。副本数为2。

Kafka机器数量=2*（50*2/100）+ 1=3台

> 峰值速度一般需要公司提供。

#### Kafka分区数计算

topic一般不需要计算，一个topic一般是一类数据。

1. 创建一个只有1个分区的topic
2. 测试这个topic的producer吞吐量和consumer吞吐量。
3. 假设他们的值分别是Tp和Tc，单位可以是MB/s。
4. 然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）


例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；

分区数=100 / 20 =5分区

`https://blog.csdn.net/weixin_42641909/article/details/89294698`

> 分区数一般设置为：3-10个


### Flume

#### 方案对比

本项目中，日志采集结构设计是：

flume--->kafka--->flume

第一个flume:
- source:Taildir Source采集日志文件
- channel:kafka channel,将数据写入kafka集群中

第二个flume:
- source:kafka source消费kafka中的数据
- channel:memory channel。
- sink:hdfs sink写入hdfs中。


第二种方案：

采用一个flume:

- source:采用Taildir Source采集日志文件。
- channle:采用的是kafka channel,将数据写入kafka集群当中。
- sink:采用hdfs sink方式，将数据写入hdfs中。

> 向kafka channel中写入数据其实是向kafka 集群中写。然后flume的sink在将数据从kafka集群中将数据读出来，写入hdfs中，这个过程需要经过网络的传输。

flume source-->kafka 集群---> flume sink---> hdfs

分析：

第一种方案简洁，向kafka集群中写入一次数据，然后再读取一次，很方便。

第二种设计结构很不好，压力比较大，首先我们的一个flume需要部署到日志服务器上面。日志服务器需要接收客户端埋点发送过来的日志数据，也就是所有数据先要写入日志服务器上，然后flume还需要读取日志向kafka集群写入一次，这个过程需要经过网络的传输，然后flume sink还需要去kafka集群读取一次数据，也就是数据还要从kafka集群写回flume一次，还要经过网络传输，最后再写入hdfs中，数据是两斤两处，对于日志服务器节点，压力比较大。

而第一种方案，把flume和kafka集群分散开，对日志服务器压力很小。

#### flume组件选型

**Source**

1. Taildir Source相比Exec Source、Spooling Directory Source的优势,这几个都是采集文件的source.


TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传，继承了Exec Source实时传输数据和Spooling Directory Source断点徐传的优点。

Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失，也即是相当于重新执行了命令，但是再脚本挂掉期间，产生的数据会丢失。可以执行一个脚本，去事实的监控一个数据文件。

Spooling Directory Source监控目录，支持断点续传。原理是将写好的文件放到这个监控目录中，然后阿静可以将文件中的内容一习性收集完。但是缺点是只要这个文件放到这个监控目录中之后，就不能修改文件的名字和追加内容，这是其局限性，所以不能事实的采集数据，所以再事实性监控，我们使用dlik cdc。

如果真要使用Spooling Directory Source实时监控数据，那么只能让文件小一点，比如几秒钟文东一个文件，尽可能的去模拟实时的场景，但是延迟高。

2. batchSize大小如何设置？
答：Event 1K左右时，500-1000合适（默认为100），也即是向channel中放数据的时候，一次性放多少个。

**Channel**

采用Kafka Channel，直接将数据写入kafka集群，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

> bug：注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。


#### flume结构

![20211212130648](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212130648.png)


#### Flume拦截器

flume拦截器属于flume的source中一个组件。使用拦截器，可以做日志的过滤或者分类。在这里做日志的清晰，清除不合法的日志。

1. 自定义类实现Interceptor 接口。
2. 实现initialize()初始化方法。
3. 实现Event intercept(Event event)拦截方法，判断一个事件是否是合法的。
4. 实现List<Event> intercept(List<Event> list)方法，这个方法是对上面第三个方法的封装，

在这里，对字符串的判断，使用的是阿里巴巴的fastJson对json字符串进行校验。

在这里创建拦截器对象使用的是builder创建者模式，自定义创建者实现Interceptor.Builder类，重写下面方法：

1. Interceptor build()创建一个拦截器对象。
2. void configure(Context context)配置信息。

### 消费kafka的Flume


#### 方案选型

消费kafka中数据的Flume有两种配置方案：
1. 配置一个完整的flume，包括kafka source，channel和hdfs sink。
2. 配置一个kafka channel省去了source组件，然后再配置一个hdfs sink，这两种方案都可以。

但是这里也存在问题，flume sink将数据写入hdfs的时候，如何去写，因为我们离线数仓是一种批处理，那么这个一批指的是一天的数据，一次性需要计算一条的数据，所以再这里存储数据的时候，最好是按照分天进行存储，所以路径应该以天为单位。

但是实现这种方案，需要我们对event添加一个时间戳，由于flume默认会用linux系统时间（如果和flink类比的话，应该是摄入事件），作为输出到HDFS路径的时间。如果数据是23:59分产生的。Flume消费kafka里面的数据时，有可能已经是第二天了，那么这部门数据会被发往第二天的HDFS路径。我们希望的是根据日志里面的实际时间（也就是事件事件），发往HDFS的路径，所以下面拦截器作用是获取日志中的实际时间。

在这里我们依然使用一个拦截器解决：

拦截器属于source端的组件，所以再上面的两种方案中，我们就只能使用第一种方案了。自定义拦截器，提取日志中的事件事件，然后把事件事件添加到event的header中，根据header中的事件时间创建文件夹写出数据。

在这里我么也可以使用第二种方案，我们把提取时间戳的拦截器放到第一个flume中，形成一个拦截器链，这样做的话，再写入kafka的时候，我们就需要保留evevt的头部信息，不能过滤掉。

自定义拦截器步骤请参考上文。

#### 组件选型

采用kafka source，再底层就是一个kafka的消费者，

##### FileChannel和MemoryChannel区别

MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。

FileChannel传输速度相对于Memory慢，但数据安全保障高,Agent进程挂掉也可以从失败中恢复数据。

为什么FileChannel不会发生数据的丢失：

![20211212140412](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212140412.png)

可以看到底层的数据结构，真实的数据存储再磁盘文件中，但是还是有一个内存队列的，这个内存队列相当于是对磁盘上的文件建立一个索引，记录了消费到哪里。

虽然磁盘上的数据不会丢失，但是内存队列中的数据断电就会丢失，所以再内存队列之外，还有一个快照文件，这个快照文件可以看作是内存队列的快照，只要内存队列发生变化，这个快照文件就会跟着变化。这样就保证数据的安全性。

如果消费者挂点，那么就可以兄快照恢复，然后恢复读取的位置，重新消费。

##### 选型

金融类公司、对钱要求非常准确的公司通常会选择FileChannel
传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。

##### FileChannel优化

通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。

checkpointDir(做快照的文件路径，保存内存索引)和backupCheckpointDir（检查点备份文件路径）也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据

批量指的是再将数据写入putlist事务列表的时候，事务列表的大小，再提交事务的时候，put事务会先检查一些channel是否有足够的空间存放数据，如果没有的话，就会立刻回滚数据，然后source会清空putlist列表，再次去拉去数据写入putlist中，但是如果立刻回滚事务，很消耗性能，所以就产生了keep_alive参数，等待几秒钟，再次提交事务，如果还没有成功，就进行回滚。

##### Sink

HDFS Sink
1. HDFS存入大量小文件，有什么影响？


元数据层面：每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命

计算层面：默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。

2. HDFS小文件处理

官方默认的这三个参数配置写入HDFS后会产生小文件，
`hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount`

基于以上：

hdfs.rollInterval=3600：1小时生成一个新文件

hdfs.rollSize=134217728：128m生成一个新文件

hdfs.rollCount =0：多少个event生成一个新文件

几个参数综合作用，效果如下：
1. 文件在达到128M时会滚动生成新文件
2. 文件创建超3600秒时会滚动生成新文件

#### 数据压缩

再写入hdfs上的文件，我么也可以进行压缩处理，使用lzo压缩。

flume目前支持三种：

二进制文件：SequenceFile。

DataStream:不做任何处理。

Compress:压缩文件，目前支持：gzip,bzip2,lzo,snappy。


### 数据采集小结


web/app---> 日志服务器--->Flume--->kafka--->flume--->hdfs

再实际中，日志服务器可能部署再大数据集群中，也可能不在。

再第一个Flume中，有三种实现方案。

第二个flume中，也有三种实现方案。

> 思考为什么中间需要一个kafka集群？

### 业务数据采集平台

业务数据是使用sqoop将数据导入到hdfs上面，对于业务数据，重要的是数据业务表之间的联系。

#### 两个重要概念

电商常识（SKU、SPU）

SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。

SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。

例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。

SPU表示一类商品。好处就是：可以共用商品图片，海报、销售属性等。

spu描述的粒度更粗，sku描述一件具体商品，粒度细。

#### 表ER模型图

本项目中设计24张表结构。

![20211212144449](https://vscodepic.oss-cn-beijing.aliyuncs.com/pic/20211212144449.png)

