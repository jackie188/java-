# 随机森林

## 集成算法概述

集成学习（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型的建模结果。基本上所有的机器学习领域都可以看到集成学习的身影，在现实中集成学习也有相当大的作用，它可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预测疾病的风险和病患者的易感性。在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成算法的身影也随处可见，可见其效果之好，应用之广。

**集成算法目标**

集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。

多个模型集成成为的模型叫做集成评估器（ensemble estimator），组成集成评估器的每个模型都叫做**基评估器**（base estimator）。通常来说，有三类集成算法：装袋法（Bagging），提升法（Boosting）和stacking。

![1632531817845](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/090339-232083.png)

装袋法（Bagging）的核心思想是构建多个**相互独立**的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结
果。装袋法的代表模型就是**随机森林**。

提升法（Boosting）中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。

## sklearn中的集成算法

sklearn中的集成算法模块ensemble：

![1632532210176](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/091011-439603.png)

集成算法中，有一半以上都是树的集成模型，可以想见决策树在集成中必定是有很好的效果。可以看看上一篇决策树笔记:[决策树](https://blog.csdn.net/qq_38163244/article/details/120454021)

> 决策树的核心问题有两个，**一个是如何找出正确的特征来进行提问，即如何分枝，二是树生长到什么时候应该停**
> **下。**
>
> - 对于第一个问题，我们定义了用来衡量分枝质量的指标不纯度，分类树的不纯度用**基尼系数或信息熵**来衡量，回归
>   树的不纯度用**MSE均方误差**来衡量。每次分枝时，决策树对所有的特征进行不纯度计算，选取不纯度最低的特征进
>   行分枝，分枝后，又再对被分枝的不同取值下，计算每个特征的不纯度，继续选取不纯度最低的特征进行分枝。

![1632532366353](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/091247-314979.png)

- 每分枝一层，树整体的不纯度会越来越小，决策树追求的是最小不纯度。因此，决策树会一直分枝，直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。
- 决策树非常容易过拟合，这是说，它很容易在训练集上表现优秀，却在测试集上表现很糟糕。为了防止决策树的过拟合，我们要对决策树进行剪枝，sklearn中提供了大量的剪枝参数。

**sklearn中的建模流程**

![1632532424737](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/091346-281047.png)

## RandomForestClassifier

~~~ python
class sklearn.ensemble.RandomForestClassifier (n_estimators=’10’, criterion=’gini’, max_depth=None,
min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’,
max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False,
n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)
~~~

随机森林是非常具有代表性的Bagging集成算法，它的所有基评估器都是决策树，分类树组成的森林就叫做随机森林分类器，回归树所集成的森林就叫做随机森林回归器。

### 重要参数

#### 控制基评估器的参数

![1632532644105](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/091725-350445.png)

这几个参数的用法，可以查看决策树中的用法，基本一致：[决策树](https://blog.csdn.net/qq_38163244/article/details/120454021)

##### n_estimators

这是森林中树木的数量，即基基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。

n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。

### 案例

树模型的优点是简单易懂，可视化之后的树人人都能够看懂，可惜随机森林是无法被可视化的。所以为了更加直观地让大家体会随机森林的效果，我们来进行一个随机森林和单个决策树效益的对比。我们依然使用红酒数据集。

**代码示例**

~~~ python
# 导入包
%matplotlib inline
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine

# 加载数据集
wine=load_wine()
wine.data.shape

# 实例化模型
# 使用训练集将实例化后的模型进行训练，使用的接口是fit()
# 使用其他的接口将测试集中的数据进行测试，然后对模型进行评估获取结果(score，y_test)

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(wine.data,wine.target,test_size=0.3)
# 生成决策树模型
clf=DecisionTreeClassifier(random_state=0)
# 生成随机森林
rfc=RandomForestClassifier(random_state=0)
# 训练模型
clf=clf.fit(xtrain,ytrain)
rfc=rfc.fit(xtrain,ytrain)

# 对我们的墨子那个进行打分
score_clf=clf.score(xtest,ytest)
score_rfc=rfc.score(xtest,ytest)
~~~

下面我们在不同的数据集上做训练，来验证我们的模型的稳定性，要使用不同的数据集，所以在这里我们选择交叉验证的方法，把我们的红酒数据集分10份，每一次使用一份做测试集，另外9分做训练集。

~~~ python
# 验证在不同的数据集上，我们训练的模型的稳定性，使用交叉验证划分我们的数据集
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

rfc=RandomForestClassifier(n_estimators=25)
rfc_s=cross_val_score(rfc,wine.data,wine.target,cv=10)

clf=DecisionTreeClassifier()
clf_s=cross_val_score(clf,wine.data,wine.target,cv=10)

plt.plot(range(1,11),rfc_s,label="randomforest")
plt.plot(range(1,11),clf_s,label="DecisionTree")
plt.legend()
plt.show()
~~~

最终画出在交叉验证下我们的学习曲线，可以看到，随机森林的效果明显好于决策树。

![1632535441644](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/100403-153498.png)

下面我们做一下n_estimators的学习曲线：

~~~ python
# 使用学习曲线
# n_estimators的学习曲线
superpa = []
for i in range(200):
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1)
    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean()
    superpa.append(rfc_s)
print(max(superpa),superpa.index(max(superpa)))
plt.figure(figsize=[20,5])
plt.plot(range(1,201),superpa)
plt.show()
~~~

**学习结果**

![1632535624073](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/100706-619293.png)

可以看到在n_estimators为19的时候效果最好，所以我们可以在19附近更加一步的细化我们的学习曲线。

下面我们增大数据集量，画出随机森林和决策树在10组交叉验证下的稳定性对比图，每一组做10次，相当于做100次训练。

~~~ python
#交叉验证：是数据集划分为n分，依次取每一份做测试集，每n-1份做训练集，多次训练模型以观测模型稳定性的方法
# 下面相当于做100次交叉验证
rfc_l = []
clf_l = []
for i in range(10):
    rfc = RandomForestClassifier(n_estimators=25)
    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean()
    rfc_l.append(rfc_s)
    clf = DecisionTreeClassifier()
    clf_s = cross_val_score(clf,wine.data,wine.target,cv=10).mean()
    clf_l.append(clf_s)
plt.plot(range(1,11),rfc_l,label = "Random Forest")
plt.plot(range(1,11),clf_l,label = "Decision Tree")
plt.legend()
plt.show()
#是否有注意到，单个决策树的波动轨迹和随机森林一致？
#再次验证了我们之前提到的，单个决策树的准确率越高，随机森林的准确率也会越高
~~~

上面代码中，每一组交叉验证的最后，我们取出平均值，最后的学习曲线如下：

![1632535766438](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/25/100927-740168.png)

### 重要属性和接口

随机森林中有三个非常重要的属性：`.estimators_`，`oob_score_`以及`.feature_importances_`。

- `.estimators_`是用来查看随机森林中所有树的列表的。
- `oob_score_`指的是袋外得分。随机森林为了确保森林中的每棵树都不尽相同，所以采用了对训练集进行**有放回抽样**的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score来导出，本质还是模型的精确度。
- 而`.feature_importances_`和决策树中的`.feature_importances_`用法和含义都一致，是返回特征的重要性。

随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。

**实例**

~~~ python
rfc = RandomForestClassifier(n_estimators=25)
rfc = rfc.fit(xtrain, ytrain)
rfc.score(xtest,ytest)

# 返回灭一个特征的重要程度
rfc.feature_importances_

# 返回某一个样本在某一树中所在叶子节点的索引号
rfc.apply(xtest)

# 输出预测的分类结果，真正的结果是ytest
rfc.predict(xtest)

#返回每一个样本被分为某一类的概率
rfc.predict_proba(xtest)
~~~

## RandomForestRegressor

### 方法说明

~~~ python
class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)
~~~

所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。

criterion不纯度指标不同：

- 分类树使用的是基尼系数和信息熵
- 回归树使用的是MSE均方误差，

模型的衡量指标不同：

- 分类树和随机森林分类器使用的是accuracy准确率。
- 回归树或者随机森林回归器使用的是R的平方（越接近1越好）或者MSE（越小越好）。

### 重要参数，属性与接口

**criterion**

回归树衡量分枝质量的指标，支持的标准有三种：

1. 输入"mse"使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失
2. 输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差
3. 输入"mae"使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失。

![1632628667954](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/115749-144844.png)

其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。

在回归树中，MSE不只是我们的**分枝质量衡量指标**，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下：

![1632628736764](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/115857-55777.png)

其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。

值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。

- R的平方的范围是1到负无穷，越接近1效果越好。

### 重要属性和接口

最重要的属性和接口，都与随机森林的分类器相一致，还是apply, fit, predict和score最为核心。值得一提的是，随机森林回归并没有predict_proba这个接口，因为对于回归来说，并不存在一个样本要被分到某个类别的概率问题，因此没有predict_proba这个接口。

**随机森林的用法**

和决策树完全一致，除了多了参数n_estimators。

~~~ python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import sklearn

boston=load_boston()
regressor=RandomForestRegressor(n_estimators=100,random_state=0)
# 在这里如果不写scoring，那么默认返回的是R的平方，写上的话使用的是负均方误差
cross_val_score(regressor,boston.data,boston.target,cv=10,scoring = "neg_mean_squared_error")
# 可以返回模型所有的打分指标
sorted(sklearn.metrics.SCORERS.keys())

~~~

返回十次交叉验证的结果，注意在这里，如果不填写scoring = "neg_mean_squared_error"，交叉验证默认的模型衡量指标是R平方，因此交叉验证的结果可能有正也可能有负。而如果写上scoring，则衡量标准是负MSE，交叉验证的结果只可能为负。

### 用随机森林回归填补缺失值

我们从现实中收集的数据，几乎不可能是完美无缺的，往往都会有一些缺失值。面对缺失值，很多人选择的方式是直接将含有缺失值的样本删除，这是一种有效的方法，但是有时候填补缺失值会比直接丢弃样本效果更好，即便我们其实并不知道缺失值的真实样貌。在sklearn中，我们可以使用sklearn.impute.SimpleImputer来轻松地将均值，中值，或者其他最常用的数值填补到数据中，在这个案例中，我们将使用均值，0，和随机森林回归来填补缺失值，并验证四种状况下的拟合状况，找出对使用的数据集来说最佳的缺失值填补方法。

**导入需要的库**

~~~ python
# 导入需要的库
# 用随机森林回归填补缺失值
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
~~~

**以波士顿数据集为例，导入完整的数据集并探索**

~~~ python
# 加载数据
dataset = load_boston()
dataset.data.shape
#总共506*13=6578个数据\
# 获取数据和标签
x_full,y_full=dataset.data,dataset.target
n_samples=x_full.shape[0]
n_features=x_full.shape[1]
~~~

**为完整数据集放入缺失值**

~~~python
#首先确定我们希望放入的缺失数据的比例，在这里我们假设是50%，那总共就要有3289个数据缺失
# 确认随机数种子
rng = np.random.RandomState(0)
# 缺失数据率是0.5
missing_rate = 0.5
n_missing_samples = int(np.floor(n_samples * n_features * missing_rate))
#np.floor向下取整，返回.0格式的浮点数
#所有数据要随机遍布在数据集的各行各列当中，而一个缺失的数据会需要一个行索引和一个列索引
#如果能够创造一个数组，包含3289个分布在0~506中间的行索引，和3289个分布在0~13之间的列索引，那我们就可
# 以利用索引来为数据中的任意3289个位置赋空值
#然后我们用0，均值和随机森林来填写这些缺失值，然后查看回归的结果如何
# randint 在上线和下限取出若干个整数，第一个是下限，第二个参数是上限
missing_features = rng.randint(0,n_features,n_missing_samples)
missing_samples = rng.randint(0,n_samples,n_missing_samples)

#missing_samples = rng.choice(dataset.data.shape[0],n_missing_samples,replace=False)
#我们现在采样了3289个数据，远远超过我们的样本量506，所以我们使用随机抽取的函数randint。但如果我们需要
# 的数据量小于我们的样本量506，那我们可以采用np.random.choice来抽样，choice会随机抽取不重复的随机数，
# 因此可以帮助我们让数据更加分散，确保数据不会集中在一些行中

X_missing = x_full.copy()
y_missing = y_full.copy()

X_missing[missing_samples,missing_features] = np.nan
X_missing = pd.DataFrame(X_missing)
#转换成DataFrame是为了后续方便各种操作，numpy对矩阵的运算速度快到拯救人生，但是在索引等功能上却不如
# pandas来得好用
~~~

**使用0和均值填补缺失值**

~~~python
# 使用0和均值填补缺失值
#使用均值进行填补
from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
X_missing_mean = imp_mean.fit_transform(X_missing) # 相当于fit+predict
# 检测是否全部填充
pd.DataFrame(X_missing_mean).isnull().sum(axis=0)

#使用0进行填补
imp_0 = SimpleImputer(missing_values=np.nan, strategy="constant",fill_value=0)
X_missing_0 = imp_0.fit_transform(X_missing)
pd.DataFrame(X_missing_0)
~~~

**使用随机森林填补缺失值**

~~~ python
# 使用随机森林填补缺失值
"""
使用随机森林回归填补缺失值
任何回归都是从特征矩阵中学习，然后求解连续型标签y的过程，之所以能够实现这个过程，是因为回归算法认为，特征
矩阵和标签之前存在着某种联系。实际上，标签和特征是可以相互转换的，比如说，在一个“用地区，环境，附近学校数
量”预测“房价”的问题中，我们既可以用“地区”，“环境”，“附近学校数量”的数据来预测“房价”，也可以反过来，
用“环境”，“附近学校数量”和“房价”来预测“地区”。而回归填补缺失值，正是利用了这种思想。
对于一个有n个特征的数据来说，其中特征T有缺失值，我们就把特征T当作标签，其他的n-1个特征和原本的标签组成新
的特征矩阵。那对于T来说，它没有缺失的部分，就是我们的Y_test，这部分数据既有标签也有特征，而它缺失的部
分，只有特征没有标签，就是我们需要预测的部分。
特征T不缺失的值对应的其他n-1个特征 + 本来的标签：X_train
特征T不缺失的值：Y_train
特征T缺失的值对应的其他n-1个特征 + 本来的标签：X_test
特征T缺失的值：未知，我们需要预测的Y_test
这种做法，对于某一个特征大量缺失，其他特征却很完整的情况，非常适用。
那如果数据中除了特征T之外，其他特征也有缺失值怎么办？
答案是遍历所有的特征，从缺失最少的开始进行填补（因为填补缺失最少的特征所需要的准确信息最少）。
填补一个特征时，先将其他特征的缺失值用0代替，每完成一次回归预测，就将预测值放到原本的特征矩阵中，再继续填
补下一个特征。每一次填补完毕，有缺失值的特征会减少一个，所以每次循环后，需要用0来填补的特征就越来越少。当
进行到最后一个特征时（这个特征应该是所有特征中缺失值最多的），已经没有任何的其他特征需要用0来进行填补了，
而我们已经使用回归为其他特征填补了大量有效信息，可以用来填补缺失最多的特征。
遍历所有的特征后，数据就完整，不再有缺失值了。
"""

# 获取缺失数据集
X_missing_reg = X_missing.copy()
# 对各个列缺失多少数据进行排序，这个排序的方法拍完序号之后带索引
sortindex = np.argsort(X_missing_reg.isnull().sum(axis=0)).values
sortindex

for i in sortindex:
    #构建我们的新特征矩阵和新标签(没有被选中的去填充的特征+原始的标签)和新标签（被选中去填充的特征）
    df = X_missing_reg
#     首先获取某一列
    fillc = df.iloc[:,i]
#     取出除了第i列之外的所有列
    df = pd.concat([df.iloc[:,df.columns != i],pd.DataFrame(y_full)],axis=1)
    #在新特征矩阵中，对含有缺失值的列，进行0的填补
#     实例化对象，执行下面之后。df_0里面全部使用0填充好了
    df_0 =SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=0).fit_transform(df)
    #找出我们的训练集和测试集
#     ytrain是被选中填充的特征中，存在的那些值
    Ytrain = fillc[fillc.notnull()]
#     ytest是被选中填充的特征中，为null的值，在这里，我们需要的是ytest的索引
    Ytest = fillc[fillc.isnull()]
    Xtrain = df_0[Ytrain.index,:]
    Xtest = df_0[Ytest.index,:]
    #用随机森林回归来填补缺失值
#     实例化对象
    rfc = RandomForestRegressor(n_estimators=100)
    rfc = rfc.fit(Xtrain, Ytrain)
    Ypredict = rfc.predict(Xtest)
    #将填补好的特征返回到我们的原始的特征矩阵中
    X_missing_reg.loc[X_missing_reg.iloc[:,i].isnull(),i] = Ypredict
~~~

**对填充好的数据进行建模**

~~~ python
# 对填充好的数据进行建模
#对所有数据进行建模，取得MSE结果
X = [x_full,X_missing_mean,X_missing_0,X_missing_reg]
mse = []
std = []
for x in X:
    estimator = RandomForestRegressor(random_state=0, n_estimators=100)
#     scoring:交叉验证中，分类和回归的打分标准不同
# 分类：accuracy
# 回归：均方误差
    scores = cross_val_score(estimator,x,y_full,scoring='neg_mean_squared_error',cv=5).mean()
    mse.append(scores * -1)
    
# mse越小越好
~~~

**绘图**

~~~ python
x_labels = ['Full data','Zero Imputation','Mean Imputation','Regressor Imputation']
colors = ['r', 'g', 'b', 'orange']
plt.figure(figsize=(12, 6))
ax = plt.subplot(111)# 添加子图
for i in np.arange(len(mse)):
    ax.barh(i, mse[i],color=colors[i], alpha=0.6, align='center')
ax.set_title('Imputation Techniques with Boston Data')
ax.set_xlim(left=np.min(mse) * 0.9,right=np.max(mse) * 1.1)
ax.set_yticks(np.arange(len(mse)))
ax.set_xlabel('MSE')
ax.set_yticklabels(x_labels)
plt.show()
~~~

![1632641921467](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/153843-736336.png)





## 机器学习调参思想

通过画学习曲线，或者网格搜索，我们能够探索到调参边缘（代价可能是训练一次模型要跑三天三夜），但是在现实中，高手调参恐怕还是多依赖于经验，而这些经验，来源于：

1. 非常正确的调参思路和方法
2. 对模型评估指标的理解
3. 对数据的感觉和经验

模型调参，第一步是要找准目标：我们要做什么？一般来说，这个目标是提升某个模型评估指标，比如对于随机森林来说，我们想要提升的是模型在未知数据上的准确率（由score或oob_score_来衡量）。找准了这个目标，我们就需要思考：模型在未知数据上的准确率受什么因素影响？

在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。

**泛化误差**

当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，**当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大**。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。

![1632637566792](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/142607-512197.png)

那模型的复杂度与我们的参数有什么关系呢？对树模型来说，树越茂盛，深度越深，枝叶越多，模型就越复杂。所以树模型是天生位于图的右上角的模型，随机森林是以树模型为基础，所以随机森林也是天生复杂度高的模型。随机森林的参数，都是向着一个目标去：减少模型的复杂度，把模型往图像的左边移动，防止过拟合。当然了，调参没有绝对，也有天生处于图像左边的随机森林，所以调参之前，我们要先判断，模型现在究竟处于图像的哪一边，总之我们要关注下面四点：

1. 模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点
2. 模型太复杂就会过拟合，模型太简单就会欠拟合
3. 对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂
4. 树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动

那具体每个参数，都如何影响我们的复杂度和模型呢？我们一直以来调参，都是在学习曲线上轮流找最优值，盼望能够将准确率修正到一个比较高的水平。然而我们现在了解了随机森林的调参方向：降低复杂度，我们就可以将那些对复杂度影响巨大的参数挑选出来，研究他们的单调性，然后专注调整那些能最大限度让复杂度降低的参数。对于那些不单调的参数，或者反而会让复杂度升高的参数，我们就视情况使用，大多时候甚至可以退避。

**各个参数的重要程度**

![1632637754249](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/142915-633111.png)

## 偏差和方差

一个集成模型(f)在未知数据集(D)上的泛化误差E(f;D)，由**方差(var)，偏差(bais)和噪声(ε)**共同决定。

![1632637815264](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/143016-402209.png)

> 偏差与方差
>
> 观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。
>
> 偏差：**模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离**。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。
>
> 方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。

![1632637852130](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/143053-839414.png)

**其中偏差衡量模型是否预测得准确，偏差越小，模型越“准”；而方差衡量模型每次预测的结果是否接近，即是说方差越小，模型越“稳”；**噪声是机器学习无法干涉的部分，一个好的模型，要对大多数未知数据都预测得”准“又”稳“。即是说，**当偏差和方差都很低的时候，模型的泛化误差就小，在未知数据上的准确率就高。**

![1632637961303](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/143242-142146.png)

通常来说，方差和偏差有一个很大，泛化误差都会很大。然而，方差和偏差是此消彼长的，不可能同时达到最小值。这个要怎么理解呢？来看看下面这张图：

![1632637993285](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/143315-280321.png)

从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。

相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。

我们调参的目标是，达到方差和偏差的完美平衡！虽然方差和偏差不能同时达到最小值，但他们组成的泛化误差却可以有一个最低点，而我们就是要寻找这个最低点。对复杂度大的模型，要降低方差，对相对简单的模型，要降低偏差。随机森林的基评估器都拥有较低的偏差和较高的方差，因为决策树本身是预测比较”准“，比较容易过拟合的模型，装袋法本身也要求基分类器的准确率必须要有50%以上。所以以随机森林为代表的装袋法的训练过程旨在降低方差，即降低模型复杂度，所以随机森林参数的默认设定都是假设模型本身在泛化误差最低点的右边。

所以，我们在降低复杂度的时候，本质其实是在降低随机森林的方差，随机森林所有的参数，也都是朝着降低方差的目标去。有了这一层理解，我们对复杂度和泛化误差的理解就更上一层楼了，对于我们调参，也有了更大的帮助。

## 随机森林在乳腺癌数据上的调参

**导入需要的库**

~~~ python
# 乳腺癌数据集上的调参
# 导入库
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
~~~

**加载数据**

~~~ python
data=load_breast_cancer()
data.data.shape
data.target.shape
~~~

**实例化模型**

~~~ python
# 实例化模型
rfc=RandomForestClassifier(n_estimators=100,random_state=90)
# 分类器默认使用准确度评估分数
score_pre=cross_val_score(rfc,data.data,data.target,cv=10).mean()
score_pre
# #这里可以看到，随机森林在乳腺癌数据上的表现本就还不错，在现实数据集上，基本上不可能什么都不调就看到95%以
# 上的准确率
~~~

**对n_estimators画出学习曲线**

~~~ python
"""
在这里我们选择学习曲线，可以使用网格搜索吗？可以，但是只有学习曲线，才能看见趋势
我个人的倾向是，要看见n_estimators在什么取值开始变得平稳，是否一直推动模型整体准确率的上升等信息
第一次的学习曲线，可以先用来帮助我们划定范围，我们取每十个数作为一个阶段，来观察n_estimators的变化如何
引起模型整体准确率的变化
"""
#####【TIME WARNING: 30 seconds】#####
scorel = []
for i in range(0,200,10):
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90)
    score = cross_val_score(rfc,data.data,data.target,cv=10).mean()
    scorel.append(score)
print(max(scorel),(scorel.index(max(scorel))*10)+1)
plt.figure(figsize=[20,5])
plt.plot(range(1,201,10),scorel)
plt.show()
#list.index([object])
#返回这个object在列表list中的索引
~~~

画学习曲线，我们可以不断的缩小范围，对学习曲线进行细化，直到找到一个不错的参数为止。

调整n_estimators的效果显著，模型的准确率立刻上升了0.005。接下来就进入网格搜索，我们将使用网格搜索对参数一个个进行调整。为什么我们不同时调整多个参数呢？原因有两个：1）同时调整多个参数会运行非常缓慢2）同时调整多个参数，会让我们无法理解参数的组合是怎么得来的，所以即便网格搜索调出来的结果不好，我们也不知道从哪里去改。在这里，为了使用复杂度-泛化误差方法（方差-偏差方法），我们对参数进行一个个地调整。

**书写网格搜索参数**

~~~ python
# 网格搜索
"""
有一些参数是没有参照的，很难说清一个范围，这种情况下我们使用学习曲线，看趋势从曲线跑出的结果中选取一个更小的区间，再跑曲线

param_grid = {'n_estimators':np.arange(0, 200, 10)}
param_grid = {'max_depth':np.arange(1, 20, 1)}
param_grid = {'max_leaf_nodes':np.arange(25,50,1)}

对于大型数据集，可以尝试从1000来构建，先输入1000，每100个叶子一个区间，再逐渐缩小范围,有一些参数是可以找到一个范围的，或者说我们知道他们的取值和随着他们的取值，模型的整体准确率会如何变化，这样的参数我们就可以直接跑网格搜索
param_grid = {'criterion':['gini', 'entropy']}
param_grid = {'min_samples_split':np.arange(2, 2+20, 1)}
param_grid = {'min_samples_leaf':np.arange(1, 1+10, 1)}
param_grid = {'max_features':np.arange(5,30,1)}
"""
~~~

通过各个参数对模型的影响程度大小开始调整

**调整max_depth**

~~~ python
#调整max_depth
param_grid = {'max_depth':np.arange(1, 20, 1)}
# 一般根据数据的大小来进行一个试探，乳腺癌数据很小，所以可以采用1~10，或者1~20这样的试探
# 但对于像digit recognition那样的大型数据来说，我们应该尝试30~50层深度（或许还不足够
# 更应该画出学习曲线，来观察深度对模型的影响
rfc = RandomForestClassifier(n_estimators=39,random_state=90)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)
GS.best_params_
GS.best_score_ #返回最佳参数对应的准确率
~~~

![1632640766117](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/152018-959898.png)

深度默认是最大的，也就是模型最复杂的情况，可以认为是总泛化误差最大的情况，所以我们减小深度。

在这里，我们注意到，将max_depth设置为有限之后，模型的准确率下降了。限制max_depth，是让模型变得简单，把模型向左推，而模型整体的准确率下降了，即整体的泛化误差上升了，这说明模型现在位于图像左边，即泛化误差最低点的左边（偏差为主导的一边）。通常来说，随机森林应该在泛化误差最低点的右边，树模型应该倾向于过拟合，而不是拟合不足。这和数据集本身有关，但也有可能是我们调整的n_estimators对于数据集来说太大，因此将模型拉到泛化误差最低点去了。然而，既然我们追求最低泛化误差，那我们就保留这个n_estimators，除非有其他的因素，可以帮助我们达到更高的准确率。

当模型位于图像左边时，我们需要的是增加模型复杂度（增加方差，减少偏差）的选项，因此max_depth应该尽量大，min_samples_leaf和min_samples_split都应该尽量小。这几乎是在说明，除了max_features，我们没有任何参数可以调整了，因为max_depth，min_samples_leaf和min_samples_split是剪枝参数，是减小复杂度的参数。在这里，我们可以预言，我们已经非常接近模型的上限，模型很可能没有办法再进步了。

那我们这就来调整一下max_features，看看模型如何变化。

**调整max_features**

~~~ python
#调整max_features
param_grid = {'max_features':np.arange(5,30,1)}
"""
max_features是唯一一个即能够将模型往左（低方差高偏差）推，也能够将模型往右（高方差低偏差）推的参数。我
们需要根据调参前，模型所在的位置（在泛化误差最低点的左边还是右边）来决定我们要将max_features往哪边调。
现在模型位于图像左侧，我们需要的是更高的复杂度，因此我们应该把max_features往更大的方向调整，可用的特征
越多，模型才会越复杂。max_features的默认最小值是sqrt(n_features)，因此我们使用这个值作为调参范围的
最小值。
"""
rfc = RandomForestClassifier(n_estimators=39,random_state=90)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)
GS.best_params_
GS.best_score_
~~~

![1632640766117](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/26/152018-959898.png)

网格搜索返回了max_features的最小值，可见max_features升高之后，模型的准确率降低了。这说明，我们把模型往右推，模型的泛化误差增加了。前面用max_depth往左推，现在用max_features往右推，泛化误差都增加，这说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。

如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式。但是在课上，出于练习和探索的目的，我们继续调整我们的参数，让大家观察一下模型的变化，看看我们预测得是否正确。依然按照参数对模型整体准确率的影响程度进行调参。

**调整min_samples_leaf**

~~~ python
#调整min_samples_leaf
param_grid={'min_samples_leaf':np.arange(1, 1+10, 1)}
#对于min_samples_split和min_samples_leaf,一般是从他们的最小值开始向上增加10或20
#面对高维度高样本量数据，如果不放心，也可以直接+50，对于大型数据，可能需要200~300的范围
#如果调整的时候发现准确率无论如何都上不来，那可以放心大胆调一个很大的数据，大力限制模型的复杂度
rfc = RandomForestClassifier(n_estimators=39
,random_state=90
)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)
GS.best_params_
GS.best_score_|
~~~

可以看见，网格搜索返回了min_samples_leaf的最小值，并且模型整体的准确率还降低了，这和max_depth的情况一致，参数把模型向左推，但是模型的泛化误差上升了。在这种情况下，我们显然是不要把这个参数设置起来的，就让它默认就好了。

**尝试min_samples_split**

~~~ python
#调整min_samples_split
param_grid={'min_samples_split':np.arange(2, 2+20, 1)}
rfc = RandomForestClassifier(n_estimators=39
,random_state=90
)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)
GS.best_params_
GS.best_score_
~~~

和min_samples_leaf一样的结果，返回最小值并且模型整体的准确率降低了。

**criterion**

~~~ python
#调整Criterion
param_grid = {'criterion':['gini', 'entropy']}
rfc = RandomForestClassifier(n_estimators=39
,random_state=90
)
GS = GridSearchCV(rfc,param_grid,cv=10)
GS.fit(data.data,data.target)
GS.best_params_
GS.best_score_
~~~

最后我们得出模型的参数：

~~~ python
rfc = RandomForestClassifier(n_estimators=39,random_state=90)
score = cross_val_score(rfc,data.data,data.target,cv=10).mean()
score
score - score_pre
~~~

在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通过max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。如果感到困惑，也可以画很多学习曲线来观察参数会如何影响我们的准确率，选取学习曲线中单调的部分来放大研究（如同我们对n_estimators做的）。学习曲线的拐点也许就是我们一直在追求的，最佳复杂度对应的泛化误差最低点（也是方差和偏差的平衡点）。

网格搜索也可以同时调整多个参数。