# 使用手册V2.0版

## Hive篇

### 什么是Hive，为什么要使用Hive，你如何理解Hive

Hive是一个基于Hadoop的数据仓库工具，可以将一些结构化的文件映射为结构化的表，并且提供了类似sql的查询功能。Hive的本质也是将hql转化为mapreduce任务进行计算。

平时我们都使用hadoop框架进行数据的计算，但是上手使用hadoop编写mr程序需要一定的编程基础，所以为了更好的开发出我们的mr程序，就有了hive工具，hlq学习起来的成本很低，我们可以直接写hql查询来查看数据，hive底层会将hql语句自动转换为mr程序。

> 所以说HIve是一个数据仓库工具，提供了类sql的语句hql可以转化为mr程序操作hdfs上面存储的数据。

### Hive架构

![1633138397495](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/093319-612218.png)

**用户接口**

- cui是自带的命令行接口
- JDBC/ODBC是数据库操作
- web ui使用http协议通过浏览器操作

**元数据**

- 默认使用derby数据库，但是只可以建立一个链接
- 生产中一般使用mysql数据库，可以建立多个链接

**驱动器**

- 解释器：将类sql语句转换为hql语句
- 编译器：将hql语句转换为job作业
- 优化器：优化job作业
- 执行器：yarn调用job在hadoop执行

### Hive在生产环境中的架构

![1633139076675](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/094652-261004.png)

Hive是一个数据仓库的工具，方便程序员们编写mapreduce程序，hql转换mapreduce程序是hive框架底层自动实现的，mysql中产生的业务数据使用sqoop导入到Hive中，用户行为数据通过flume采集到hive中，在hive底层，数去全部存储在hdfs上面，然后通过hive工具去分析hdfs上面的数据。

### 画出hive的架构图，解释每一部分的作用

![1633139402469](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/095004-113468.png)

### Hive优缺点

**优点**

- 简单，非常容易上手，会写sql就能开发出mr程序，**实现分布式计算**。
- 是一个数据仓库的工具，可以处理大量的数据，但是对于小数据没有优势
- 可以支持用户自定义的函数UDF/UDTF。

**缺点**

- hql表达能力有限，迭代式计算无法表达。
- 数据挖掘方面不擅长，实现不了挖掘算法效率高的算法。
- HIVE效率比较低，延迟比较高，因为底层使用的是hadoop。
- hive调优比较困难，粒度比较粗
- 不能修改数据，适合读多写少的情景。

### Hql转化为mapreduce程序的过程

- 编译器首先完成hql的词法和语法的解析，然后将hql转化为抽象语法树AST。
- 遍历抽象抽象语法树，抽象出查询的基本组成单位，查询块，生成基本的查询块。
- 接着遍历生成的基本查询块，然后将遍历出来的结果翻译为执行操作树。
- 生成完操作树后会在逻辑层去优化，会对生成的操作树进行优化变换，去合并不必要的reduceSink操作，目的就是为了减少shuffle的数据量，
- 优化之后会再次遍历优化之后的操作树，翻译为mr任务，这一次翻译出来的任务就是物理的mr任务，物理层的优化器会再次进行mr任务的一个转换，再次从物理层进行优化，最后生成一个执行计划。
- 最后提交执行计划执行。

> Hql-解释器-->抽象语法树-编译器-->查询块-编译器--逻辑优化器>操作树-物理优化器-->mr任务-执行器-->执行计划

### Hive的两张表关联，使用MapReduce怎么实现？

- 如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。
- 如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。

### 请谈一下Hive的特点，Hive和RDBMS有什么异同？

hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，**但是Hive不支持实时查询，Hive的延迟比较高**。

**Hive与关系型数据库的区别**

![1632707644904](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/27/095406-581410.png)

![1633139502521](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/095144-999240.png)

### 请说明hive中 Sort By，Order By，Cluster By，Distrbute By各代表什么意思？

- Order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
- Sort by：不是全局排序，其在数据进入 reducer 前完成排序.因此，如果用 sort by 进行排序，并且设置 mapred.reduce.tasks>1，也就是map任务大于1个， 则 **sort by 只保证每个 reducer 的输出有序，不保证全局有序，保证每一个reduce内局部有序**。
- Distribute by：按照指定的字段对数据进行划分输出到不同的reduce中，可能是按照字段的hash值进行划分。
- Cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。

**小结**

![1633139753219](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/095554-608131.png)

### Hql中常用的窗口函数有哪些

![1633139907243](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/095828-317055.png)

![1633140160037](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/100241-101850.png)

**详解窗口函数**

![1633140465134](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/100745-431581.png)

上面代表分完区的数据：

current row代表的是当前行，从当前行，可以选择前n行和后n行，也可以选择到分区的上边界或者分区的后边界。

### 写出Hive中split、coalesce及collect_list函数的用法（可举例）？

- split将字符串转化为数组，即：split('a,b,c,d' , ',') ==> ["a","b","c","d"]。
- coalesce(T v1, T v2, …) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULL。
- collect_list列出该字段所有的值，**不去重** => select collect_list(id) from table。

### Hive有哪些方式保存元数据，各有哪些特点？

Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器（derby数据库）、本地元存储服务器（mysql数据库）、远程元存储服务器，每种存储方式使用不同的配置参数。

1. 内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。自带的存储数据库，每一次只可以建立一个会话链接。
2. 在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。通常我们使用的是Mysql数据库存储元数据信息，而使用HDFS存储数据信息。
3. 在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。

### Hive内部表和外部表的区别？

未被 external 修饰的是内部表，被 external 修饰的为外部表。


- 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径，位置是`hive.metastore.warehouse.dir`（默认：`/user/hive/warehouse`），若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。
- 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据，内部表的数据属于hive管理，而外部表的数据属于hdfs管理。

### Hive 有索引吗

Hive 支持索引（3.0 版本之前），但是 Hive 的索引与关系型数据库中的索引并不相同，比如，Hive 不支持**主键或者外键**。并且 Hive 索引提供的功能很有限，效率也并不高，因此 Hive 索引很少使用。

- **索引适用的场景：**

适用于不更新的静态字段。以免总是重建索引数据。每次建立、更新数据后，都要重建索引以构建索引表。

- **Hive 索引的机制如下：**

hive 在指定列上建立索引，会产生一张索引表（Hive 的一张物理表），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。

Hive 0.8 版本后引入 bitmap 索引处理器，这个处理器适用于去重后，值较少的列（例如，某字段的取值只可能是几个枚举值）因为索引是用**空间换时间**，索引列的取值过多会导致建立 bitmap 索引表过大。

**注意**：Hive 中每次有数据时需要及时更新索引，相当于重建一个新表，否则会影响数据查询的效率和准确性，**Hive 官方文档已经明确表示 Hive 的索引不推荐被使用，在新版本的 Hive 中已经被废弃了**。

**扩展**：Hive 是在 0.7 版本之后支持索引的，在 0.8 版本后引入 bitmap 索引处理器，在 3.0 版本开始移除索引的功能，取而代之的是 2.3 版本开始的物化视图，自动重写的物化视图替代了索引的功能。

### 数据建模用的哪些模型？

> 介绍一下事实表和维度表：
>
> 事实表，即为事实数据表的简称。主要特点是含有大量的数据，并且这些数据是可以汇总，并被记录的。
>
> 记录既定事实数据的数据集，比如某时某地某物发生某事，记录为一条数据，一般是原始数据；例如2020年1月1日购物平台袜子销售金额为1000元，这条数据也可以继续细分，如2020年1月1日1时1分1秒20元卖出一双A牌B型号袜子。
>
> 维度表可以看作是用户来分析数据的窗口，维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，维度表包含帮助汇总数据的特性的层次结构。
>
> 维度表由主键和属性组成，一个维度表只能有一个主键，且主键的值不能重复，一个维度表可以包含多个属性，且这些属性可以进行扩展。一般我们用DIM来表示维度表



**星型模型**

![1632835904327](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/28/213147-392488.png)

**星形模式**

星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样。星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：

- 维度表只和事实表关联，维表之间没有关联；
- 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键；
- 以事实表为核心，维表围绕核心呈星形分布。

**雪花模型**

![1632836011529](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/28/213333-529476.png)

雪花模式

雪花模式(Snowflake Schema)是对星形模式的扩展。**雪花模式的维度表可以拥有其他维度表的**，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能比星型模型要低。

**星座模型**

![1632836060943](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202109/28/213423-5483.png)

星座模型

星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而**星座模式是基于多张事实表的，而且共享维度信息**。前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。

### 为什么要对数据仓库进行分层

- 用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。
- 如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。
- 通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。

### Hive中常见的函数

**聚合函数**（多进一出）

- count()
- sum()
- avg()

**数学函数**

- around（）四舍五入函数

**集合函数**

- size()查询集合长度
- mapkeys()查询map中所有的key

**类型转换函数**

- cast()

**日期函数**

- unix_timestamp()
- time_add()

**条件函数**

- isNull()

**字符串函数**

- length()
- subString()

**表生成函数（列转行）**

- explode()

**开窗函数**

- rank()
- rownumber()

### Hive的函数：UDF、UDAF、UDTF的区别？

- UDF：用户自定义函数，单行进入，单行输出（一进一出）
- UDAF：多行进入，单行输出（多进一出）
- UDTF：单行输入，多行输出（一进多出）

**讲一下自定义用户函数的步骤**

![1633140847597](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/101411-224626.png)

**自定义UDF函数**

1. 继承`org.apache.hadoop.hive.ql.exec.UDF`
2. 需要实现`evaluate`函数；`evaluate`函数支持重载，不可以修改函数名；
3. 在`hive`的命令行窗口创建函数(可以创建临时函数和永久的函数)
4. 如果自定义函数要输入多个参数怎么办，就重载`EVALUTE()`方法，每个方法中的参数不同即可，也可以用可变形参的方法，但是局限性在于可变形参的类型必须一样。

**自定义UDTF函数**

1. 自定义类继承`genericUDTF`类。
2. 实现其三个方法：`initialize`,`process`,`close`等三个方法。
   1. `initialize()`：进行初始化操作，定义输出数据类型
   2. `process`:处理业务逻辑
   3. `close`：关闭资源
3. `initialize`函数里面声明输出参数的名称和类型，`process`里面写具体处理的业务逻辑，`close`关闭流操作。

### 使用过 Hive 解析 JSON 串吗

**Hive 处理 json 数据总体来说有两个方向的路走**：

1. 将 json 以字符串的方式整个入 Hive 表，然后通过使用 UDF 函数解析已经导入到 hive 中的数据，比如使用LATERAL VIEW json_tuple的方法，获取所需要的列名。
2. 在导入之前将 json 拆成各个字段，导入 Hive 表的数据是已经解析过的。这将需要使用第三方的 SerDe。

### 所有的Hive任务都会有MapReduce的执行吗？

- hive 0.10.0为了执行效率考虑，简单的查询，就是只是select，不带count,sum,group by这样的，都不走map/reduce，直接读取hdfs文件进行filter过滤。这样做的好处就是不新开mr任务，执行效率要提高不少，但是不好的地方就是用户界面不友好，有时候数据量大还是要等很长时间，但是又没有任何返回。

### 说说对Hive桶表的理解？

桶表是对数据`某个字段`进行哈希取值，然后放到不同文件中存储。数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。

### Hive底层与数据库交互原理？

Hive 的查询功能是由 HDFS 和 MapReduce结合起来实现的，对于大规模数据查询还是不建议在 hive 中，因为过大数据量会

造成查询十分缓慢。

Hive 与 MySQL的关系：只是借用 MySQL来存储 hive 中的表的元数据信息，称为 metastore（元数据信息），而真正的数据存储在Hdfs上面。

### Hive本地模式

大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多，因为一旦触发执行JOB,必定会涉及集群资源的管理与调度，很费性能和耗时。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。

用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。

### Hive 中的压缩格式TextFile、SequenceFile，RCfile 、ORCfile各有什么区别？

**TextFile**

默认格式，**存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大**。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。

**SequenceFile**

SequenceFile是Hadoop API提供的一种二进制文件支持，**存储方式为行存储，其具有使用方便、可分割、可压缩的特点**。

SequenceFile支持三种压缩选择：`NONE`，`RECORD`，`BLOCK`。Record压缩率低，**一般建议使用BLOCK压缩**。

优势是文件和hadoop api中的MapFile是相互兼容的

**RCFile**

存储方式：**数据按行分块，每块按列存储**。结合了行存储和列存储的优点：

首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；

其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取；

**ORCFile**

存储方式：数据按行分块 每块按照列存储。

压缩快、快速列存取。

效率比rcfile高，是rcfile的改良版本。

> 小结：
>
> **相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应**。
>
> **数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势**。

### Hive表关联查询，如何解决数据倾斜的问题？

倾斜原因：map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。

1. key分布不均匀;
2. 业务数据本身的特性;
3. 建表时考虑不周;
4. 某些SQL语句本身就有数据倾斜;

如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。

**解决方案**

1. 参数调节：

~~~ java
hive.map.aggr = true //首先在map端进行聚合
hive.groupby.skewindata=true
~~~

有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。

**SQL 语句调节**：

1. 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。
2. 大小表Join：使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce。
3. 大表Join大表：把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。
4. count distinct大量相同特殊值:count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。

### Fetch抓取

Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：`SELECT * FROM employees`;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。

在`hive-default.xml.template`文件中`hive.fetch.task.conversion`默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。

### 小表、大表Join

将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。

实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。

### 大表Join大表

1. 空KEY过滤，有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空。
2. 空key转换，有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。

### Group By

默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。

**开启Map端聚合参数设置**     

1. 是否在Map端进行聚合，默认为True，`hive.map.aggr = true`   
2. 在Map端进行聚合操作的条目数目`hive.groupby.mapaggr.checkinterval = 100000`   
3. 有数据倾斜的时候进行负载均衡（默认是false）`hive.groupby.skewindata = true`，**当选项设定为 true，生成的查询计划会有两个MR Job**。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是**相同的Group By Key有可能被分发到不同的Reduce中**，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。

### Count(Distinct) 去重统计

数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换

### 笛卡尔积

尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积

### 行列过滤

- 列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。
- 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。

### 并行执行

Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。

通过设置参数`hive.exec.parallel`值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。

### Hive 小文件过多怎么解决

![1633154997892](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/140959-627050.png)

第二个表示：在map结束的时候合并小文件，默认是true

第三个参数表示在mr结束的时候合并小文件，默认的参数是false

最后一个参数表示设置文件的最小大小，当文件小于这个值的时候，就会启动一个线程对文件进行合并操作

1. 使用 hive 自带的 concatenate 命令，自动合并小文件

~~~ sql
-- 用法
--对于非分区表
alter table A concatenate;

--对于分区表
alter table B partition(day=20201224) concatenate;
~~~

> 注意：
>
> 1、concatenate 命令只支持 RCFILE 和 ORC 文件类型。
>
> 2、使用 concatenate 命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。
>
> 3、当多次使用 concatenate 后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小 size。

2. 调整参数减少 Map 数量

设置 map 输入合并小文件的相关参数（执行 Map 前进行小文件合并）：在 mapper 中将多个文件合成一个 split 作为输入（CombineHiveInputFormat底层是 Hadoop 的CombineFileInputFormat方法）：

~~~ sql
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 默认

-- 每个 Map 最大输入大小（这个值决定了合并后文件的数量）：
set mapred.max.split.size=256000000;   -- 256M

-- 一个节点上 split 的至少大小（这个值决定了多个 DataNode 上的文件是否需要合并）：
set mapred.min.split.size.per.node=100000000;  -- 100M

-- 一个交换机下 split 的至少大小(这个值决定了多个交换机上的文件是否需要合并)：
set mapred.min.split.size.per.rack=100000000;  -- 100M

~~~

3. 减少 Reduce 的数量

reduce 的个数决定了输出的文件的个数，所以可以调整 reduce 的个数控制 hive 表的文件数量。

hive 中的分区函数 distribute by 正好是控制 MR 中 partition 分区的，可以通过设置 reduce 的数量，结合分区函数让数据均衡的进入每个 reduce 即可：

~~~ sql
#设置reduce的数量有两种方式，第一种是直接设置reduce个数
set mapreduce.job.reduces=10;

#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数
set hive.exec.reducers.bytes.per.reducer=5120000000; -- 默认是1G，设置为5G

#执行以下语句，将数据均衡的分配到reduce中
set mapreduce.job.reduces=10;
insert overwrite table A partition(dt)
select * from B
distribute by rand();
~~~

对于上述语句解释：如设置 reduce 数量为 10，使用 rand()， 随机生成一个数 x % 10 ，这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小。

4. 使用 hadoop 的 archive 将小文件归档

Hadoop Archive 简称 HAR，是一个高效地将小文件放入 HDFS 块中的文件存档工具，它能够将多个小文件打包成一个 HAR 文件，这样在减少 namenode 内存使用的同时，仍然允许对文件进行透明的访问。

~~~ sql
#用来控制归档是否可用
set hive.archive.enabled=true;
#通知Hive在创建归档时是否可以设置父目录
set hive.archive.har.parentdir.settable=true;
#控制需要归档文件的大小
set har.partfile.size=1099511627776;

使用以下命令进行归档：
ALTER TABLE A ARCHIVE PARTITION(dt='2021-05-07', hr='12');

对已归档的分区恢复为原文件：
ALTER TABLE A UNARCHIVE PARTITION(dt='2021-05-07', hr='12');

~~~

> 注意:**归档的分区可以查看不能 insert overwrite，必须先 unarchive**

### Hive调优

![1633141298756](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/102140-147107.png)

![1633141656384](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/102738-108120.png)

![1633141854445](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/103056-117747.png)

#### 源码角度理解hive执行原理

![1633142184004](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/103625-164480.png)

1. 提交一个执行计划。
2. 在driver端生成一个执行计划，
3. 编译器去元数据库查询执行计划
4. 元数据库返元数据信息
5. 编译器返回执行计划给驱动器
6. 编译器发送执行计划命令给执行引擎开始执行执行计划
7. 执行引擎提交计划到集群中去执行
8. 执行引擎返回执行的结果到驱动器
9. 驱动器返回结果给用户

#### Hive的存储格式和压缩格式

![1633142813335](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/104654-665346.png)

#### Hive中的分析函数

![1633146131360](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/114212-897226.png)

![1633146654696](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/115056-678547.png)

#### 说说你对分区表和分桶表的理解

![1633147155726](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/115917-71055.png)

#### Fetch抓取（Hive可以避免进行MapReduce）

 Hive中对某些情况的查询`可以不必使用MapReduce计算`。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。  

在`hive-default.xml.template`文件中`hive.fetch.task.conversion`默认是more，老版本hive默认是`minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce`。

~~~ xml
<property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
    <description>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have
      any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
    </description>
  </property>
~~~

**案例**

把`hive.fetch.task.conversion`设置成`none`，然后执行查询语句，都会`执行mapreduce`程序。

~~~ sql
hive (default)> set hive.fetch.task.conversion=none;
hive (default)> select * from score;
hive (default)> select s_score from score;
hive (default)> select s_score from score limit 3;
~~~

把`hive.fetch.task.conversion`设置成`more`，然后执行查询语句，如下查询方式都`不会执行mapreduce程序`。

#### 本地模式

大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时`Hive的输入数据量是非常小`的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。`对于小数据集，使用本地模式执行时间可以明显被缩短`。   

用户可以通过设置`hive.exec.mode.local.auto`的值为`true`，来让Hive在适当的时候`自动启动这个优化`。

~~~ java
set hive.exec.mode.local.auto=true;  --开启本地MapReduce
-- 设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M
set hive.exec.mode.local.auto.inputbytes.max=51234560;
-- 设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4
set hive.exec.mode.local.auto.input.files.max=10
~~~

开启本地模式查询：

~~~ sql
set hive.exec.mode.local.auto=true; 
select * from score cluster by s_id;
~~~

#### group By

 默认情况下，Map阶段同一Key数据分发给一个reduce，`当一个key数据过大时就倾斜了`。         

并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以`先在Map端进行部分聚合，最后在Reduce端得出最终结果`。

**开启Map端聚合参数设置**

~~~ java
--  是否在Map端进行聚合，默认为True
set hive.map.aggr = true;
--在Map端进行聚合操作的条目数目
set hive.groupby.mapaggr.checkinterval = 100000;
-- 有数据倾斜的时候进行负载均衡（默认是false）
set hive.groupby.skewindata = true;
~~~

当负载均衡选项设定为 true，生成的查询计划会有两个MR Job。

- 第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；
- 第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。

#### Count(distinct)

数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换，也就是在使用Count进行统计的时候，先对数据进行分组然后在统计。

~~~ sql
-- 直接对数据进行统计
SELECT count(DISTINCT id) FROM bigtable;
-- 先分组然后在查询
SELECT count(id) FROM (SELECT id FROM bigtable GROUP BY id) a;
~~~

但是这样做的话，会多执行一个分组的任务，但是对于数据量特别大的情况，是非常值得的。

#### 笛卡尔积

尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。不论在什么情况下都应该绝对的避免笛卡尔积。

#### 使用分区剪裁、列剪裁

在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。         

在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤（要尽量先进行where过滤，然后在进行关联操作），比如：

**案例**

~~~ sql
create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';
create table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

-- 先关联再where
select a.id FROM bigtable a left join ori o on a.id=o.id where o.id<=10;

-- 先过滤然后在进行关联
select a.id FROM bigtable a left join ori o  on (o.id<=10 and a.id=o.id);

-- 子查询
select a.id from bigtable a join (select  id from ori o where o.id<=10) o on o.id=a.id;
~~~

#### 动态分区调整

关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。  

以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可。

**开启动态分区需要设置一些参数**

1. 开启动态分区参数设置

~~~ java
set hive.exec.dynamic.partition=true;
~~~

2. 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）

~~~ java
set hive.exec.dynamic.partition.mode=nonstrict;
~~~

3. 在所有执行MR的节点上，最大一共可以创建多少个动态分区。

~~~ java
set  hive.exec.max.dynamic.partitions=1000;
~~~

4. 在每个执行MR的节点上，最大可以创建多少个动态分区`。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。

~~~ java
set hive.exec.max.dynamic.partitions.pernode=100
~~~

5. 整个MR Job中，最大可以创建多少个HDFS文件。

在linux系统当中，每个linux用户最多可以开启1024个进程，每一个进程最多可以打开2048个文件，即持有2048个文件句柄，下面这个值越大，就可以打开文件句柄越大。

~~~ java
set hive.exec.max.created.files=100000
~~~

6. 当有空分区生成时，是否抛出异常。一般不需要设置。

~~~ java
set hive.error.on.empty.partition=false
~~~

#### 数据倾斜

**Map数量**

通常情况下，作业会通过input的目录产生一个或者多个map任务。

主要的决定因素有：`input的文件总个数`，`input的文件大小`，集群设置的文件块大小(目前为128M，可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改）。

*举例：*

- **一个大文件**：假设input目录下有1个文件a，`大小为780M`，那么hadoop会将该文件a`分隔成7个块`（**6个128m的块和1个12m的块**），从而产生7个map数。
- **多个小文件** ：假设input目录下有`3个文件a，b，c`大小分别为`10m，20m，150m`，那么hadoop会`分隔成4个块`（**10m，20m，128m，22m**），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块，Hadoop是按照文件大小进行切分。
- **是不是map数越多越好？**：`答案是否定的`。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。
- **是不是保证每个map处理接近128m的文件块，就高枕无忧了？**：`答案也是不一定`。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时
- **小结**：针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数；这个就看看真实的场景去确认map的数量。

如何适当增加map数

当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。

**案例**

针对：**是不是保证每个map处理接近128m的文件块，就高枕无忧了？**

~~~ sql
Select data_desc,
count(1),
count(distinct id),
sum(case when …),
sum(case when …),
sum(…)
from a group by data_desc
~~~

如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。

~~~ sql
set mapreduce.job.reduces =10;
create table a_1 as
select * from a
distribute by rand();
~~~

 这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。        

 每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多。         

看上去，貌似这两种有些矛盾，一个是要合并小文件，一个是要把大文件拆成小文件，这点正是重点需要关注的地方，根据实际情况，控制map数量需要遵循两个原则：`使大数据量利用合适的map数；使单个map任务处理合适的数据量`；

**Reduce数量**

调整reduce个数方法一：

- 每个Reduce处理的数据量默认是256MB

~~~ sql
-- 参数一
hive.exec.reducers.bytes.per.reducer=256123456
~~~

- 设置Reduce个数

~~~ sql
-- 参数二
hive.exec.reducers.max=1009

计算reducer数的公式很简单：

N=min(参数2，总输入数据量/参数1)
~~~

调整reduce个数方法二

- 设置每个job的Reduce个数

~~~ sql
set mapreduce.job.reduces = 15;
~~~

**reduce个数并不是越多越好**

- 过多的启动和初始化reduce也会消耗时间和资源；
- 另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题.
- 在设置reduce个数的时候也需要考虑这两个原则：`处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适`

#### 并行执行

Hive会将一个查询转化成`一个或者多个阶段`。这样的阶段可以使MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。      

通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。

~~~ sql
set hive.exec.parallel=true;     -- 打开任务并行执行
set hive.exec.parallel.thread.number=16; -- 同一个sql允许最大并行度，默认为8。
~~~

#### 严格模式

Hive提供了一个严格模式，可以`防止用户执行“高危”的查询`。        

通过设置属性`hive.mapred.mode`值为默认是非严格模式`nonstrict` 。开启严格模式需要修改`hive.mapred.mode`值为`strict`，开启严格模式可以禁止3种类型的查询。

**开启严格模式**

~~~ xml
<property>
    <name>hive.mapred.mode</name>
    <value>strict</value>
    <description>
      The mode in which the Hive operations are being performed. 
      In strict mode, some risky queries are not allowed to run. They include:
        Cartesian Product.
        No partition being picked up for a query.
        Comparing bigints and strings.
        Comparing bigints and doubles.
        Orderby without limit.
    </description>
  </property>
~~~

1. 对于分区表，`用户不允许扫描所有分区`，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。**没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。**
2. 对于`使用了order by语句的查询，要求必须使用limit语句`。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，**强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间**。
3. `限制笛卡尔积的查询`。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候`不使用ON语句而是使用where语句`，**这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。\不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。**

#### JVM重用

JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。         

Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。`JVM重用可以使得JVM实例在同一个job中重新使用N次`。N的值可以在`Hadoop的mapred-site.xml`文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。

~~~ xml
<property>
  <name>mapreduce.job.jvm.numtasks</name>
  <value>10</value>
  <description>How many tasks to run per jvm. If set to -1, there is
  no limit. 
  </description>
</property>
~~~

也可以在hive中开启jvm重用

~~~ sql
set  mapred.job.reuse.jvm.num.tasks=10;
~~~

这个设置来设置我们的jvm重用，这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。

#### 推测执行

 在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 `Hive 同样可以开启推测执行`设置开启推测执行参数：Hadoop的`mapred-site.xml文`件中进行配置

~~~ xml
<property>
  <name>mapreduce.map.speculative</name>
  <value>true</value>
  <description>If true, then multiple instances of some map tasks  may be executed in parallel.</description>
</property>

<property>
  <name>mapreduce.reduce.speculative</name>
  <value>true</value>
  <description>If true, then multiple instances of some reduce tasks   may be executed in parallel.</description>
</property>
~~~

不过hive本身也提供了配置项来控制`reduce-side`的推测执行：

~~~ sql
<property>
    <name>hive.mapred.reduce.tasks.speculative.execution</name>
    <value>true</value>
    <description>Whether speculative execution for reducers should be turned on. </description>
  </property>
~~~

#### 表的优化

**Join**

**Join 原则**

1. 小表Join大表：将key相对分散，并且`数据量小的表放在join的左边`，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。

~~~ sql
select  count(distinct s_id)  from score;
select count(s_id) from score group by s_id; -- 在map端进行聚合，效率更高x`
~~~

2. 多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job）

3. 大表Join大表空KEY过滤

有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：

~~~ sql
create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

~~~

不过滤进行连接

~~~ sql
INSERT OVERWRITE TABLE jointable SELECT a.* FROM nullidtable a JOIN ori b ON a.id = b.id;
~~~

过滤空值后连接

~~~ sql
INSERT OVERWRITE TABLE jointable SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;
~~~

**空Key转换**

有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如:

~~~ sql
set hive.exec.reducers.bytes.per.reducer=32123456;
set mapreduce.job.reduces=7;
INSERT OVERWRITE TABLE jointable SELECT a.* FROM nullidtable a LEFT JOIN ori b ON  CASE WHEN  a.id IS NULL  THEN 'hive'  ELSE a.id  END;
~~~

结果：这样的后果就是所有为null值的id全部都变成了相同的字符串“hive”，及其容易造成数据的倾斜（所有的key相同，相同key的数据会到同一个reduce当中去）

为了解决这种情况，我们可以通过hive的rand函数，随记的给每一个为空的id赋上一个随机值，这样就不会造成数据倾斜**随机分布：**

~~~ sql
set hive.exec.reducers.bytes.per.reducer=32123456;
set mapreduce.job.reduces=7;
INSERT OVERWRITE TABLE jointable SELECT a.* FROM nullidtable a LEFT JOIN ori b ON  CASE WHEN  a.id IS NULL  THEN concat('hive',rand())  ELSE a.id  END;
~~~

> 大表join小表或者小表join大表，就算是关闭map端join的情况下，在新的版本当中基本上没有区别了（hive为了解决数据倾斜的问题，会自动进行过滤）

**MapJoin**

如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join（在Reduce阶段完成join）。容易发生数据倾斜。`可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。

开启MapJoin参数设置：

1. 设置自动选择Mapjoin

~~~ sql
set hive.auto.convert.join = true; --默认为true
~~~

2. 大表小表的阈值设置（默认25M以下认为是小表）：

~~~ sql
set hive.mapjoin.smalltable.filesize=25123456;
~~~

![1632829096770](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1632829096770.png)

首先是Task A，它是一个Local Task（在客户端本地执行的Task），负责扫描小表b的数据，将其转换成一个HashTable的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。         

接下来是Task B，该任务是一个没有Reduce的MR，启动MapTasks扫描大表a,在Map阶段，根据a的每一条记录去和DistributeCache中b表对应的HashTable关联，并直接输出结果。

由于MapJoin没有Reduce，所以由Map直接输出结果文件，有多少个Map Task，就有多少个结果文件。

**案例实操：**

1. 开启Mapjoin功能

~~~ sql
set hive.auto.convert.join = true; -- 默认为true
~~~

2. 执行小表JOIN大表语句

~~~ sql
INSERT OVERWRITE TABLE jointable2
SELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
FROM smalltable s
JOIN bigtable  b
ON s.id = b.id;
~~~

3. 执行大表JOIN小表语句

~~~ sql
NSERT OVERWRITE TABLE jointable2
SELECT b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
FROM smalltable s
JOIN bigtable  b
ON s.id = b.id;
~~~

## Hadoop篇

### 什么是Hadoop

Hadoop是一个能够对大量数据进行分布式处理的软件框架。以一种可靠、高效、可伸缩的方式进行数据处理。主要包括三部分内容：Hdfs，MapReduce，Yarn。

Hadoop在广义上指一个生态圈，泛指大数据技术相关的开源组件或产品，如HBase，Hive，Spark，Zookeeper，Kafka，flume。

### 介绍下Hadoop和Spark的差异

![1633158114339](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/150155-838283.png)

### Hadoop常见的版本有哪些，分别有哪些特点，你一般是如何进行选择的?

由于Hadoop的飞速发展，功能不断更新和完善，Hadoop的版本非常多，同时也显得杂乱。目前市面上，主流的是以下几个版本：

**Apache 社区版本**

Apache 社区版本 完全开源，免费，是非商业版本。Apache社区的Hadoop版本分支较多，而且部分Hadoop存在Bug。在选择Hadoop、Hbase、Hive等时，需要考虑兼容性。同时，这个版本的Hadoop的部署对Hadoop开发人员或运维人员的技术要求比较高。

**Cloudera版本**

Cloudera 版本 开源，免费，有商业版和非商业版本，是在Apache社区版本的Hadoop基础上，选择相对稳定版本的Hadoop，进行开发和维护的Hadoop版本。由于此版本的Hadoop在开发过程中对其他的框架的集成进行了大量的兼容性测试，因此使用者不必考虑Hadoop、Hbase、Hive等在使用过程中版本的兼容性问题，大大节省了使用者在调试兼容性方面的时间成本。

**Hortonworks版本**

Hortonworks 版本 的 Hadoop 开源、免费，有商业和非商业版本，其在 Apache 的基础上修改，对相关的组件或功能进行了二次开发，其中商业版本的功能是最强大，最齐全的。

所以基于以上特点进行选择，我们一般刚接触大数据用的就是CDH，在工作中大概率用 Apache 或者 Hortonworks。

### 单介绍Hadoop1.0，2.0，3.0的区别吗？

**Hadoop1.0由分布式存储系统HDFS和分布式计算框架MapReduce组成,切记没有Yarn调度器**，其中HDFS由一个NameNode和多个DateNode组成，MapReduce由一个JobTracker和多个TaskTracker组成。在Hadoop1.0中容易导致单点故障，拓展性差，性能低，支持编程模型单一的问题。

Hadoop2.0即为克服Hadoop1.0中的不足，提出了以下关键特性：

1. **Yarn**：它是Hadoop2.0引入的一个全新的通用资源管理系统，完全代替了Hadoop1.0中的JobTracker。在MRv1 中的 JobTracker 资源管理和作业跟踪的功能被抽象为 ResourceManager 和 AppMaster 两个组件。Yarn 还支持多种应用程序和框架，提供统一的资源调度和管理功能。
2. **NameNode 单点故障得以解决，**Hadoop2.2.0 同时解决了 NameNode 单点故障问题和内存受限问题，并提供 NFS，QJM 和 Zookeeper 三种可选的共享存储系统。
3. **HDFS 快照**：指 HDFS（或子系统）在某一时刻的只读镜像，该只读镜像对于防止数据误删、丢失等是非常重要的。例如，管理员可定时为重要文件或目录做快照，当发生了数据误删或者丢失的现象时，管理员可以将这个数据快照作为恢复数据的依据。
4. **支持Windows 操作系统**：Hadoop 2.2.0 版本的一个重大改进就是开始支持 Windows 操作系统
5. **Append：新版本的 Hadoop 引入了对文件的追加操作**

> 同时，新版本的Hadoop对于HDFS做了两个非常重要的**「增强」**，分别是支持异构的存储层次和通过数据节点为存储在HDFS中的数据提供内存缓冲功能（待了解）

相比于Hadoop2.0，Hadoop3.0 是直接基于 JDK1.8 发布的一个新版本，同时，Hadoop3.0引入了一些重要的功能和特性：

1. **HDFS可擦除编码**：这项技术使HDFS在不降低可靠性的前提下节省了很大一部分存储空间
2. **多NameNode支持**：在Hadoop3.0中，新增了对多NameNode的支持。当然，处于Active状态的NameNode实例必须只有一个。也就是说，从Hadoop3.0开始，在同一个集群中，支持一个 ActiveNameNode 和 多个 StandbyNameNode 的部署方式。
3. MR Native Task优化
4. Yarn基于cgroup 的内存和磁盘 I/O 隔离
5. Yarn container resizing

### HDFS组成架构

![1633162793938](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/161955-691731.png)

架构主要由四个部分组成，分别为**HDFS Client、NameNode、DataNode和Secondary NameNode**。下面我们分别介绍这四个组成部分。
**Client：就是客户端。**

- 文件切分，文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；
- 与NameNode交互，获取文件的位置信息；
- 与DataNode交互，读取或者写入数据；
- Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；
- Client可以通过一些命令来访问HDFS；

**NameNode：就是Master，它是一个主管、管理者。**

- 管理HDFS的名称空间；
- 管理数据块（Block）映射信息；
- 配置副本策略；
- 处理客户端读写请求。
- 主要用来记录文件的元数据信息（文件名，副本数，权限，文件块信息，位置）。
- 一个NameNode会存在单点故障

**DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。**

- 存储实际的数据块；
- 执行数据块的读/写操作。
- 和NameNode之间存在心跳信息。

**Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。**

- 辅助NameNode，分担其工作量；
- 定期合并Fsimage和Edits，并推送给NameNode；
- 在紧急情况下，可辅助恢复NameNode。

### NameNode与SecondaryNameNode 的区别与联系？

**区别**

1. NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。
2. SecondaryNameNode主要用于定期合并命名空间镜像和命名空间镜像的编辑日志。

**联系：**

1. SecondaryNameNode中保存了一份和namenode一致的镜像文件（fsimage）和编辑日志（edits）。
2. 在主namenode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode恢复数据。

### HDFS 中的 block 默认保存几份？

默认存储3分文件

### HDFS 默认 BlockSize 是多大？

**分块：**
HDFS存储系统中，引⼊了⽂件系统的分块概念（block），块是存储的最⼩单位，HDFS1.0定义其⼤⼩为64MB。与单磁盘⽂件系统相似，存储在 HDFS上的⽂件均存储为多个块，不同的是，如果某⽂件⼤⼩没有到达64MB，该⽂件也不会占据整个块空间。在分布式的HDFS集群上，Hadoop系统 保证⼀个块存储在⼀个datanode上。

HDFS的namenode只存储整个⽂件系统的元数据镜像，这个镜像由配置dfs.name.dir指定，datanode则存有⽂件的metainfo和具体的分块，存储路径由dfs.data.dir指定。

**分片：**
hadoop的作业在提交过程中，需要把具体的输⼊进⾏分⽚。具体的分⽚细节由InputSplitFormat指定。分⽚的规则为

FileInputFormat.class中的getSplits()⽅法指定：

~~~ java
//具体分片使用的算法
long splitSize = computeSplitSize(goalSize, minSize, blockSize）
computeSplitSize:
Math.max(minSize, Math.min(goalSize, blockSize));
~~~

其中goalSize为“InputFile⼤⼩，”/“我们在配置⽂件中定义的mapred.map.tasks”值，minsize为mapred.min.split.size，blockSize为64，所以，这个算式为取分⽚⼤⼩不⼤于block，并且不⼩于在mapred.min.split.size配置中定义的最⼩Size。

当某个分块分成均等的若⼲分⽚时，会有最后⼀个分⽚⼤⼩⼩于定义的分⽚⼤⼩，则该分⽚独⽴成为⼀个分⽚。

> 在hadoop1.x中默认是64M，在hadoop2.x中默认是128M.

### 文件大小设置，增大有什么影响？

HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。
**思考：为什么块的大小不能设置的太小，也不能设置的太大？**

- HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。 因而，**传输一个由多个块组成的文件的时间取决于磁盘传输速率**。
- 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。
- 块的大小：10ms×100×100M/s = 100M，如图

![1633162649884](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/161730-231582.png)

> 在企业中  一般128m（中小公司）   256m （大公司）

### 说下Hadoop常用的端口号

**Hadoop2.x**

- `dfs.namenode.http-address`:50070
- `dfs.datanode.http-address`:50075
- `SecondaryNameNode`辅助名称节点端口号：50090
- `dfs.datanode.address`:50010
- `fs.defaultFS`内部访问端口:8020 或者9000
- `yarn.resourcemanager.webapp.address`:8088
- 历史服务器web访问端口：19888

**Hadoop3.x**

- HDFS NameNode 内部通常端口(`fs.defaultFS`)：8020/9000/9820
- HDFS NameNode 对用户的查询端口（`dfs.namenode.http-address`）：9870
- Yarn查看任务运行情况的：8088
- 历史服务器：19888

> 常用的配置文件
>
> 3.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml workers
> 2.x core-site.xml  hdfs-site.xml  yarn-site.xml  mapred-site.xml slaves

### 简单介绍一下搭建Hadoop集群的流程

在正式搭建之前，我们需要准备以下6步：

**「准备工作」**

1. 关闭防火墙
2. 关闭SELINUX
3. 修改主机名
4. ssh无密码拷贝数据
5. 设置主机名和IP对应
6. jdk1.8安装

**「搭建工作:」**

- 下载并解压Hadoop的jar包
- 配置hadoop的核心文件
- 格式化namenode
- 启动....

### HDFS在上传⽂件的时候，如果其中⼀个块突然损坏了怎么办？

其中⼀个块坏了，只要有其它块存在，会⾃动检测还原。

### Map Reduce

1. InputFormat
   1. 默认的是TextInputformat  kv  key偏移量，v :一行内容
   2. 处理小文件CombineTextInputFormat 把多个文件合并到一起统一切片

2. Mapper 
   1. setup()初始化；  
   2. map()用户的业务逻辑； 
   3. clearup() 关闭资源；

3. 分区
   1. 默认分区HashPartitioner ，默认按照key的hash值%numreducetask个数
   2. 自定义分区

4. 排序

   1. 部分排序  每个输出的文件内部有序。
   2. 全排序：  一个reduce ,对所有数据大排序。
   3. 二次排序：  自定义排序范畴， 实现 writableCompare接口， 重写compareTo方法
   4. 流量倒序  按照上行流量 正序


5. Combiner 

   1. 前提：不影响最终的业务逻辑（求和 没问题   求平均值有问题）
   2. 提前聚合map  => 解决数据倾斜的一个方法

6. Reducer

   1. 用户的业务逻辑；
   2. setup()初始化；reduce()用户的业务逻辑； clearup() 关闭资源；
7. OutputFormat

   1. 默认TextOutputFormat  按行输出到文件
   2. 自定义

### 哪些场景才能使⽤Combiner呢？

1. Combiner的输出是Reducer的输⼊，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该⽤
   于那种Reduce的输⼊key/value与输出key/value类型完全⼀致，且不影响最终结果的场景。⽐如累加，最⼤值等。Combiner的使⽤⼀定得慎重，如果⽤好，它对job执⾏效率有帮助，反之会影响reduce的最终结果。
2. combiner最基本是实现本地key的聚合，对map输出的key排序（也就是在map端做一部分聚合操作），value进⾏迭代。
3. combiner的⽬的是减少map⽹络流量。combiner的对象是对于map。combiner具有和reduce相似的功能。只不过
   combiner合并对象，是对于⼀个map。reduce合并对象，是对于多个map。

### NameNode的作⽤，NameNode在启动的时候会做哪些操作？

namenode总体来说是**管理和记录恢复**功能。⽐如管理datanode，保持⼼跳，如果超时则排除。对于上传⽂件都有镜像
images和edits,这些可以⽤来恢复,总的来说是管理所有文件的元数据信息。

**NameNode启动的时候，会加载fsimage**，Fsimage加载过程完成的操作主要是为了：

1. 从fsimage中读取该HDFS中保存的每⼀个⽬录和每⼀个⽂件
2. 初始化每个⽬录和⽂件的元数据信息
3. 根据⽬录和⽂件的路径，构造出整个namespace在内存中的镜像
4. 如果是⽂件，则读取出该⽂件包含的所有blockid，并插⼊到BlocksMap中。

**启动过程如下**

![1633169830635](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/181711-861855.png)

如上图所⽰，namenode在加载fsimage过程其实⾮常简单，就是从fsimage中不停的顺序读取⽂件和⽬录的元数据信息，并在内存中构建整个namespace，同时将每个⽂件对应的blockid保存⼊BlocksMap中，此时BlocksMap中每个block对应的datanodes列表暂时为空。当fsimage加载完毕后，整个HDFS的⽬录结构在内存中就已经初始化完毕，所缺的就是每个⽂件对
应的block对应的datanode列表信息。这些信息需要从datanode的blockReport中获取，所以加载fsimage完毕后，namenode进程进⼊rpc等待状态，等待所有的datanodes发送blockReports。

### 介绍一下HDFS读写流程

**读数据流程**

![1633159973884](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/153254-65470.png)

详细过程：

1. 客户端通过DistributedFileSystem 向NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的DataNode 地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet 为单位来做校验）。
4. 客户端以Packet 为单位接收，先在本地缓存，然后写入目标文件。

**写数据流程**

![1633240013210](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/134654-145286.png)

![1633160050454](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/153411-572238.png)

![1633169437806](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/181039-401280.png)

> HDFS 上传流程，举例说明⼀个256M的⽂件上传过程

1. 客户端通过Distributed FileSystem 模块向NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。
2. NameNode 返回是否可以上传。
3. 客户端请求第一个 Block 上传到哪几个DataNode 服务器上，客户端⾸先根据返回的信息先将⽂件分块(Hadoop2.X版本每⼀个block为 128M，⽽之前的版本为 64M);。
4. NameNode 返回3 个DataNode 节点，分别为dn1、dn2、dn3。
5. 客户端通过FSDataOutputStream 模块请求dn1 上传数据，dn1 收到请求会继续调用dn2，然后dn2 调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3 逐级应答客户端。
7. 客户端开始往dn1 上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet 为单位，dn1 收到一个Packet 就会传给dn2，dn2 传给dn3；dn1 每传一个packet会放入一个应答队列等待应答。
8. 当一个Block 传输完成之后，客户端再次请求NameNode 上传第二个Block 的服务器。（重复执行3-7 步）。

> 存储文件副本的时候遵循机架感知策略：
>
> Hadoop3.1.3副本节点选择:
>
> 1. 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。
> 2. 第二个副本在另一个机架的随机一个节点
> 3. 第三个副本在第二个副本所在机架的随机节点

![1633228735017](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/103856-113171.png)

### 为什么要引入secondary namenode

**思考：NameNode 中的元数据是存储在哪里的？**

首先，我们做个假设，如果存储在NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。

这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode 节点断电，就会产生数据丢失。因此，引入Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits 中。这样，一旦NameNode 节点断电，可以通过FsImage 和Edits 的合并，合成元数据。

但是，如果长时间添加数据到Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage 和Edits 的合并，如果这个操作由NameNode 节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage 和Edits 的合并。

### secondary namenode工作机制

![1633163160445](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/162600-21412.png)

**第一阶段：NameNode启动**

1. 第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
2. 客户端对元数据进行增删改的请求。
3. NameNode记录操作日志，更新滚动日志。
4. NameNode在内存中对数据进行增删改查。

**第二阶段：Secondary NameNode工作**

1. Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。
2. Secondary NameNode请求执行checkpoint。
3. NameNode滚动正在写的edits日志。
4. 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。
5. Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
6. 生成新的镜像文件fsimage.chkpoint。
7. 拷贝fsimage.chkpoint到NameNode。
8. NameNode将fsimage.chkpoint重新命名成fsimage。

> CheckPoint时间设置：
>
> **通常情况下， SecondaryNameNode每隔 一小时执行次 一小时执行次 。**
>
> hdfs-default.xml
>
> <property> 
>
> <name> dfs.namenode.checkpoint.period</<name>
>
> </<value> 3600 s</value>//设置一小时执行一次
>
>  </property >
>
> **一分钟 检查一次操作数，当达到 检查一次操作数，当达到 检查一次操作数，当达到 检查一次操作数，当达到 1百万 时， SecondaryNameNode执行 一次**
>
> <property> 
>
> <name>dfs.namenode.checkpoint.txns</name>
> <value>1000000</value>
> <description>操作动作次数</description>
> </property>
> <property>
> <name>dfs.namenode.checkpoint.check.period</name>
> <value>60s</value>
> <description> 1 分钟检查一次操作次数</description>
> </property>

### DataNode工作机制

![1633229449785](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/105051-961526.png)

1. 一个数据块在DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。
2. DataNode 启动后向NameNode 注册，通过后，周期性（6 小时）的向NameNode 上报所有的块信息。
3. 心跳是每3 秒一次，心跳返回结果带有NameNode 给该DataNode 的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10 分钟没有收到某个DataNode 的心跳，则认为该节点不可用。
4. 集群运行中可以安全加入和退出一些机器。

### 掉线时限参数设置

![1633229638791](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/105359-282009.png)

> 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。

### 介绍一下MapReduce的Shuffle过程，并给出Hadoop优化的方案(包括：压缩、小文件、集群的优化)

MapReduce数据读取并写入HDFS流程实际上是有10步：

![1633160204104](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/153645-130196.png)

那么到底什么是shuffle过程呢？

1. Map方法之后Reduce方法之前这段处理过程叫**「Shuffle」**
2. Map方法之后，数据首先进入到分区方法（partitioin()），把数据标记好分区，然后把数据发送到环形缓冲区；环形缓冲区默认大小100m，环形缓冲区达到80%时，进行溢写；溢写前对数据进行排序，排序按照对key的索引进行字典顺序排序，排序的手段**「快排」**；溢写产生大量溢写文件，需要对溢写文件进行**「归并排序」**；对溢写的文件也可以进行Combiner操作，前提是汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待Reduce端拉取。
3. 每个Reduce拉取Map端对应分区的数据。拉取数据后先存储到内存中，内存不够了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。在进入Reduce方法前，可以对数据进行分组操作。

### hadoop小文件问题

#### HDFS小文件影响

- 影响NameNode的寿命，因为文件元数据存储在NameNode的内存中,灭一个文件占据namenode的存储空间是150字节，这样的画不管是大文件还是小文件，都需要占10字节，浪费存储空间。
- 影响计算引擎的任务数量，比如每个小的文件都会生成一个Map任务，也就是说在启动map任务的时候，hadoop会针对单独文件进行划分并且启动map任务，这样的画就会启动很多map任务，非常影响集群的性能。
- 小文件多的话非常影响性能，会导致一台namenode宕机后，另一台namenode长时间无法启动。

**小文件给hadoop集群带来的瓶紧**

![1633241428595](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1633241428595.png)

1. 磁盘上面存放过多的小文件。磁盘io速度很慢，所以hadoop集群在随机从磁盘上面读取小文件的话非常慢，定位每一个文件会花费大量时间，但是读取文件只需要一小段时间。
2. 每一个小文件开启一个map和reduce，那么每一次开启和关闭erduce会花费大量的时间。
3. 集群中的资源有限，所以大部分任务是处于等待资源的状态，拖慢系统的运行速度。

#### 如何解决小文件

针对小文件，我们应该如何处理呢？

**数据输入小文件处理**

- 合并小文件：对小文件进行归档（Har）、自定义Inputformat将小文件存储成SequenceFile文件。
- 采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景
- 对于大量小文件Job，可以开启JVM重用

**Map阶段**

- 增大环形缓冲区大小。由100m扩大到200m，
- 增大环形缓冲区溢写的比例。由80%扩大到90%
- 减少对溢写文件的merge次数。（10个文件，一次20个merge）
- 不影响实际业务的前提下，采用Combiner提前合并，减少 I/O

**Reduce阶段**

- 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致 Map、Reduce任务间竞争资源，造成处理超时等错误。
- 设置Map、Reduce共存：调整 `slowstart.completedmaps` 参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间，也即是不必等待map任务执行完成后才执行reduce任务。
- 规避使用Reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。
- 增加每个Reduce去Map中拿数据的并行数
- 集群性能可以的前提下，增大Reduce端存储数据内存的大小

**IO 传输**

- 采用数据压缩的方式，减少网络IO的的时间
- 使用SequenceFile二进制文件

**整体**

- MapTask默认内存大小为1G，可以增加MapTask内存大小为4G
- ReduceTask默认内存大小为1G，可以增加ReduceTask内存大小为4-5g
- 可以增加MapTask的cpu核数，增加ReduceTask的CPU核数
- 增加每个Container的CPU核数和内存大小
- 调整每个Map Task和Reduce Task最大重试次数

**文件压缩**

![1633160722552](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1633160722552.png)

> 如果面试过程问起，我们一般回答压缩方式为Snappy，特点速度快，缺点无法切分（可以回答在链式MR中，Reduce端输出使用bzip2压缩，以便后续的map任务对数据进行split）

**行式存储和列式存储**

行式存储并不会破坏原来数据存储的行信息，行式存储对于条件查询，性能是比较差的，因为需要遍历表中所有数据。

![1633243363844](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/144244-269611.png)

对于列时存储，如果是条件查询，因为列上建立有索引，所以查询起来只会操作对应的列，磁盘io压力小很多。

![1633243494835](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/144456-160161.png)

### 介绍一下 Yarn 的 Job 提交流程

这里一共也有两个版本，分别是详细版和简略版，具体使用哪个还是分不同的场合。正常情况下，将简略版的回答清楚了就很OK，详细版的最多做个内容的补充：

![1633160796432](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1633160796432.png)

**简略版步骤**

1. `client`向RM提交应用程序，其中包括启动该应用的`ApplicationMaster`的必须信息，例如`ApplicationMaster`程序、启动`ApplicationMaster`的命令、用户程序等
2. `ResourceManager`启动一个`container`用于运行`ApplicationMaster`
3. 启动中的`ApplicationMaster`向`ResourceManager`注册自己，启动成功后与RM保持心跳
4. `ApplicationMaster`向`ResourceManager`发送请求,申请相应数目的`container`
5. 申请成功的`container`，由`ApplicationMaster`进行初始化。`container`的启动信息初始化后，AM与对应的`NodeManager`通信，要求NM启动`container`
6. `NM`启动`container`
7. `container`运行期间，`ApplicationMaster`对`container`进行监控。`container`通过RPC协议向对应的AM汇报自己的进度和状态等信息
8. 应用运行结束后，`ApplicationMaster`向`ResourceManager`注销自己，并允许属于它的`container`被收回

**详细过程**

![1633161168192](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/145514-729440.png)

### 介绍下Yarn默认的调度器，调度器分类，以及它们之间的区别

Hadoop调度器主要分为三类：

- FIFO Scheduler：先进先出调度器：优先提交的，优先执行，后面提交的等待【生产环境不会使用】
- Capacity Scheduler：容量调度器：允许创建多个任务对列，多个任务对列可以同时执行。但是一个队列内部还是先进先出，每个队列可配置⼀定的资源量，每个队列采⽤FIFO, 为了防⽌同⼀个⽤户的作业独占资源，那么调度器会对同⼀个⽤户提交的作业所占资源进⾏限定，⾸先按以下策略选择⼀个合适队列：计算每个队列中正在运⾏的任务数与其应该分得的计算资源之间的⽐值，选择⼀个该⽐值最⼩的队列；然后按以下策略选择该队列中⼀个作业：按照作业优先级和提交时间顺序选择，同时考虑⽤户资源量限制和内存限制。。【Hadoop2.7.2默认的调度器】
- Fair Scheduler：公平调度器：第一个程序在启动时可以占用其他队列的资源（100%占用），当其他队列有任务提交时，占用资源的队列需要将资源还给该任务。还资源的时候，效率比较慢，同⼀队 列中的作业公平共享队列中所有资源。【CDH版本的yarn调度器默认】

### 了解过哪些Hadoop的参数优化

我们常见的**「Hadoop参数调优」**有以下几种：

- 在hdfs-site.xml文件中配置多目录，最好提前配置好，否则更改目录需要重新启动集群,不配置多目录会导致所有数据文件全部存储在一个目录中。
- NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作

~~~~ java
dfs.namenode.handler.count=20 * log2(Cluster Size)
  //比如集群规模为10台时，此参数设置为60
~~~~

- 编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟
- 服务器节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量
- 单个任务可申请的最多物理内存量，默认是8192（MB）

### 了解过Hadoop的基准测试吗?

们搭建完Hadoop集群后需要对HDFS读写性能和MR计算能力测试。测试jar包在hadoop的share文件夹下。

### 你是怎么处理Hadoop宕机的问题的?

- 如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：`yarn.scheduler.maximum-allocation-mb`（单个任务可申请的最多物理内存量，默认是8192MB）。

- 如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。

### 你是如何解决Hadoop数据倾斜的问题的，能举个例子吗?

1. 提前在map进行combine，减少传输的数据量
   1. 在Mapper加上combiner相当于提前进行reduce，即把一个Mapper中的相同key进行了聚合，减少shuffle过程中传输的数据量，以及Reducer端的计算量。
   2. 如果导致数据倾斜的key 大量分布在不同的mapper的时候，这种方法就不是很有效了
2. 数据倾斜的key 大量分布在不同的mapper,在这种情况，大致有如下几种方法：

**「局部聚合加全局聚合」**

第一次在map阶段对那些导致了数据倾斜的key 加上1到n的随机前缀，这样本来相同的key 也会被分到多个Reducer 中进行局部聚合，数量就会大大降低。第一次执行是做预处理，尽量让数据分散：

第二次mapreduce，去掉key的随机前缀，进行全局聚合。

**「思想」**：二次mr，第一次将key随机散列到不同 reducer 进行处理达到负载均衡目的。第二次再根据去掉key的随机前缀，按原key进行reduce处理。**这个方法进行两次mapreduce，性能稍差**

**「增加Reducer，提升并行度」**

~~~ java
JobConf.setNumReduceTasks(int)
~~~

**「实现自定义分区」**

根据数据分布情况，自定义散列函数，将key均匀分配到不同Reducer

### HAnamenode 是如何工作的? 

![1633163094225](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/02/162454-597162.png)

ZKFailoverController主要职责

1. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态。
2. 会话管理：如果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主NN，同时标记状态为Active。
3. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN。
4. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态

> NameNode的HA⼀个备⽤，⼀个⼯作，且⼀个失败后，另⼀个被激活。他们通过journal node来实现共享数据。

### 简单说⼀下hadoop的map-reduce编程模型

1. map task会从本地⽂件系统读取数据，转换成key-value形式的键值对集合。使⽤的是hadoop内置的数据类型，⽐如
   longwritable、text等。
2. 将键值对集合输⼊mapper进⾏业务处理过程，将其转换成需要的key-value在输出之后会进⾏⼀个partition分区操作，
   默认使⽤的是hashpartitioner，可以通过重写hashpartitioner的getpartition⽅法来⾃定义分区规则。
3. 会对key进⾏进⾏sort排序，grouping分组操作将相同key的value合并分组输出，在这⾥可以使⽤⾃定义的数据类型，重
   写WritableComparator的Comparator⽅法来⾃定义排序规则，重写RawComparator的compara⽅法来⾃定义分组规则
4. 进⾏⼀个combiner归约操作，其实就是⼀个本地段的reduce预处理，以减⼩后⾯shufle和reducer的⼯作量
   reduce task会通过⽹络将各个数据收集进⾏reduce处理，最后将数据保存或者显⽰，结束整个job。

### hadoop的TextInputFormat作⽤是什么，如何⾃定义实现？

InputFormat会在map操作之前对数据进⾏两⽅⾯的预处理

1. 是getSplits，返回的是InputSplit数组，对数据进⾏split分⽚，每⽚交给map操作⼀次
2. 是getRecordReader，返回的是RecordReader对象，对每个split分⽚进⾏转换为key-value键值对格式传递给map，常⽤的InputFormat是TextInputFormat，使⽤的是LineRecordReader对每个分⽚进⾏键值对的转换，以⾏偏移量作为键，
   ⾏内容作为值。
3. ⾃定义类继承InputFormat接⼜，重写createRecordReader和isSplitable⽅法 在createRecordReader中可以⾃定义分隔符

### map-reduce程序运⾏的时候会有什么⽐较常见的问题？

⽐如说作业中⼤部分都完成了，但是总有⼏个reduce⼀直在运⾏。这是因为这⼏个reduce中的处理的数据要远远⼤于其他的reduce，可能是因为对键值对任务划分的不均匀造成的数据倾斜。解决的⽅法可以在分区的时候重新定义分区规则对于value数据很多的key可以进⾏拆分、均匀打散等处理，或者是在map端的combiner中进⾏数据预处理的操作。

### Hadoop性能调优？

1. 调优可以通过系统配置、程序编写和作业调度算法来进⾏。 hdfs的block.size可以调到128/256（⽹络很好的情况下，默认为64）
2. 调优的⼤头：mapred.map.tasks、mapred.reduce.tasks设置mr任务数（默认都是1）

~~~ java
mapred.tasktracker.map.tasks.maximum //每台机器器上的最⼤大map任务数
mapred.tasktracker.reduce.tasks.maximum //每台机器器上的最⼤大reduce任务数
mapred.reduce.slowstart.completed.maps //配置reduce任务在map任务完成到百分之⼏几的时候开始进⼊入
//配置压缩项，消耗cpu提升⽹网络和磁盘io 合理理利利⽤用combiner 。注意重⽤用writable对象
mapred.compress.map.output,mapred.output.compress
~~~

这个⼏个参数要看实际节点的情况进⾏配置，reduce任务是在33%的时候完成copy，要在这之前完成map任务，（map可以提前完成）

**调优角度**

![1633245161347](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1633245161347.png)

**分表：**对于一些大表，并且每一天是增量方式的增加存储数据，这样的话查询一次数据速度会很慢，所以可以采用分表，把一些常用的字段从大表中剥离出来，每天对小表进行操作，然后在更新大表。

**分区表：**比如用于的日志，每一天会产生大量的日志，如果存储在一个表中，会导致表过大，所以可以采取分区方式存储。分区表也是为了减少查询时候的查询范围。

**充分利用中间结果：**也就是针对一张大表，我们可以先查询出一部分数据，然后其他的查询都基于前面的子查询进行，这样显然可以提高效率。

![1633245878464](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/152439-646067.png)

**压缩**：好处是可以减少Io传输的数据量，节省磁盘空间，缺点是需要解压缩，耗费性能。压缩在大数据中的使用场景：输入数据，中间数据，输出数据。

![1633246243331](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/153045-471009.png)

mr过程：

![1633246315837](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/03/153158-889156.png)

1. 使用压缩后的数据作为map端的输入数据
2. map额度输出到reduce端的输入使用压缩
3. reduce端输出的结果可以压缩

上面在不同的阶段，我们需要的压缩比是不一样的，在reduce端我们更希望采用压缩比高的算法，因为更加的节省磁盘的空间，而在shuffle阶段的压缩，我们更新网压缩和解压缩速率高的。在map端的压缩，我们希望支持分片，这样可以有多个map