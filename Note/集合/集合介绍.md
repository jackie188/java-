集合介绍

## Collection(单列集合)

### List(有序,可重复)

#### ArrayList

- 底层数据结构是**数组**,查询快,增删慢（因为：增删后涉及到其他数据的位移）
- 线程不安全,效率高

#### Vector

- 底层数据结构是**数组**,查询快,增删慢（因为：增删后涉及到其他数据的位移）
- 线程安全,效率低

#### LinkedList

- 底层数据结构是**双向链表**,查询慢,增删快
- 线程不安全,效率高

### Set(无序,唯一)

#### HashSet

- 底层数据结构是**哈希表**。
- 哈希表依赖两个方法：hashCode()和equals()，这两个方法老保证存储数据的唯一性。
- 执行顺序：
  - 首先判断hashCode()值是否相同
    - 是：继续执行equals(),看其返回值
      - 是true:说明元素重复，不添加
      - 是false:就直接添加到集合
    - 否：就直接添加到集合
- 最终：自动生成hashCode()和equals()即可

> 我们使用Set集合都是需要去掉重复元素的, 如果在存储的时候逐个equals()比较, 效率较低,哈希算法提高了去重复的效率, 降低了使用equals()方法的次数

![1641371006272](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/162326-350296.png)

#### LinkedHashSet

- 底层数据结构由链表和哈希表组成。
- 由**链表**保证元素有序。
- 由**哈希表**保证元素唯一。

#### TreeSet

- 底层数据结构是**红黑树**。(是一种自平衡的二叉树)
- 如何保证元素唯一性呢?
  - 据比较的返回值是否是0来决定，所以说保证数据元素的唯一性是通过我们的比较函数实现的。
  - 通过比较，如果返回的值等于0，说明两个元素值相等，那么就不进行存储。
- 如何保证元素的排序呢?
  - 两种方式
    - 自然排序(元素具备比较性): 让元素所属的类实现Comparable接口
      - TreeSet类的add()方法中会把存入的对象提升为Comparable类型
      - 调用对象的compareTo()方法和集合中的对象比较(当前存入的是谁,谁就会调用compareTo方法)
      - 根据compareTo()方法返回的结果进行存储
    - 比较器排序(集合具备比较性):让集合接收一个Comparator的实现类对象
      - 创建TreeSet的时候可以制定 一个Comparator
      - 如果传入了Comparator的子类对象, 那么TreeSet就会按照比较器中的顺序排序
      - add()方法内部会自动调用Comparator接口中compare()方法排序
      - 调用的对象(就是当前存入的对象)是compare方法的第一个参数,集合中的对象(已经添加进去的对象)是compare方法的第二个参数
    - 两种方式的区别
      - TreeSet构造函数什么都不传, 默认按照类中Comparable的顺序(没有就报错ClassCastException)
      - TreeSet如果传入Comparator, 就优先按照Comparator

## Map(双列集合)

A:Map集合的数据结构仅仅针对**键**有效，与值无关。
B:存储的是键值对形式的元素，**键唯一，值可重复**。

### HashMap

底层数据结构是:

- jdk1.8以下：（数组+单向链表）哈希表
- jdk1.8+：（数组+[单向链表 / 红黑树]）哈希表，根据情况会选择链表和红黑树之间进行转换

> 线程不安全，效率高

- 哈希表依赖两个方法：hashCode()和equals()
- 执行顺序：
  - 首先判断hashCode()值是否相同
    - 是：继续执行equals(),看其返回值
      - 是true:说明元素重复，不添加
      - 是false:就直接添加到集合
    - 否：就直接添加到集合
- 最终：自动生成hashCode()和equals()即可

### LinkedHashMap

底层数据结构由**链表和哈希表**组成。

- 由链表保证元素有序。
- 由哈希表保证元素唯一。

### Hashtable

底层数据结构是**哈希表**。线程安全，效率低

- 哈希表依赖两个方法：hashCode()和equals()
- 执行顺序：
  - 首先判断hashCode()值是否相同
    - 是：继续执行equals(),看其返回值
      - 是true:说明元素重复，不添加
      -  是false:就直接添加到集合
    - 否：就直接添加到集合
- 最终：自动生成hashCode()和equals()即可

### TreeMap

底层数据结构是**红黑树**。(是一种自平衡的二叉树)

- 如何保证元素唯一性呢?
  - 根据比较的返回值是否是0来决定
- 如何保证元素的排序呢?
  -  两种方式
    -  自然排序(元素具备比较性)：让元素所属的类实现Comparable接口
    - 比较器排序(集合具备比较性)：让集合接收一个Comparator的实现类对象   

> 详情请参考TreeSet

## TreeSet和TreeMap对比

### 相同点：

1. 都是有序集合
2. TreeMap是TreeSet的底层结构
3. 运行速度都比hash慢

### 区别：

1. TreeSet只存储一个对象，而TreeMap存储两个对象Key和Value（仅仅key对象有序）
2. TreeSet中不能有重复对象，而TreeMap中可以存在
3. TreeMap的底层采用红黑树的实现，完成数据有序的插入，排序。

### 红黑树的特点：

1. 每个节点要么是红色/黑色。
2. 根节点是黑色的。
3. 所有的叶节点都是黑色空节点。
4. 每个红色节点的两个子节点都是黑色。（从每个叶子到根的路径上不会有两个连续的红色节点）
5. 从任一节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点。

为了让大家了解 TreeMap 和 TreeSet 之间的关系，下面先看 TreeSet 类的部分源代码：

```java
public class TreeSet<E> extends AbstractSet<E> 
    implements NavigableSet<E>, Cloneable, java.io.Serializable 
 { 
    // 使用 NavigableMap 的 key 来保存 Set 集合的元素
    private transient NavigableMap<E,Object> m; 
    // 使用一个 PRESENT 作为 Map 集合的所有 value。
    private static final Object PRESENT = new Object(); 
    // 包访问权限的构造器，以指定的 NavigableMap 对象创建 Set 集合
    TreeSet(NavigableMap<E,Object> m) 
    { 
        this.m = m; 
    } 
    public TreeSet()                                      // ①
    { 
        // 以自然排序方式创建一个新的 TreeMap，
        // 根据该 TreeSet 创建一个 TreeSet，
        // 使用该 TreeMap 的 key 来保存 Set 集合的元素
        this(new TreeMap<E,Object>()); 
    } 
    public TreeSet(Comparator<? super E> comparator)     // ②
    { 
        // 以定制排序方式创建一个新的 TreeMap，
        // 根据该 TreeSet 创建一个 TreeSet，
        // 使用该 TreeMap 的 key 来保存 Set 集合的元素
        this(new TreeMap<E,Object>(comparator)); 
    } 
    public TreeSet(Collection<? extends E> c) 
    { 
        // 调用①号构造器创建一个 TreeSet，底层以 TreeMap 保存集合元素
        this(); 
        // 向 TreeSet 中添加 Collection 集合 c 里的所有元素
        addAll(c); 
    } 
    public TreeSet(SortedSet<E> s) 
    { 
        // 调用②号构造器创建一个 TreeSet，底层以 TreeMap 保存集合元素
        this(s.comparator()); 
        // 向 TreeSet 中添加 SortedSet 集合 s 里的所有元素
        addAll(s); 
    } 
    //TreeSet 的其他方法都只是直接调用 TreeMap 的方法来提供实现
    ... 
    public boolean addAll(Collection<? extends E> c) 
    { 
        if (m.size() == 0 && c.size() > 0 && 
            c instanceof SortedSet && 
            m instanceof TreeMap) 
        { 
            // 把 c 集合强制转换为 SortedSet 集合
            SortedSet<? extends E> set = (SortedSet<? extends E>) c; 
            // 把 m 集合强制转换为 TreeMap 集合
            TreeMap<E,Object> map = (TreeMap<E, Object>) m; 
            Comparator<? super E> cc = (Comparator<? super E>) set.comparator(); 
            Comparator<? super E> mc = map.comparator(); 
            // 如果 cc 和 mc 两个 Comparator 相等
            if (cc == mc || (cc != null && cc.equals(mc))) 
            { 
                // 把 Collection 中所有元素添加成 TreeMap 集合的 key 
                map.addAllForTreeSet(set, PRESENT); 
                return true; 
            } 
        } 
        // 直接调用父类的 addAll() 方法来实现
        return super.addAll(c); 
    } 
    ... 
 } 
```

从上面代码可以看出，TreeSet 的 ① 号、② 号构造器的都是新建一个 TreeMap 作为实际存储 Set 元素的容器，而另外 2 个构造器则分别依赖于 ① 号和 ② 号构造器，由此可见，TreeSet 底层实际使用的存储容器就是 TreeMap。

与 HashSet 完全类似的是，TreeSet 里绝大部分方法都是直接调用 TreeMap 的方法来实现的，这一点读者可以自行参阅 TreeSet 的源代码，此处就不再给出了。

## 如何选择集合

 **是否是键值对象形式:**

- 是：Map
  - **键是否需要排序:**
    - 是：TreeMap
    - 否：HashMap
  - 不知道，就使用HashMap。
- 否：Collection
  - 元素是否唯一:
    - 是：Set
      - 元素是否需要排序:
        - 是：TreeSet
        - 否：HashSet
      - 不知道，就使用HashSet
  - 否：List
    - 要安全吗:
      - 是：Vector
      -  否：ArrayList或者LinkedList
        - 增删多：LinkedList
        - 查询多：ArrayList
      - 不知道，就使用ArrayList
    - 不知道，就使用ArrayList

## 常见集合方法和遍历方式

    //方法
    Collection:
            add()
            remove()
            contains()
            iterator()
            size()
        
        遍历：
            增强for
            迭代器
            
        |--List
            get()
            
            遍历：
                普通for
        |--Set
    
    Map:
        put()
        remove()
        containskey(),containsValue()
        keySet()
        get()
        value()
        entrySet()
        size()
        
        遍历：
            根据键找值
            根据键值对对象分别找键和值
## HashMap底层实现原理（jdk1.7\jdk1.8+）

> 面试常问特点：

### 底层结构

- jdk1.8以下：HashMap的底层是：数组+链表(单向链表)
- jdk1.8+：HashMap的底层是：数组+[链表(单向链表) / 红黑树 ]

### 线程不安全

put方法没有加锁，所有的方法都没有实现加锁的功能。

### 初始化默认大小

16 【1 << 4】

### 扩容 

**扩容触发机制：**

- 当前存储过的键值对的数量【即HashMap中的一个size属性】必须大于等于阈值（threshold）【注意：阈值=数组length*加载因子】；
- 当前加入的数据是否发生hash冲突
- 加载因子：0.75 当元素存储（使用）到达现有数组的75%的时候进行扩容（例如：下标100的数组，任意75个下标存储后就会扩容）
-  扩大容量：每次在原基础乘以2，扩容后为原来的2倍

~~~java
     /**
     * The default initial capacity - MUST be a power of two.
     */
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
 
     /**
     * The load factor used when none specified in constructor.
     */
    static final float DEFAULT_LOAD_FACTOR = 0.75f;
 
     /**
     * The next size value at which to resize (capacity * load factor).
     * @serial
     */
    // If table == EMPTY_TABLE then this is the initial capacity at which the
    // table will be created when inflated.
    int threshold;
 
     /**
     * Constructs an empty <tt>HashMap</tt> with the default initial capacity
     * (16) and the default load factor (0.75).
     */
    public HashMap() {
        this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);
    }
~~~

准确点来说是 Entry[]数组 （根据Entry的内部结构不同，来断定具体是什么数据结构）

- 数组的特点是：寻址容易，插入和删除困难（因为添加删除会涉及到其他元素的位移）；
- 链表的特点是：寻址困难，插入和删除容易（因为寻址需要一个一个的遍历过去，无法直接定位）；

那么我们能不能综合两者的特性，做出一种寻址容易，插入删除也容易的数据结构？

答案是肯定的，这就是我们要提起的哈希表，哈希表有多种不同的实现方法，我接下来解释的是最常用的一种方法——拉链法，我们可以理解为“链表的数组”

> 问：数组我们都知道，但什么是Entry数组呢？

答：就是一个数组里放的都是Entry

> 问：那这里的Entry又是什么呢？

答：注意：每个集合类里面的存放实现都有可能是不一样的（比如 TreeSet 里面也有 Entry，但是和 HashMap 的 Entry 内部结构实现是不同的）

HashMap 的 Entry 就是我们说的链表结构 (单向链表)

下图中我们可以看到，在HashMap里面有个静态类 Entry<K,V> 也是 key value 形式的；

![1641377339822](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/180901-58989.png)

里面有 next 属性，类型也是Entry<K,V>

![1641377371283](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/180932-584235.png)

这就是一个 Entry 里面嵌套了另一个 Entry（典型的链表结构）

注意：我们可以看到这个 Entry类里面只有一个 Entry<key,value> next 属性，只有下一个，所以这是一个 单向链表

HashMap 结构 ，里面有个Entry（这是用来存放数据的）,Entr就是单链表的节点类型。

**下面是HashMap的结构图，结合上面的代码就很好理解了吧**

![1641377453692](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181055-970758.png)

![1641377485539](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181126-432976.png)

### HashMap中有一个table的全局属性属性，该属性是Enrty数组

数据都存储在这个table数组里面：

![1641377559087](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181240-711053.png)

![1641377574261](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181255-458916.png)

首先执行初始化 ：初始化会加载一些配置信息

初始化完成，如果在调用put方法时，发现table里面没有任何数据，那么会调用 inflateTable(int toSize)

![1641377661359](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181423-320154.png)

下面这个方法会找到大于等于toSize的最小的2的次方数，然后更新阈值。

![1641377688008](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181449-643326.png)

### 【重点】HashMap 是如何添加数据的（我们主要看红色标注区域）

![1641377781679](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/181623-309402.png)

1. 首先拿到我们的 key 算出对应的 HashCode【line：492】
2. 根据 key 和 table.length 调用 indexFor() 来计算出一个数值 i，这个数值是 table 数组的下标【line：493】
3. 重点来了，我们需要比较这个 key 是否已经存在，如果存在则给对应的 value 重新赋值【line：494~502】
   1. 根据计算出来的下标 i 拿到该 table数组 在该下标内的 Entry
   2. 判断 entry 是不是 null ，如果不是则表示该值已存在，拿到值（for遍历过程）
   3. 判断数组中存在的 entry 的 hash 和我们计算出的 hash 是否一致、 key 是否一致
   4. 如果一致，则说明 HashMap 中该 key 已经存在，我们进入 if() 内进行赋值操作，并返回旧的值
4. **如果该数组下标内没有找到对应的 key，我们则调用 addEntry() 方法进行添加【line：505】**

addEntry()方法如下：

1. 先进行扩容机制的判断【line：878~882】
2. 调用createEntry()方法进行添加键值对
3. 进行链表的添加（将旧节点加入新节点的 next 中）
4. 添加完成后为全局变量size++（扩容时会用size进行判断）

大概看一下我们就可以理解，有兴趣的同学可以看一下 （红色区域为添加，蓝色区域后面会讲到）

> **链表的添加方式：**每次都是在**最外层**创建一个**新的链表点**，然后把**旧的链表插入新的点**，所以最外层的（顶层的）永远是最新的

![1641378021724](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/182023-71641.png)

> 也就是每一次都把元素插入在链表的头处。
>
> 在创建Entry节点的时候，传入了一个e节点，在创建的时候，会把节点e连接到尾部。

5. get(key)方法如何实现的呢？

如果你认真的跟着我上面的步骤理解了 put(key,value)方法，我想不用看源码也能大概说出来是如何实现的

1. 我们根据 key 算出 hashCode
2. 用这个 HashCode 再算出，数组中的一个下标
3. 如果这个下标是null 那么我们返回null
4. 如果不是，那我们就遍历对比链表里面的每一个Entry，找到就返回，找不到就返回null

### 【重点】扩容机制源码

> **扩容必须同时满足两个条件：**
>
> > 1.  **存放新值的时候，发现当前已有键值对（key-value）元素的个数（size）必须大于等于阈值（阈值=加载因子\*当前数组长度）**
> > 2.  **存放新值的时候，当前新值数据key发生hash碰撞（当前key计算的hash值换算出来的数组下标位置已经存在值）**

在put()方法中有调用addEntry()方法，这个方法里面是具体的存值，在存值之前还有判断是否需要扩容

> 判断扩容条件：(size >= threshold) && (null != table[bucketIndex])【line：878】
>
> 如果下面两个条件都发生，那么就扩容
>
> 1. 判断当前个数是否大于等于阈值(size >= threshold)
> 2. 当前存放是否发生哈希碰撞(null != table[bucketIndex])
>
> 扩容调用方法：resize(int) 【line：879】
>
> 扩容大小为原先数组大小的两倍 

![1641378842156](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183403-111225.png)

从这里可以看到，每次扩容，容量都变为原始容量的2倍。

> **阈值：threshold**  
>
> **threshold 就是所说的阈值，它是一个全局变量，决定了数组是否进行扩容的判断条件之一【上图中 line：878】** 
>
> **阈值**根据**加载因子**和**数组大小**决定的 
>
> 默认情况下： **阈值=加载因子 \* 当前数组大小**

![1641378940793](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183542-778916.png)

![1641378964856](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183606-699480.png)

### 构造方法

> HashMap的构造函数有4个：
>
> 下图中的3个构造函数可以看到都是调用了同一个构造函数 **public HashMap (int initialCapacity, float loadFactor)**
>
> 代码中选中的是加载因子

![1641379034115](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183715-537880.png)

**public HashMap (int initialCapacity, float loadFactor)**

![1641379078315](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183800-827964.png)

**如果需要扩容，调用扩容的方法：resize(int)**

![1641379166865](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/183928-388113.png)

### 小结

HashMap的扩容需要同时满足两个条件：

1. 存放新值的时候，发现当前已有键值对（key-value）元素的个数（size）必须大于等于阈值（阈值=加载因子*当前数组长度）
2. 存放新值的时候，当前新值数据key发生hash碰撞（当前key计算的hash值换算出来的数组下标位置已经存在值）

因为上面这两个条件，所以存在下面这些情况

1. 就是hashmap在存值的时候（默认大小为16，负载因子0.75，阈值12），可能达到最后存满16个值的时候，再存入第17个值才会发生扩容现象，因为前16个值，每个值在底层数组中分别占据一个位置，并没有发生hash碰撞。【key不触发hash碰撞】
2. 当然也有可能存储更多值（超多16个值，最多可以存26个值）都还没有扩容。原理：前11个值全部hash碰撞，存到数组的同一个位置（这时元素个数小于阈值12，不会扩容），后面所有存入的15个值全部分散到数组剩下的15个位置（这时元素个数大于等于阈值，但是每次存入的元素并没有发生hash碰撞，所以不会扩容），前面11+15=26，所以在存入第27个值的时候才同时满足上面两个条件，这时候才会发生扩容现象。

### HashMap（jdk1.8+）的变化

> 变化如下：
>
> 1. 数据存储结构进行改变（数组+链表+红黑树）
> 2. 对数组下标的定位生成，做了改动，使之更具有分散性（减少经常向同一个下标存储的情况）
> 3. 加入了链表和红黑树相互转变的机制（减少链的深度）

![1641379396305](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/184317-920495.png)

#### 存储key value的内部存储类 Entry 变为 Node 或 TreeNode

~~~java
transient Node<K,V>[] table;
~~~

~~~java
    /**
     * Basic hash bin node, used for most entries.  (See below for
     * TreeNode subclass, and in LinkedHashMap for its Entry subclass.)
     */
    static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }
~~~

**TreeNode**

~~~java
    /**
     * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn
     * extends Node) so can be used as extension of either regular or
     * linked node.
     */
    static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
        TreeNode<K,V> parent;  // red-black tree links
        TreeNode<K,V> left;
        TreeNode<K,V> right;
        TreeNode<K,V> prev;    // needed to unlink next upon deletion
        boolean red;
        TreeNode(int hash, K key, V val, Node<K,V> next) {
            super(hash, key, val, next);
        }
      ......
    }
~~~

#### put方法的改变（加入了红黑树的转变机制）

使用树型结构，而不去使用单向链表

当节点**超过8**时，从**链表转变为红黑树**结构，从而减小链的深度 

当节点**小于6**时，从**红黑树转变为链表**结构

~~~java
     /**
     * The bin count threshold for using a tree rather than list for a
     * bin.  Bins are converted to trees when adding an element to a
     * bin with at least this many nodes. The value must be greater
     * than 2 and should be at least 8 to mesh with assumptions in
     * tree removal about conversion back to plain bins upon
     * shrinkage.
     */
    static final int TREEIFY_THRESHOLD = 8;
 
    /**
     * The bin count threshold for untreeifying a (split) bin during a
     * resize operation. Should be less than TREEIFY_THRESHOLD, and at
     * most 6 to mesh with shrinkage detection under removal.
     */
    static final int UNTREEIFY_THRESHOLD = 6;
~~~

#### **put方法的改变**

![1641379658724](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/05/184739-923322.png)

1. 根据情况判断是否进行初始化【line：628~629】
2. 计算出数组的下标，取出数据存入变量p，判断是否为null，如果为null则直接生成新节点存入，跳至661【line：630~631】
3. 如果该下标中存在数据则进入else【line：632~660】
   1. 下标中先取链表的 顶层 Node节点，判断key是否相同。相同则直接进入line：653进行数据替换【line：634~636】
   2. 判断该下标Node节点类型是否为TreeNode（树结构），如果是则使用内部类TreeNode的putTreeVal进行存储【line：637~638】
   3. 如果传入的key不是 顶层 Node节点，数组下标节点也不是TreeNode（树结构），那么就对该下标下的Node节点进行循环遍历【line：639~652】
   4. 对value值进行替换处理

### LinkedHashMap 底层实现原理

 LinkedHashMap 继承了 HashMap 所以很多方法都是继承来的，但是 LinkedHashMap 又是有序的，我们可以看到构造器中有个 **accessOrder 参数，这个就是来控制有序的**。

![1641464912067](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/182833-51986.png)

## HashTable底层实现原理

面试常问特点：

1. 底层结构

HashTable底层的数据结构是数组+链表（**数组中节点的类型也是Entry类型**）

> jdk1.8以下：HashMap的底层是：数组+链表(单向链表)
>
> jdk1.8+：HashMap的底层是：数组+[链表(单向链表) / 红黑树 ]

2. 线程安全（put方法加了synchronized修饰） ,各个方法都添加上了synchronized方法，但是各个方法组合起来使用，不一定是线程安全的方法。
3. 初始化默认大小：11 

~~~java
     /**
     * Constructs a new, empty hashtable with a default initial capacity (11)
     * and load factor (0.75).
     */
    public Hashtable() {
        this(11, 0.75f);
    }
~~~

4. 扩容 

```java
  扩容触发机制：数据大小【size()】大于等于阈值【count >= threshold】
  if (count >= threshold) {
            // Rehash the table if the threshold is exceeded
            rehash();

            tab = table;
            hash = key.hashCode();
            index = (hash & 0x7FFFFFFF) % tab.length;
}

  加载因子：0.75

  扩大容量：每次原大小乘以2再加1【(old.length << 1)+1】
  //计算方法
  int newCapacity = (oldCapacity << 1) + 1;
```
> 关于2n+1的扩展，在hashtable选择用取模的方式来进行，那么尽量使用素数、奇数会让结果更加均匀一些，具体证明，可以看看已经证明这点的技术文章
>
> 关于hash，hashmap用2的幂，主要是其还有一个hash过程即二次hash，不是直接用key的hashcode，这个过程打散了数据
> 总体就是一个减少hash冲突，并且找索引效率还要高，实现都是要考量这两因素的 

### put方法

添加方法和HashMap几乎是一样的

![1641465983265](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/184624-618502.png)

1. 用 key 算出对应的 HashCode【line：465】
2. 根据 HashCode和 table.length 计算出 table 数组的下标【line：466】
3. 循环遍历table下标中的entry链表，比较这个 key 是否已经存在，如果存在则给对应的 value 重新赋值【line：468~475】
4. 如果该数组下标内没有对应的key，我们则调用 addEntry() 方法进行添加【line：477】

> 这里插入一个元素，采用的是头插法

![1641466075907](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/184756-194223.png)

### 扩容方法rehash()

![1641466164114](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/184925-234419.png)

## TreeMap 底层实现原理

1. 了解结构 

TreeMap 有一个 Entry<k,v> root 属性 （用来存放数据的）

Entry 类型又有三个 Entry 类型的 left、right、parent 属性,和自己的 key、value属性；（用来存放 树结构 左、右、父的对象数据，还有自己的值；color 属性是当前树节点的颜色）

由此可见 Entry 类型是个 红黑树 的结构，而 TreeMap 里面存储的是 Entry 结果自然就是红黑树.

~~~java
    /**
     * The comparator used to maintain order in this tree map, or
     * null if it uses the natural ordering of its keys.
     *
     * @serial
     */
    private final Comparator<? super K> comparator;

    private transient Entry<K,V> root;

    /**
     * The number of entries in the tree
     */
    private transient int size = 0;

    /**
     * The number of structural modifications to the tree.
     */
    private transient int modCount = 0;
~~~

TreeMap中的节点类型

~~~java
    /**
     * Node in the Tree.  Doubles as a means to pass key-value pairs back to
     * user (see Map.Entry).
     */

    static final class Entry<K,V> implements Map.Entry<K,V> {
        K key;
        V value;
        Entry<K,V> left;
        Entry<K,V> right;
        Entry<K,V> parent;
        boolean color = BLACK;

        /**
         * Make a new cell with given key, value, and parent, and with
         * {@code null} child links, and BLACK color.
         */
        Entry(K key, V value, Entry<K,V> parent) {
            this.key = key;
            this.value = value;
            this.parent = parent;
        }
    }
~~~

## ArrayList 底层实现原理

面试常问特点：

1. 底层结构：数组 
2. 线程不安全（add方法没有加锁）  
3. 初始化默认大小：10 
4. 扩容

```java
  扩容触发机制：当存储第11个数据时，11超过了默认的10，就会触发扩容

  扩大容量：每次在原基础上增加0.5倍，扩容后为原来的1.5倍
```
**底层数据结构**

~~~java
     /**
     * Default initial capacity.
     */
    private static final int DEFAULT_CAPACITY = 10;
 
    /**
     * Shared empty array instance used for empty instances.
     */
    private static final Object[] EMPTY_ELEMENTDATA = {};
 
     /**
     * Constructs an empty list with an initial capacity of ten.
     */
    public ArrayList() {
        super();
        this.elementData = EMPTY_ELEMENTDATA;
    }
~~~

### 扩容机制源码

ensureCapacityInternal() 方法

1. 在调用Add方法时，会先通过 ensureCapacityInternal() 方法确保当前ArrayList维护的数组具有存储新元素的能力【Line:440】
2. ensureCapacityInternal() 判断ArrayList默认的元素存储数据是否为空，为空则设置最小要求的存储能力为必要存储的元素和默认存储元素个数的两个数据之间的最大值，然后调用ensureExplicitCapacity方法实现这种最低要求的存储能力【Line:208】
3. 如果最低要求的存储能力 > ArrayList 已有的存储能力，这就表示ArrayList的存储能力不足，因此需要调用 grow();方法进行扩容 【Line:215】

![1641466570811](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/185612-453997.png)

![1641466605316](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/185646-254368.png)

扩容方法源码（扩容后为原来的1.5倍）

### grow()方法 

1. 数字转换为二进制，使用向右位移符 >> 移动所有二进制数，实现除法（源码中 oldCapacity >> 1表示所有二进制向右移动1位，表示除以2，移出去的数直接忽略）
2. 新size = 旧的size + 扩大的size（Line：236）
3. 创建一个新的数组
4. 通过Arrays.copyOf方法，将原数组的数据复制到新数组

![1641466647870](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/185728-620715.png)

### 总结：

举例说明：添加20个元素到ArrayList中 

当第一次插入元素时才分配10（默认）个对象空间。之后扩容会按照1.5倍增长。

也就是当添加第11个数据的时候，Arraylist继续扩容变为10*1.5=15；

当添加第16个数据时，继续扩容变为15 * 1.5 =22个；
## Vector 底层实现原理

面试常问特点：

1. 底层结构：数组 

2. 线程安全（add方法添加synchronized锁） 
3. 初始化默认大小：10 

      扩容触发机制：当存储第N+1个数据时，N+1超过了先前的数组最大个数N，就会触发扩容
    
      扩大容量：每次在原基础上增加1倍，也就是总大小为原来的2倍
**初始化大小**

~~~java
    /**
     * Constructs an empty vector so that its internal data array
     * has size {@code 10} and its standard capacity increment is
     * zero.
     */
    public Vector() {
        this(10);
    }
~~~

### 扩容机制源码

**ensureCapacityHelper() 方法** 

基本上ArrayList一样

![1641466782756](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/185943-820420.png)

![1641466800728](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/190001-542779.png)

扩容方法源码（扩容后为原来的2倍）

### grow()方法 

1. (capacityIncrement > 0) ? capacityIncrement : oldCapacity（大多情况会返回 oldCapacity）
2. 新size = 旧的size + 旧的size（Line：256）
3. 创建一个新的数组
4. 通过Arrays.copyOf方法，将原数组的数据复制到新数组

![1641466848878](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/190049-249582.png)

### **总结：**

举例说明：添加40个元素到 Vector 中 

当第一次插入元素时才分配10（默认）个对象空间。之后扩容会按照2倍增长。

也就是当添加第11个数据的时候，Vector 继续扩容变为10*2=20；

当添加第21个数据时，继续扩容变为20 * 2 =40个；

## LinkedList 底层实现原理

1. 了解结构。我们进入类中，看到它的变量

LinkedList 有两个 Node 类型的 first、last属性，和自己的size；（用来存放第一个和最后一个，还有总大小的值）

Node 类型又有两个 Node 类型的 next、prev属性,和自己的 item 属性；（用来存放 前一个和后一个对象数据，还有自己的值）

由此可见 Node 类型是个 双向链表 的结构，而 LinkedList 里面存储的是 Node 结果自然就是 双向链表

![1641466968240](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202201/06/190249-986646.png)

~~~java
    transient int size = 0;

    /**
     * Pointer to first node.
     * Invariant: (first == null && last == null) ||
     *            (first.prev == null && first.item != null)
     */
    transient Node<E> first;

    /**
     * Pointer to last node.
     * Invariant: (first == null && last == null) ||
     *            (last.next == null && last.item != null)
     */
    transient Node<E> last;
~~~

**内部节点类型**

~~~java
    private static class Node<E> {
        E item;
        Node<E> next;
        Node<E> prev;

        Node(Node<E> prev, E element, Node<E> next) {
            this.item = element;
            this.next = next;
            this.prev = prev;
        }
    }
~~~

## HashMap、Hashtable、ConcurrentHashMap的原理与区别

### HashTable

- **底层数组+链表实现(节点类型是Entry)**，无论key还是value都**不能为null**，线程**安全**，实现线程安全的方式是在修改数据时锁住整个HashTable，效率低，ConcurrentHashMap做了相关优化。
- 初始size为**11**，扩容：newsize = olesize*2+1
- 计算index的方法：index = (hash & 0x7FFFFFFF) % tab.length（key的哈希值对数组的长度取余）

### HashMap

- jdk1.7底层数组+链表实现，可**以存储null键和null值，但是null键只能有一个**，线程**不安全**,在jdk1.8中底层的实现是数组+链表+红黑树。
- 初始size为**16**，扩容：newsize = oldsize*2，size一定为2的n次幂，每一次扩容容量变为原始容量的2倍。
- 扩容针对整个Map，每次扩容时，原来数组中的元素依次重新计算存放位置，并重新插入。
- 插入元素后才判断该不该扩容，有可能无效扩容（插入后如果扩容，如果没有再次插入，就会产生无效扩容）
- 扩容需要满足两个条件，数组中的容量超过阈值，另一个条件是当前插入的元素是否发生了冲突。
- 计算index方法：index = hash & (tab.length – 1)，也就是key的哈希值对数组长度-1取余。

#### HashMap的初始值还要考虑加载因子:

- **哈希冲突**：若干Key的哈希值按数组大小取模后，如果落在同一个数组下标上，将组成一条Entry链，对Key的查找需要遍历Entry链上的每个元素执行equals()比较。查询的时间复杂度是o(n)，但是在1.8版本中，为了加快查找效率，引入了红黑树。

- **加载因子**：为了降低哈希冲突的概率，默认当HashMap中的键值对达到数组大小的75%时，即会触发扩容。因此，如果预估容量是100，即需要设定100/0.75＝134的数组大小。

- **空间换时间**：如果希望加快Key查找的时间，还可以进一步降低加载因子，加大初始大小，以降低哈希冲突的概率。

HashMap和Hashtable都是用hash算法来决定其元素的存储，因此HashMap和Hashtable的hash表包含如下属性：

- 容量（capacity）：hash表中桶的数量
- 初始化容量（initial capacity）：创建hash表时桶的数量，HashMap允许在构造器中指定初始化容量
- 尺寸（size）：当前hash表中记录的数量
- 负载因子（load factor）：负载因子等于“size/capacity”。负载因子为0，表示空的hash表，0.5表示半满的散列表，依此类推。轻负载的散列表具有冲突少、适宜插入与查询的特点（但是使用Iterator迭代元素时比较慢）

除此之外，hash表里还有一个“负载极限”，“负载极限”是一个0～1的数值，“负载极限”决定了hash表的最大填满程度。当hash表中的负载因子达到指定的“负载极限”时，hash表会自动成倍地增加容量（桶的数量），并将原有的对象重新分配，放入新的桶内，这称为rehashing。

HashMap和Hashtable的构造器允许指定一个负载极限，HashMap和Hashtable默认的“负载极限”为0.75，这表明当该hash表的3/4已经被填满时，hash表会发生rehashing。

“负载极限”的默认值（0.75）是时间和空间成本上的一种折中：

- 较高的“负载极限”可以降低hash表所占用的内存空间，但会增加查询数据的时间开销，而查询是最频繁的操作（HashMap的get()与put()方法都要用到查询）
- 较低的“负载极限”会提高查询数据的性能，但会增加hash表所占用的内存开销

程序猿可以根据实际情况来调整“负载极限”值。

### **ConcurrentHashMap**

- 底层采用分段的**数组+链表**实现，线程**安全**
- 通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。)
- Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术
- 有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁
- 扩容：段内扩容（段内元素超过该段对应Entry数组长度的75%触发扩容，不会对整个Map进行扩容），插入前检测需不需要扩容，有效避免无效扩容

Hashtable和HashMap都实现了Map接口，但是Hashtable的实现是基于Dictionary抽象类的。Java5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。

HashMap基于哈希思想，实现对数据的读写。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，然后找到bucket位置来存储值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞时，对象将会储存在链表的下一个节点中。HashMap在每个链表节点中储存键值对对象。当两个不同的键对象的hashcode相同时，它们会储存在同一个bucket位置的链表中，可通过键对象的equals()方法来找到键值对。如果链表大小超过阈值（TREEIFY_THRESHOLD,8），链表就会被改造为树形结构。

在HashMap中，null可以作为键，这样的键只有一个，但可以有一个或多个键所对应的值为null。**当get()方法返回null值时，即可以表示HashMap中没有该key，也可以表示该key所对应的value为null**。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个key，应该用**containsKey()**方法来判断。而在Hashtable中，无论是key还是value都不能为null。

Hashtable是线程安全的，它的方法是同步的，可以直接用在多线程环境中。而HashMap则不是线程安全的，在多线程环境中，需要手动实现同步机制。

Hashtable与HashMap另一个区别是HashMap的迭代器（Iterator）是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。

![1641468842189](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1641468842189.png)

从类图中可以看出来在存储结构中ConcurrentHashMap比HashMap多出了一个类Segment，而Segment是一个可重入锁。

ConcurrentHashMap是使用了锁分段技术来保证线程安全的。

**锁分段技术**：首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 

ConcurrentHashMap提供了与Hashtable和SynchronizedMap不同的锁机制。Hashtable中采用的锁机制是一次锁住整个hash表，从而在同一时刻只能由一个线程对其进行操作；而ConcurrentHashMap中则是一次锁住一个桶。

ConcurrentHashMap默认将hash表分为16个桶，诸如get、put、remove等常用操作只锁住当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有16个写线程执行，并发性能的提升是显而易见的。