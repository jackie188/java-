# 分布式技术详解

[TOC]

## 分布式的发展历程

### 单点集中式
特点：App、DB、FileServer都部署在⼀台机器上。并且访问请求量较少

![1625660142295](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/11/120110-335300.png)

### 应⽤服务和数据服务拆分

特点：App、DB、FileServer分别部署在独⽴服务器上。并且访问请求量较少
![1625660184796](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1625660184796.png)

### 使⽤缓存改善性能

特点：数据库中频繁访问的数据存储在缓存服务器中，减少数据库的访问次数，降低数据库的压⼒

![1625660223077](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/28/093851-285574.png)

### 应⽤服务器集群

特点：多台应⽤服务器通过负载均衡同时对外提供服务，解决单台服务器处理能⼒上限的问题

![1625660259286](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/07/201741-932882.png)

### 数据库读写分离

特点：数据库进⾏读写分离（主从）设计，解决数据库的处理压⼒

![1625660310434](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1625660310434.png)

### 反向代理和CDN加速

特点：采⽤反向代理和CDN加快系统的访问速度

![1625660354941](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/28/094801-600751.png)

### 分布式⽂件系统和分布式数据库

特点：数据库采⽤分布式数据库，⽂件系统采⽤分布式⽂件系统

随着业务的发展，最终数据库读写分离也将⽆法满⾜需求，需要采⽤分布式数据库和分布式⽂件系统来⽀撑，分布式数据库是数据库拆分后的最后⽅法，只有在单表规模⾮常庞⼤的时候才使⽤，更常⽤的数据库拆分⼿段是业务分库，将不同业务的数据库部署在不同的机器上

![1625660430595](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/07/202030-666889.png)

## 分布式技术详解

1. 并发性

2. 分布性
    ⼤任务拆分成多个任务部署到多台机器上对外提供服务
3. 缺乏全局时钟
    时间要统⼀
4. 对等性
    ⼀个服务部署在多台机器上是⼀样的，⽆任何差别
5. 故障肯定会发⽣
    硬盘坏了 CPU烧了....

## 分布式事务

### ACID特性

- 原⼦性（Atomicity）：⼀个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执⾏过程中发⽣错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执⾏过⼀样。
- ⼀致性（Consistency）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表⽰写⼊的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以⾃发性地完成预定的⼯作。⽐如A有500元，B有300元，A向B转账100，⽆论怎么样，A和B的总和总是800元
- 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进⾏读写和修改的能⼒，隔离性可以防⽌多个事务并发执⾏时由于交叉执⾏⽽导致数据的不⼀致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串⾏化（Serializable）。
- 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 2P/3P

2P= Two Phase commit ⼆段提交（RDBMS（关系型数据库管理系统）经常就是这种机制，保证强⼀致性）

3P= Three Phase commit 三段提交

> 说明：2P/3P是为了保证事务的ACID（原⼦性、⼀致性、隔离性、持久性）

#### 2P的两个阶段

**阶段1：提交事务请求（投票阶段）询问是否可以提交事务**

![1625660757010](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/07/202557-585940.png)

**阶段2：执⾏事务提交（commit、rollback） 真正的提交事务**

![1625660794340](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1625660794340.png)

#### 3P的三个阶段

- 阶段1：是否提交-询问是否可以做事务提交
- 阶段2：预先提交-预先提交事务
- 阶段3：执⾏事务提交（commit、rollback）真正的提交事务

![1625660845117](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/28/100926-10421.png)

说明：3P把2P的阶段⼀拆分成了前⾯两个阶段，先询问一遍是否可以提交，然后在做预提交命令。

### CAP理论

**⼀致性（Consistency）**：

分布式数据库的数据保持⼀致，如果一个系统对一个写操作返回成功，那么之后的读请求必须返回这个新的数据，如果返回失败，那么所有的读操作都不能读取到这个新的数据， 对调用者而言保证了数据的一致性。

**可⽤性（Availability）**：

任何⼀个节点挂了，其他节点可以继续对外提供服务，所有的读写请求在一定的时间内可以得到响应，可终止，不会一直等待。不会因为一台节点挂掉而导致整个集群无法对外提供服务。

**分区容错性（⽹络分区，存在节点间网络的传输）Partition tolerance**：

⼀个数据库所在的机器坏了，如硬盘坏了，数据丢失了，可以新增⼀台机器，然后从其他正常的机器把备份的数据同步过来，在网络分区的情况下，被分隔的节点仍能够对外提供服务。

在单点服务的情况下，CAP理论没有什么问题，因为没有节点之间的网络传输，也就是没有P的存在，但是在分布式的情况下，由于分区容错性必然存在，但是CAP三者有不能共存，所以在分布式架构中,p是一定要保证的，也就是只能从C,A中取其一。也就是只能保证CP或者AP，也就是说A，C在P存在的情况下模式不能共存的。可以考虑一种情景，加入节点D和节点E之间存在网络传输，也就是存在网络分区，P存在的前提，并且两个节点之间的网络不可达，那么如果要保证A，也就是数据的一致性，那么此时就不能对外提供服务，必须保证网络可达后数据同步完成后才能对外提供服务，也就是满足可用性，试想，如果数据不一致，那么对外提供服务使用的数据就是旧的数据。另外一种情况是如果要保证服务的可用性，那么即使数据的一致性没有得到保证也要对外提供服务，那么此时就不能考虑数据的一致性，也就是不能等待数据同步完成后提供服务，必须把对外提供服务放在第一位，这个时候只能不考虑数据的一致性，使用旧的数据对外提供服务，所以在P存在的情况下，C和A只能取其一。

所以目前存在的中间件，只有CP或者AP两种架构。

CAP理论的特点：CAP只能满.其中2条

![1625660966982](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/28/101642-767012.png)

- CA(放弃P)：将所有的数据放在⼀个节点。满⾜⼀致性、可⽤性。
- AP(放弃C)：放弃强⼀致性，⽤最终⼀致性来保证。
- CP(放弃A)：⼀旦系统遇⻅故障，受到影响的服务器需要等待⼀段时间，在恢复期间⽆法对外提供服务。

**举例说明CAP理论**

举例说明CAP理论：
有3台机器分别有3个数据库分别有两张表,数据都是⼀样的

- Machine1-db1-tbl_person、tbl_order
- Machine2-db2-tbl_person、tbl_order
- Machine3-db3-tbl_person、tbl_order

1. 当向machine1的db1的表tbl_person、tbl_order插⼊数数据时，同时要把插⼊的数据同步到machine2、machine3，这就是⼀致性，也就是多个节点上面的数据要保持一致。
2. 当其中的⼀台机器宕机了，可以继续对外提供服务，把宕机的机器重新启动起来可以继续服务，这就是可⽤性，某一台机器宕机，不会影响对外提供的服务。
3. 当machine1的机器坏了，数据全部丢失了，不会有任何问题，因为machine2和machine3上还有数据，重新加⼀台机器machine4，把machine2和machine3其中⼀台机器的备份数据同步过来就可以了，这就是分区容错性，数据存在副本，保证容错机制。

### BASE理论

BASE理论在CAP理论的基础之上做出妥协，弱化了CAP理论。降低了发生分区容错对一致性和可用性的要求。

基本可⽤（bascially available）、软状态（soft state）、最终⼀致性（Eventually consistent）

**基本可⽤：**

在分布式系统出现故障，允许损失部分可⽤性（服务降级、⻚⾯降级），或者响应的时间变长。

**软状态：**

允许分布式系统出现中间状态。⽽且中间状态不影响系统的可⽤性。

- 这⾥的中间状态是指不同的data replication之间的数据更新可以出现延时的最终⼀致性
- 如CAP理论⾥⾯的⽰例，当向machine1的db1的表tbl_person、tbl_order插⼊数数据时，同时要把插⼊的数据同步到machine2、machine3，当machine3的⽹络有问题时，同步失败，但是过⼀会⽹络恢复了就同步成功了，这个同步失败的状态就称为软状态，因为最终还是同步成功了。

例如淘宝下单：加入购物车，待支付，支付中，已支付状态，并不会直接从加入购物车然后一下子变为已支付状态，存在中间状态，但是不会对最终结果造成影响。存在中间状态，可以给系统提供一个缓冲的时间。

**最终⼀致性：**

data replications经过⼀段时间达到⼀致性。节点之间的数据同步可以存在延迟，但是一定的时限之后必须达成数据的一致性，状态变为最终的状态。

> 数据一致性在**CAP理论中指的是强一致性**,**而在BASE理论中指的是最终一致性**,并不是相同意义上的一致性.

### 选举算法Quorum机制，WARO

#### WARO

**WARO是一种简单的副本控制协议，写操作时候，只有当所有的写操作都更新成功之后，这一次写操作才算成功，否则视为失败。优先保证读取成功，任何一个节点读取到的数据都是最新的数据，牺牲了更新服务的可用性，只要有一个副本发生宕机了，写服务就不会成功，但是只要有一个节点存活，那么就可以对外提供读服务。**

kafka中的ack确认机制就是使用waro协议。kafka对WARO协议进行了优化，只需要保证ISR中的节点返回ack即可。

> 简单来说就是更新写操作需要所有的节点全部在线参与，而读取服务只要有一个节点即可提供服务。

#### Quorum机制

10个副本，一次成功更新3个，那么至少读取8个副本的数据，这里面至少有一个副本更新成功数据，可以保证读取到了最新的数据，**无法保证强一致性**，也就是无法实现任何时刻任何用户或者节点都可以读取到最近一次成功提交的副本数据，需要配合一个获取最新成功提交的版本号的metadate服务，这样可以确定最新已经成功提交的版本号，然后从已经读取到的数据中就可以确认最新写入的数据。

> **简单理解就是写操作不要求全部节点全部在线，需要N个节点在线即可，然后读取，然后数据成功写入这N个节点，读取的时候，读取10-N+1个节点数，这样保证有一个节点的数据是更新的。**

在主从架构和选举算法中，上面这两种协议应用都是比较广泛的。

### Paxos一致性算法

这里所说的一致性指的是CAP理论中强一致性。解决的是集群中多个节点之间的数据一致性问题，只是一种算法思想和模型，可以理解为一种协议。raft算法和zookeeper中的zab算法都是借鉴了paxos算法的思想。

Paxos算法解决的是一个分布式系统中如何就某一个值（决议）达成一致，一个典型的场景是，在一个分布式数据库系统中，如果各个节点的初始状态是一致的，每一个节点执行相同的操作序列，那么他们最后也可以得到一个一致的状态，为了保证每一个节点执行相同的操作序列，需要在每一条指令上面执行一个“一致性算法”用来保证每一个节点看到的指令是一致的，在Paxos算法 中，有三种角色：Proposer(提议者)，Acceptor(接受者)，Learners(记录员)。

Proposer提议者：只要Proposer发出的提案Propose被半数以上的Acceptor接受，Proposer就被认为该提案例的value被确定了。

Acceptor接受者：只要Acceptor接受了某一个提案，Acceptor就认为该提案例的value被选定了。

Learner记录员：Acceptor告诉Learner那个value被选中，Learner就认为哪一个value被选定。

Paxos算法分为两个阶段：

阶段一（prepare):

1. Proposer受到client请求或者发现本地有未提交的值，未提交的值可能是某一个节点发生网络拥堵或者是宕机本来应该提交的提案没有被提交，恢复正常后还要继续提交。选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。集群中的每一个节点都可以充当这三种角色。节点每一次都会把提案的编号记录在本地，每一次发出新的提案时候，会对提案编号进行累加。提案编号是递增的序列。
2. Acceptor收到一个编号为N的Prepare请求，如果该轮的paxos（一轮paxos算法确定一个value值）
   1. 本节点已经有已提交（也就是已经持久化的编号）的value记录，对比本地记录的编号和编号N，如果本地持久化的编号大于接受的编号N，那么就拒绝响应，否则，也就是本地编号小于接受的编号N，那么就返回本地该记录的value值和编号。
   2. 如果本地没有已经提交的记录，那么判断本地是否有编号N1（这个编号N1指的是只有有节点发送prepare过来，那么本节点就会记录编号，这个编号是作为节点的初始编号），如果N1>N,那么就拒绝响应，否则将N1改为N（如果没有N1，则记录N），并响应prepare。

第一步其实就是确定最大的编号

阶段二（accept):

1. 如果Proposer收到半数以上的Acceptor对其发出的编号为N的Prepare请求的响应，那么他就会发送一个针对[N,V]提案的Accept请求，给半数以上的Acceptor,V就是收到的响应中编号最大的value,如果响应中不包含任何的value，那么V就由Proposer自己 决定。
2. 如果Acceptor收到的一个针对编号为N的提案的Accept请求，Acceptor对比本地的记录编号，如果小于等于N，则接受该值，并且提交记录value，否则就拒绝请求。

Proposor如果收到大多数的Acceptor响应，那么就选定该value值，并且同步给leaner，使未响应的Acceptor达成一致。

### Raft算法

#### 概念

分布式一致性算法，raft算法会首先选举出leader,leader完全负责replicated log的管理，leader负责接受所有客户的更新请求，然后把更新的数据同步到所有的follower节点，并且在安全的时候执行这些请求，如果leader故障，会从follower中重新选择出新的leader来管理。

三种状态：一个节点任意时刻处于三者之一

- leader：处理所有额度客户端的请求（如果客户端的请求是发送给Follower的，Follower会将请求重新定向给Leader）
- Follower：不会发送任何的请求，只会简单的响应来自Leader或者Candidate的请求。
- Candidate：用于选举产生新的Leader候选人。

Term：任期，Leader从产生的那一刻到重新选举为止为一个任期，每一个节点都维持着当前的任期领导。每一个Leader都维护一个Id，分发给所有的Follower，保证让所有的Follower直到当前的Leader是哪一个节点。这个Id也是递增的序列。

- Id可以防止脑裂的发生（**就是一个集群中同时存在两个Leader节点**），比如由于网络原因，Leader和某一个Follower之间没有心跳，或者说超时，那么此时这个Follower就会变为Candidate，选举Leader，为自己拉票，如果选举成功，那么他自己会生成一个Id,然后分发给集群中的Follower节点，但是如果旧的Leader收到的话，因为这个Id比自己的Id号码大，所以他自己就会重新变为Follower节点。这样可以防止脑裂发生。
- term是递增的，存储在log日志当中的Entry对象中，Entry对象中也会存储Id号，代表当前的entry是在哪一个term时期写入的。
- 每一个时期只能有一个Leader或者没有（选举失败）
- 每一次rpc通信时候传递该Leader的Id号，如果rpc收到的Id号大于本地的号，切换为Follower,小于本地任期号则返回错误信息。

两个RPC通信：

- RequestVote RPC：负责选举，包含参数lastIndex（状态机中存储数据的最后一条数据的索引号）,lastTerm（任期Id号）。
- AppendEntriest RPC：负责数据的交互。

日志序列：每一个节点上维持着一份持久化Log，通过一致性协议算法，保证每一个节点中的log保持一致。并且顺序存放，这样客户端就可以在每一个节点中读取到相同的数据。

状态机：日志序列同步到多数节点时候并且返回成功，leader将该日志提交到状态机，并且在下一次心跳通知所有节点提交状态机（携带最后提交的lastIndex)

**何时出发选举Leader:**

- 集群初始化时候，都是Follower，随机超时，变成Candidate(也就是如果超时，那么一个follower就变为Candidate，开始向其他节点拉票)，发起选举。
- 如果Follower在election timeout内没有收到来自Leader的心跳，那么主动触发选举。

选举过程：从发出选举节点的角度

1. 增加节点本地的term,切换到candidate状态。
2. 投自己一票，其他的节点投票逻辑：每一个节点同一个任期内最多只能投一张票，候选人知道的信息不能够比自己少，也就是状态机中的数据一定是最新的，通过term id和last-log-Index来保证，先来先得。
3. 然后在给其他节点发送RequestVote请求（拉票请求），包含term参数，
4. 等待回复
   1. 如果收到大多数的投票，赢得选举，将自己切换为Leader即可，立刻给所有节点发送心跳消息。
   2. 如果本节点是投票节点，被告知别人当选，切换为Follower状态，（前提是收到的Id比原来旧的Id大，切换为Follower状态）
   3. 一段时间内既没有收到大部分的投票，有没有收到leader的心跳通知，则保持candidate重新发出选举。

日志序列同步：日志序列需要存储在磁盘进行持久化，崩溃时候可以从日志恢复

1. 客户端发送命令给Leader。
2. Leader把日志条目加载自己的日志序列中。Leader不能删除和修改自己的日志，只可以做追加操作。
3. Leader发送AppaenEntries RPC请求给所有的follower。携带了preLogIndex（前一条日志的序列号），preLogTerm(前一条日志的任期号）。每一个节点上面都有这两个参数

follower收到后，进行日志序列的匹配

1. 匹配上则追加到自己的日志序列中
2. 匹配不上则拒绝请求，Leader将日志index调小，重新同步直到匹配上，然后leader将匹配上的数据位置后面的数据全部发送到followerj节点，follower将leader的日志序列覆盖到本地中

一旦新的日志序列条目变为majority的了，也就是多数follower节点响应同步成功，，将日志序列应用到状态机中

1. Leader在状态机李提交自己日志序列的条目，也就是持久化日志，，然后返回结果给客户端
2. Leader下次发送AppendEntries RPC时候，告知follower已经净提交的日志序列的条目信息（lastIndex)
3. follower收到RPC之后，提交到自己的状态机里面。

异常情况：提交状态机时候，如果term为上一任期，必须与当前任期数据一起提交，否则可能出现覆盖已经提交状态机的日志

新选举的leader一定拥有所有已提交状态机的日志条目

- leader在当日志序列条目已经复制到大多数follower机器上面时候，才会提交日志条目
- 而选举出的leader的logIndex必须大于等于大多数节点，因此leader肯定有最新的日志

安全原则

1. 选举安全原则：对于一个给定的任期号，最多只有一个领导人被选举出来
2. 状态机安全原则：如果一个leader已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志。也就是状态机中日志的索引号和所有节点中日志索引号应该是一样的
3. 领导人完全原则：如果某一个日志条目在某一个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中。
4. 领导人只附加原则：领导人绝对不会删除或者覆盖自己的日志，只会增加。
5. 日志匹配原则：如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同。

### ZAB协议

ZAB协议是为分布式协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议，实现分布式数据的一致性，所有客户端的请求都是写入到leader进程当中，然后由Leader同步到其他的节点中，成为Follower节点，在集群数据同步的过程中，如果出现Follower节点崩溃或者Leader进程崩溃时，都会通过Zab协议来保证数据的一致性。

raft算法中，leader负责处理**读写**请求，在ZAB中，**Leader可以处理读写请求，follower可以处理读请求**。在 raft中Leader的选举是放射状的，在ZAB中Leader的选举是网状的。

ZAB协议包括两种基本的模式：崩溃恢复（leader宕机的话重新选举）和消息广播（正常模式）。

**消息广播模式：**

集群中所有事物的请求（写请求），都是由Leader来处理，其他的服务器为Follower，Leader将客户端的事务请求转化为事务Proposal，并且将Proposal分发给集群中其他的Follower。

完成广播之后，Leader等待Follower反馈，当有过半的Follower反馈信息后，Leader将再一次向集群中的Follower广播Commit信息，Commit信息就是确认将之前的Proposal提交。

Leader节点的写入是一个两步的操作，第一步是广播事务操作（客户端的写命令），第二部是广播提交操作，其中过半数指的是反馈的follower节点数>=N/2+1，N是全部的Follower节点数量。

**崩溃恢复：**

- 初始化集群，刚刚启动的时候。
- Leader崩溃，因为故障宕机。
- Leader失去了半数的机器支持，与集群中超过一半的机器断联。

此时开启新一轮的Leader选举，选举产生的Leader会与过半的Follower进行同步，使数据一致，当参与过半的机器同步完成后，就退出恢复模式，然后进入消息广播模式。退出恢复模式的前提就是Leader与集群中半数以上的Follower达成一致。

整个zookeeper集群的一致性保证就是在上面两个状态之间进行切换，当Leader服务正常的时候，就是正常的消息广播模式，当Leader不可用的时候，则进入崩溃恢复模式，崩溃恢复阶段会进行数据的同步，完成之后，重新进入消息广播阶段。

zxid是ZAB协议的一个事务编号，zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端的每一个事务请求，计数器累加1，而高32位则代表Leader周期年代的编号。两部分保证这个数字全局唯一。

Leader周期（epoch)，可以理解为当前集群所处的年代或者周期，每当有一个新的Leader选举出现时候，就会从这个Leader服务器上取出其本地日志中最大事务的zxid，并且从中读取epoch值，然后累加1，以此作为新的周期ID，高32位代表每一代Leader的唯一性，低32位代表了每一代Leader中事务的唯一性。

zookeeper集群中的每一个节点，都处于一下三种状态之一：

- following:服从Leader的命令
- leadering:负责协调事务
- electionlocking:选举状态

### 负载均衡的策略有哪些

#### 轮询

将请求按照先后到达的时间分配到服务器上面，他均衡的对待后面的每一台服务器，而不关心服务器实际的连接数和当前系统的负载量。缺点是每一台服务器可能配置不一样，按照相同方式分配任务，可能导致有的服务器负载量很大，而有的很小，效率低，浪费资源。

#### 加权轮询法

不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此他们的抗压能力也不相同，给配置高负载低的服务器配置更高的加权，让其处理更多的请求，而配置低负载高的机器，给其分配较低的权重，降低其系统的负载，加权轮询可以很好的处理不同配置的服务器负载量不同的问题，并将请求顺序按照权重先后分配给服务器，简单来说就是把轮询方式的优化，给配置高，负载低的服务器分配更多的请求。

#### 随机法

通过系统的随机算法，根据后端服务器的列表大小来随机的选取其中的一台服务器进行访问，由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实实际的效果越来越接近于平均分配调用到后端的每一台服务器，也就是轮询方式的结果，这两种方式效果差别不是很大。

#### 加权随机法

与加权轮训法一样，加权随机法根据后端服务器的配置，系统的负载分配不同的权重，不同的是，他是按照权重随机请求后端服务器，并不是顺序请求。

#### 源地址哈希法

源地址哈希法的思想是根据获取客户端的IP地址，通过哈希函数得到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客户端要访问的服务器的序号，采用源地址哈希法进行负载均衡，统一IP地址的客户端，当后端服务器的列表不发生变化时候，他每次都会映射到同一台后端服务器进行访问。

使用这种方式最大的好处是可以共享session。

#### 最小连接数法

最小连接数算法比较灵活这智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，他是根据后端服务器当前的连接情况，动态的选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能的提高后端服务器的利用效率，将请求合理的分配到每一台后端服务器上。也就是每一台服务器有一个最小的连接数目，如果某一台服务器最小连接数目全部用完，那么新来的连结请求将分配到其他的服务器上面。

### 集群，分布式，SOA和微服务的概念及区别

**单机&集群**

早期的话，通常将服务部署到一台服务器上，由于客户量比较少，所以可以扛得住访问，但是随着客户量的增加，一台服务器不足以应对所有客户的访问，那么就增加多台服务器，把我们的服务部署到多台机器上面，这样可以增大并发访问的量，在这里，这多个节点的地位通常是平等的，每一个节点都可以提供完整的服务，并且每一台服务器上面部署的实例是一致的，所以此时就衍生出了集群的概念。

其实集群强调的就是多台服务器的地位是对等的，而且每一个节点都提供完整的服务。

- 不同的服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或者互备（热备，主从等），指的是同一种组件的多个实例，形成的逻辑上的整体，单个节点可以提供完整的服务，集群是物理形态。

**分布式**

比如集群中的一个节点实例，可能包含很多的功能，A,B,C三个模块的功能。A模块的访问量最大，B,C访问量远远小于模块A的访问量，所以B,C不需要进行扩展，我们可以仅仅对A模块进行分布式部署，将A模块进行单独部署，单独为一个实例。那么A,B,C三个模块工作时候可能需要协调工作，所以分布式强调的是多个节点之间相互协调对外提供服务。

而集群中每一个节点部署的是相同的实例，但是不同节点之间也有协作和交互，所以集群和分布式之间的概念很模糊，没有明确的界限。

- 服务的不同模块部署在不同的服务器上面，单个节点不能提供完整的服务，需要多个节点之间协调提供服务（也可以是相同组件部署在不同的节点上，但是节点之间通过复杂的交换信息协作提供服务），分布式强调的是协调的工作方式。

>  集群强调节点与节点之间的功能对等，对外提供服务，而分布式强调节点与节点之间协调对外提供服务，但是集群多个节点之间也需要交互协作，所以二者之间界限模糊。

#### SOA

如果把模块A,B,C进行分布式部署，可能后面涉及很多模块，多个模块之间存在相互的调用和交互关系，比较复杂，维护成本很高，所以此时就出现了SOA，主要就是为了解决系统之间交互过于复杂的问题，**SOA是面向服务的概念。**

比如系统中多个模块需要进行交互，那么每一个模块都需要维护其他模块所在的地址和其他信息，这样系统臃肿而复杂，而SOA的做法是引入了总线ESB，所有模块之间的交互都是通过总线进行，总线维护各个模块的地址，而每一个模块不在需要维护其他模块的地址信息，这样系统就解耦了，而各个模块只需要知道ESB总线在哪里即可。

但是这也是系统的瓶颈，所有额模块之间的交互都需要通过ESB总线，那么系统的效率就取决于ESB总线，ESB会成为一个单点的瓶颈。可以说微服务的出现就是为了解决单点瓶颈问题。

- 面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能，一个服务通常以独立的形式存在于操作系统的进程中，各个服务器之间通过网络调用。
  - 中心化实现：ESB企业服务总线，各个服务通过ESB总线进行交互，解决异构系统之间的连通性，通过协议的转换，消息解析，消息路由把服务提供者的数据传送到服务的消费者，很重，有一定的逻辑，可以解决一些共用逻辑的问题。
  - 去中心化实现：微服务

#### 微服务

在SOA上做的升华，微服务架构强调的一个重点是业务需求要彻底的组件化和服务化，原有的单个业务系统会拆分为多个可以独立开发，设计，运行的小应用，这些小应用之间通过微服务完成交互和继承

更加强调服务单一职责，还在SOA的范畴之内，只不过把服务的粒度拆解的更加细。

轻量级通信，去掉ESB总线，采用restAPI通信。

## 分布式系统的设计目标

- 可扩展性：通过对服务，存储的扩展，来提高系统的处理能力，通过对多台服务器的协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。

- 高可用：单点故障不会影响整体服务，单点故障指的是系统中某一个组件一旦失效，会让整个系统无法工作。高可用通常指的是单点故障问题。

- 无状态：无状态的服务才能满足部分机器宕机不影响全部，可以随时进行系统扩展的需求。有状态的话一旦节点宕机，会发生数据的丢失。

- 可管理：便于运维，出现问题后能不能及时发现定位。

- 高可靠：同样的请求返回同样的数据，数据的更新能够持久化，数据不会丢失。也就是满足幂等性原则。

## 分布式事务有哪些解决方案

要区别于本地事务，数据库中的事务。

基于XA协议（专门用来解决分布式事务的协议）：两阶段提交和三阶段提交，需要数据库层面支持。mysql支持XA协议。

基于事务补偿机制的：TCC，基于业务层面实现。

本地消息表：基于本地数据库+mq，维护本地状态（进行中），通过mq调用服务，完成后响应一条消息回调，将状态改成完成，需要配合定时任务扫描，重新发送消息调用服务，需要保证幂等性。

基于事务消息：mq(消息中间件来支持事务消息)

### XA协议

对比两阶段提交和三阶段提交有哪些改进

其中TM是事务管理器，负责提交一个一个的事务，起到协调的作用，而RM是处理一个一个的事务。

![1625986678708](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/11/145758-78564.png)

第一阶段（prepare)：每一个参与者执行本地事务但是不进行提交，进入ready状态，并通知协调者已经准备就绪。

第二阶段（commit):当协调者确认每一个参与者都ready之后，通知参与者进行commit操作，如果有参与者fail，则发送rollback命令，各个参与者进行回滚，保证数据的一致性。

问题：

- 单点故障问题：一旦事务管理器出现故障，整个系统不可用（参与者都会阻塞）。
- 数据不一致：在阶段二，如果事务管理器只发送了部分commit消息，此时网络发生异常，那么只有部分参与者接受到commit消息，也就是说只有部分参与者提交了事务，使得系统的数据不一致。
- 响应时间较长：参与者和协调者资源都被锁住，提交或者回滚之后才能够释放。
- 不确定性：当事务管理器发送commit之后，并且此时只有一个参与者收到了commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。

三阶段提交协议：主要是针对两阶段的优化，解决了2PC单点故障的问题，但是性能问题和不一致问题仍然没有根本解决。

三阶段提交在开始有一个预提交命令，可以用来检测是否所有的库都正常在线，用来探测数据库是否存活，如果存活才可以进行接下来的提交，因为如果库宕机，而事务管理器发出了sql命令，那么会锁住资源。

![1625990667094](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202107/28/140320-474379.png)

阶段一用来检测所有的库是否都是正常的，preCommit对应于二阶段提交中的第一阶段，用来发出sql命令，锁定资源。三阶段提交比二阶段提交多了一个探测的命令。探测机制提高了两阶段的成功率。

三阶段提交解决了单点故障问题：如果事务管理器挂掉，那么所有的RM都会阻塞，无法释放资源。所以在三阶段提交中引入一个超时机制

引入了超时机制解决参与者阻塞的问题，如果参与者一直没有收到协调者发送的doCommit命令，那么等待时间超时后，参与者就会进行本地提交，自己进行提交，不在收到协调者控制，2PC只有协调者有超时机制。但是数据不一致和不确定性并没有解决。这里的超时机制是对于参与者而说的，也就是说参与者一直收不到命令，就会进行自动提交。

而对于两阶段提交，如果事务管理器一直也没有收到RM返回的消息，那么等待时间超时，事务管理器就会自动发送回滚命令。

上面两个超时机制针对的对象不一样，针对数据库而言，在两阶段提交中没有超时机制，在三阶段提交中有超时机制。

1. 第一阶段：canCommit阶段，协调者询问事务参与者，是否有能力完成此次事务，
   1. 如果返回yes，那么就进入第二阶段
   2. 有一个返回no或者等待响应超时，则中断事务，并且向所有的参与者发送abort请求。
2. 第二阶段：PreCommit阶段，此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事务操作，参与者执行完成事务操作后（此时属于未提交事务的状态），就会向协调者反馈ACK确认，表示我已经准备好提交了，并等待协调者下一步的命令。
3. 第三阶段:DoCommit阶段，在阶段二中如果所有的参与者节点都返回了ack确认，那么协调者就会从预提交状态转变为提交状态，然后向所有的参与者节点发送doCommit请求，参与者节点在收到提交请求后就会各自执行事务的提交操作，并且向协调者反馈ack确认消息，协调者收到所有参与者的ACK确认消息后完成事务，相反，如果有一个参与者为完成preCommit的反馈ack确认或者反馈超时时候，那么协调者就会向所有的参与者节点发送abort请求，从而中断事务。

> 两阶段=执行sql命令+commit/rollback
>
> 三阶段=canCommit(提高成功率)+preCommit+doCommit+引入参与者超时机制，避免单点故障和参与者锁住资源不释放

## 简述TCC事务模型

TCC(补偿事务)，里面有三种操作：

- Try：做资源预留，是在业务层面的操作，并不需要事务的支持。
- Confirm：提交。
- Cancle：取消事务。

针对每一个操作，都要注册一个与其对应的确认和补偿（撤销）操作。

Try操作业务检查及资源的预留情况，Confirm做业务的确认操作，Cancle实现一个与Try相反的操作既回滚操作，TM首先发起所有额度分支事务的Try操作，任何一个分支事务的Try操作执行失败，TM将会发起所有分支事务的Cancle操作，如果Try操作全部成功，TM将会发起所有分支事务的Confirm操作，其中Confirm/Cancle操作如果执行失败，TM会进行重试。

TCC模型对业务的侵入性比较强，改造的难度较大，每一个操作都需要有Try,Confirm,Cancle三个接口实现。

TCC中会添加事务日志，如果Confirm或者Cancle阶段出错，则会进行重试，所以这两个阶段需要支持幂等性，如果重试失败，则需要人工介入进行恢复处理等。 

## 如何理解RPC

远程过程调用

RPC要求在调用方放置被调用的方法的接口，调用方只要调用了这些接口，就相当于调用了被调用方法的实际方法，十分易用，于是，调用方可以像调用内部接口一样调用远程的方法，而不是用封装参数名和参数值等操作。

包含：

1. 动态代理，封装调用细节
2. 序列化和反序列化，数据的传输与接收
3. 通信，可以选择七层的http，四层的tcp/udp。
4. 异常处理等。

首先，调用方调用的是接口，必须为接口构造一个类的实现，显然，要使用动态代理，这样，调用方的调用就被动态代理接收到了。

第二，动态代理接收到调用后，应该想办法调用远程的实际实现，包括下面几步：

- 识别具体要调用的远程方法的IP，端口。
- 将调用方法的传入参数进行序列化。
- 通过通信将请求发送到远程的方法中。

这样，远程的服务器就接收到了调用方的请求，他应该：

- 反序列化各个调用参数。
- 定位到实际要调用的方法，然后输入参数，执行方法。
- 按照调用的路径返回调用的结果。

## zookeeper

### zk的初始化选举和崩溃选举过程

zxid:事务的ID，leader会将客户端的命令封装为一个proposal对象，这个zxid也会封装在这个对象中，zxid是全局唯一的，表示客户端的一次请求以及请求内容。zxid有两部分内容，第一部分是当前leader的任期，另外一部分是一个递增的序列，表示一个事务的id，这两部分决定了zxid全局唯一。

sid:节点的id，是我们人为指定的，在整个集群中，id也是惟一的。

初始化选举：

先对比zxid，在对比sid,先投票给自己，投票内容为（zxid,sid),zxid是最后一条事务的id。投票过程中，先广播自己的票，获取到广播票的节点，把广播票和自己的票对比，首先对比zxid，谁的zxid大，谁就获胜，如果zxid相同，那么就对比sid，谁的sid大，谁就获胜。如果哪一方输掉，那一方就更改自己的选票，把zxid改为获胜一方的zxid，sid改为获胜一方的sid。

投票箱：每个节点在本地维护自己和其他节点的投票信息，改投票时候需要更新信息，并且广播出去。

节点状态：

- looking:竞选状态
- following:随从状态，同步leader状态，参与投票。
- observing:观察状态，同步leader状态，不参与投票。
- leading:领导者状态。

初始化：没有历史数据，5个节点为例

- 节点1启动，此时只有一台服务器启动，他发出去的请求没有任何响应，所以他的选举状态一直是looking状态。
- 节点2启动，他与节点1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以serverid（sid）值较大的服务器2胜出，但是由于没有达到半数以上，所以服务器1,2还是继续保持looking状态。
- 节点3启动，与1,2节点通信交换数据，服务器3成为服务器 1,2,3中的leader，此时一共有三台服务器选举了3，所以3成为leader.
- 节点4启动，理论上服务器4应该是服务器1,2,3,4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以服务器4只能切换为follower。
- 节点5启动，与节点4一样。

崩溃选举：

- 变更状态，leader故障之后，follower进入looking状态
- 各个节点开始投票，首先投自己（zxid,sid),再广播投票
- 接收到投票，对比zxid和sid,如果本节点小，则将投票该我接受的投票信息，并记录投票信息，重新广播，否则本节点大，则可以不做处理。
- 统计本地投票信息，超过半数，则切换为leading状态并且广播。

### 简述zk的数据模型

可以把zk理解为一个文件系统，zk中每一个节点都是一个znode。

zk中的数据模型是一种树型结构，具有一个固定的根节点（/),可以在根节点下面创建子节点，并且在子节点下继续创建下一级节点，每一个层级使用/隔开，且只能使用绝对路径的方式查找zk的节点，而不可以使用相对路径。

zk中的节点分类：

- 持久类型：将节点创建为持久类型的节点，该数据节点会一直存储在zk的服务器上面，即使创建该节点的客户端与服务器的会话关闭了，该节点依然不会被删除，除非显示的调用delete函数进行删除操作。
- 临时节点：如果将节点创建为临时节点，那么该节点的数据不会一直存储在zk服务器上面，当创建该临时节点的客户端会话因为超市或发生异常关闭时，该节点也在相应的zk服务器上面被删除，也可以主动调用delete删除。
- 有序节点：有序节点并不算是一种单独的节点类型，而是在持久节点和临时节点的基础之上，增加一个节点有序的性质，创建有序节点的时候，zk服务器会自动的使用一个单调递增的数字作为后缀，追加到创建节点的后面，例如一个客户端创建了一个路径为/work、task的有序节点，那么zk服务器将会生成一个序号并且追加到该节点的路径后面，最后该节点的路径为/works/task-1。

节点的内容：一个二进制数组(byte data[])，用来存储节点的数据，ACL访问控制（文件访问权限），子节点数据（因为临时节点不允许有子节点，所以其子节点字段为null），记录自身状态信息的stat。

stat+节点路径可以查看状态信息

czxid:创建节点的事务id。

mzxid:最后一次被更新的事务id

pzxid:子节点最后一次被修改的事务id。

ctime:创建时间

mtime:最后更新时间

version:版本号，表示的是对节点的数据内容，子节点信息或者acl信息修改的次数，可以避免并发问题，使用之前获取的版本进行cas操作更新。

cversion:子节点版本号

aversion:acl版本号

ephemeralOwner:创建节点的sessionid，如果是持久节点，值为0。

dataLength:数据内容长度。

numChildren:子节点个数

### zk的数据同步原理

根据这三个参数的大小对比结果，选择对应的数据同步方式。

- peerLastZxid:Learner服务器（Follower或Observer)最后处理的zxid。
- minCommittedLog:Leader服务器proposal缓存队列committedLog中最小的zxid。
- maxCommittedLog:leader服务器proposal缓存队列committedLog中最大的zxid。

zookeeper中数据同步一共四类，如下：

- DIFF:直接差异化同步：peerlastZxid介于minCommittedLog和maxCommittedLog之间。
- TRUNC+DIFF：先回滚然后在差异化同步：当leader服务器发现某一个learner包含了一条自己没有的事务记录，那么就需要让该learner进行事务回滚到Leader服务器上存在的记录，然后在进行同步，回滚到最接近peerLastZxid的地方。
- TRUNC：仅回滚同步，peerLastZxid大于maxCommittedLog，leader会要求learner回滚到zxid的值为maxCommittedLog对应的事务操作。
- SNAP:全量同步，peerLastZxid小于minCommittedLog。

在初始化阶段，leader服务器会优先初始化以全量同步方式来同步数据

learner先向leader注册，上报peerlastZxid

### zk的watch机制实现原理

监听机制，znode中有很多数据内容，如果数据发生变化，如何去通知客户端，这就是watch机制。



待补充

### zk分布式锁实现原理

- 上来直接创建一个锁节点下的一个接一个的临时顺序节点
- 如果自己不是第一个节点，就对自己上一个节点加监听器
- 只要和三分一个节点释放锁，自己就排到前面去，相当于是一个排队机制
- 而且用临时顺序节点，如果某个客户端创建临时顺序节点之后，自己宕机了，zk感知到哪个客户端宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者是自动取消自己的排队，解决了惊群效应。

惊群效应：唤醒后面的线程是一个一个的唤醒，不会全部唤醒然后去让他们竞争锁，也就是说线程就像排队一样，一个接一个的唤醒。但是这样排队的话也有一个问题，队列排的太长，有的线程可能长时间获取不到锁，效率很低。

zk采用临时节点锁，就是防止发生死锁。

### zk的应用场景

通过对zk中丰富的数据节点进行交叉使用，配合Watcher事件通知机制，可以非常方便的构建一系列分布式应用会涉及到的核心功能：

1. 数据发布/订阅：配置中心
2. 负载均衡，提供服务者列表
3. 命名服务，提供服务名到服务地址的映射
4. 分布式协调通知：watch机制和临时节点，获取各个节点的任务进度，通过修改节点发出通知。
5. 集群管理：是否有机器退出或者加入，选举master。
6. 分布式锁。
7. 分布式队列。

第一类：在约定目录下面创建临时目录节点，监听节点数目是否是要求的数目

第二类：和分布式锁服务中的控制时序场景基本原理一致，入列有编号，在特定的目录下面创建PERSISTENT_SEQUENTIAL节点，创建成功时watch通知等待的队列，队列删除序号最小的节点泳衣消费，此场景下zookeeper的znode用于消息存储，znode存储的数据就是消息队列中的消息内容，SEQUENTIAL序列号就是消息的编号，按序取出即可，由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。

### zk中一个客户端修改了某一个节点的数据，其他客户端能够马上获取到这个最新数据么

是可以读取到的，但是需要额外操作，正常情况下，leader更新数据之后，如果还没有来得及同步follower节点，那么客户端去follower节点读取的数据是历史的数据，所以如果想读取最新的数据，需要我们执行sync操作，同步数据之后，就可以读取到最新的数据。

### 请谈谈zk对事务性的支持

类似于mysql中的事务操作。

zookeeper对事务性的支持主要是依赖下面四个函数：zoo_create_op_init，zoo_delete_cp_init，zoo_set_op_init以及zoo_check_op_init。

每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations，当准备好一个事务中的所有操作后，可以使用zoo_multo来提交所有的操作，由zookeeper服务来保证这一系列操作的原子性，也就是说只要其中有一个操作失败之后，相当于此次提交的任何一个操作都没有对服务端的数据造成影响，zoo_multi的返回值是第一个失败操作的状态信号。也可以进行异步返回。

### 简述zk中的观察者机制

在zk中如果想把一个节点设置为observe节点的话，可以添加下面两个参数

~~~ java
peerType=observe
server.1:localhost:2181:3181:observe
~~~

observer节点和follower节点一样，同样可以处理读请求，还可以接受写请求，只不过会把写请求重新定向类leader节点。

观察者的设计是希望能够动态的扩展zookeeper集群又不会降低它的写性能。

如果扩展的节点是follower节点，则写入操作提交时候需要同步的节点数会增多，导致写入性能下降，而follower又是参与投票的，也会导致投票成本增加。为了解决这两个问题，引入observer节点。

observer是一种新的节点类型，解决扩展问题时的同时，只是获取投票结果，并不参与投票，同时也可以处理读写请求，写请求转发给leader,负责接受leader同步过来的提交数据，observer的节点故障也不会影响集群的可用性。observer同步的数据全部是commit的数据。zk容忍节点故障的个数是一半。超过半数就无法投票。

跨数据中心部署，把节点分散到多个数据中心可能因为网络的延迟会极大的拖慢系统，使用observer的话，更新操作都在一个单独的数据中心来处理，并发送到其他的数据中心，让其他数据中心的节点消费数据。

无法完成消除数据中心之间的网络延迟，因为observer需要把更新请求转发到另一个数据中心的leader,并处理同步消息，网络的速度极慢的话也会有影响，他的优势是为本地读请求提供快速响应。

> 不参与投票，宕机后并不影响可用性



