## Zookeeper

### 什么是zookeeper

ZooKeeper 是一个开放源码的**分布式协调服务**，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

分布式应用程序可以基于 Zookeeper 实现诸如**数据发布 / 订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列**等功能。

Zookeeper从设计模式角度来理解：是一个**基于观察者模式设计的分布式服务管理框架**，**它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应**。

Zookeeper 保证了如下分布式一致性特性：

- 顺序一致性：从同一个客户端发起的事务请求,最终将会严格地按照其发起顺序被应用到ZooKeeper中去。
- 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的,也就是说,要么整个集群所有机器都成功应用了某一个事务,要么都没有应用,一定不会出现集群中部分机器应用了该事务,而另外一部分没有应用的情况。
- 单一视图：无论客户端连接的是哪个 ZooKeeper服务器,其看到的服务端数据模型都是一致的。
- 可靠性：一旦服务端成功地应用了一个事务,并完成对客户端的响应,那么该事务所引起的服务端状态变更将会被一直保留下来,除非有另一个事务又对其进行了变更。
- 实时性（**最终一致性**）：通常人们看到实时性的第一反应是,一旦一个事务被成功应用,那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是ZooKeeper仅仅保证在一定的时间段内,客户端最终一定能够从服务端上读取到最新的数据状态。

> zookeeper满足的是cp原则，但是这里的一致性，指的是弱一致性，而不是强一致性。

客户端的**读请求**可以被集群中的**任意一台机器处理**，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求， 这些请求会先发送给leader节点，同时发给其他 zookeeper 机器并且达成一致后（这里使用额是过半机制），请求才会返回成功。因此，随着 zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。

有序性是 zookeeper 中非常重要的一个特性，**所有的更新都是全局有序的**，每个更新都有一个唯一的时间戳 ，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个 zookeeper 最新的 zxid。

### 谈谈你对ZooKeeper的理解？

Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的**数据一致性**问题。

多个个体之间需要相互通信，有两种方式去实现：

1. 一种是让个体与个体之间进行迭代时交流，比如a和b，b和c以此类推这种方式。
2. 另一种是引入一个外部系统，让这个外部系统作为一个中间人去协调各个外部的独立个体相互通信。

> 总的来说，代表两种处理问题的方式：
>
> - ⼀种是独⽴个体之间互相直接交流来解决，
> - ⼀种是需要第三⽅介⼊来协调解决。

这里可以类比在hadoop中引入yarn外部调度框架机制，解耦行很好。

那么对于多个进程之间的互相交流的解决⽅法也是这样的，由⼀个第三⽅⽆关进程介⼊来协调处理，此时这个第三⽅就是ZooKeeper。所以zookeeper就像一个家长或者调度者，来协调各个实体之间进行通信。

这种⽅法还有⼀个好处，就是在⼀定程度上降低了**个体的复杂性与要求**，以及由此产⽣的额外问题。

对于进程来说，降低了对业务开发⼈员的要求，不需要具备完整的进程间通信相关知识，同时降低了进程本⾝的复杂度，不需要⽀持完整的进程间通信，可能只需⽀持客⼾端即可。

这种⽅式的另⼀个好处是可以被抽象出来做成⼀个独⽴的中间件供⼤家使⽤，ZooKeeper就是这样的。

所以从本质来说，ZooKeeper就是⼀个第三⽅，也称中间⼈，它搭建了⼀个平台，让所有其它进程通过它来进⾏间接的交流，保证整个系统的数据一致性 。

### zookeeper提供什么服务

我们从最常⻅的场景⼊⼿，从宏观上了解下zookeeper是如何使⽤的，以及它应该具备哪些能⼒。

场景⼀：
有两个应⽤程序进程A和B，A先处理数据，处理完后通知B，B再接着处理。我们应该如何利⽤zookeeper来完成这个呢？⼀起来分析⼀下。

⾸先，进程A连接上zookeeper，在上⾯创建⼀个节点来表⽰⾃⼰的存在，假设节点名称就叫foo吧。

然后在节点上设置⼀个数据叫doing，表⽰⾃⼰正在处理数据。过了⼀会处理完后，把节点上的数据更新为done。这样进程A的⼯作就算完了。可是这怎么去影响到进程B呢？我们知道zookeeper完成的是进程间的间接交流，即进程之间是不碰⾯的。因此只能借助于这个树形⾥的节点。

进程B也要连上zookeeper，然后找到foo节点，看好它上⾯的数据是否由doing变成了done，如果是⾃⼰就开始处理数据，如果否那就继续等着。

问题是进程B不能⾃⼰⽼盯着foo节点啊，这样太累了，伤神，况且它还要做其它事情呢。那这个事情应该由谁来做呢？很显然是zookeeper嘛。

于是进程B就对zookeeper说，你给我盯着foo节点，什么时候变成done了通知我⼀声，我就开⼲了。

因此，zookeeper需要具有盯梢能⼒和通知其它进程的能⼒。这在zookeeper中对应⼀个专业术语，叫**Watch**。

Watch的作⽤和⽤法与上⾯描述的⼀样。就是进程B找到foo节点，在上⾯放⼀个Watch就可以了。

这样zookeeper就知道进程B对foo节点⽐较关注，于是zookeeper就盯着foo节点，⼀有⻛吹草动，⻢上通知进程B。

> 注：关于Watch有⾮常多的细节问题，这⾥就不谈了。

需要注意的是，这个Watch是⼀次性的，即只能使⽤⼀次。也就是说，zookeeper通知过进程B之后，Watch就被⽤掉了，以后就不会再通知了。

如果进程B还需要被通知怎么办？很简单，那就在foo节点上再放⼀个新的Watch即可。如此这般下去，就可以保证⼀直被通知了。

我想这个Watch之所以被设计成⼀次性的，就是zookeeper不想让⾃⼰太累。睁着⼀双⼤眼，盯的东西太多太久的话，确实很累。

另外，zookeeper在通知进程B的时候，是可以把foo节点存放的数据⼀并发送过去的。

细⼼的朋友可能已经发现，zookeeper可以主动向进程B发通知或推数据，说明zookeeper和进程B之间的连接需要被⼀直保持。

因为进程B的位置⽐较随意，本来就是业务进程嘛。⼀旦连接断开，就像断了线的⻛筝，zookeeper再也⽆法找到进程B了。

不过zookeeper的位置是固定的，⼀旦连接断掉后，进程B可以再次向zookeeper发起连接请求，如果断开的时间⾜够短的话，进程B应该还可以在zookeeper上找回⾃⼰曾经拥有的⼀切。

这就涉及到了会话，因此zookeeper还要有⼀定的会话延续能⼒，⽅便在断开时间不⻓的时候找回原来的会话。

因此zookeeper应该有，**监视节点、通知进程、保持⻓连接，会话延续**等这样的能⼒。

场景⼆：

有时为了⾼可⽤或⾼性能，通常会把⼀个应⽤程序运⾏多份。假如运⾏了四份，那就是四个进程，分别是A、B、C、D。

当⼀个调⽤过来时，发现A、B、C、D都可以调，那就根据配置的负载均衡策略选出⼀个调⽤即可。

假设D进程所在的机器不幸掉电了，其实就是D挂了，那么此时再来⼀个调⽤的话，会发现只有A、B、C可以调，D⾃动就不存在了。

这其实就是Dubbo功能的⼀部分，那该如何基于zookeeper实现呢？照例⼀起分析下吧。

由于zookeeper是基于树形的数据结构，所以还是要拿节点说事。当进程A启动时，需要连接上zookeeper，然后创建⼀个节点来代表⾃⼰。

节点名称和节点上存放的数据可以根据实际情况来定，⾄少要包括该进程运⾏的IP和端⼝信息。进程B、C、D也做同样的事情。

如果让进程A、B、C、D的节点都位于同⼀个⽗节点下⾯，这样当⼀个调⽤过来后，只要找到这个⽗节点，读出它的所有⼦节点，就得到了所有可调⽤的进程信息。

如果某⼀时刻，进程D挂掉了，那么⽗节点下⾯进程D对应的那个节点应该会⾃动被zookeeper删除。这在
zookeeper⾥有个专业术语，叫**临时节点（Ephemeral Node）**。那么与之对应的⾃然就是**永久节点**了。

其实⼯作过程是这样的，业务进程启动后与zookeeper建⽴连接，然后在zookeeper⾥创建临时节点并写⼊⾃⼰的相关信息。接着通过周期性的⼼跳和zookeeper保持住连接。

⼀旦业务进程挂掉，zookeeper将接受不到⼼跳了，那么在超过⼀定的时间后，zookeeper将会删除与之对应的临时节点，表⽰这个业务进程不再可⽤了。

Dubbo的做法是将接⼝名称和IP端⼝信息和我们设置的信息整合成⼀个类似URL的字符串，然后以这个字符串作为名称来创建临时节点。

**临时节点不允许有孩⼦节点，只有永久节点才可以。**

ZooKeeper提供的服务包括：

- 分布式消息同步和协调机制
- 服务器节点动态上下线
- 统一配置管理
- 负载均衡
- 集群管理

### zookeeper的数据结构

数据结构，相比都非常的了解，链表，数据，树等等很多，那么zookeeper选择树作为自己的数据结构。

ZooKeeper提供基于类似于Linux文件系统的目录节点树方式的数据存储，即**分层命名空间**。

Zookeeper 并不是用来专门存储数据的，它的作用主要是用来**维护和监控你存储的数据的状态变化**，通过监控这些数据状态的变化，从而可以达到基于数据的集群管理，ZooKeeper节点的数据上限是1MB

我们可以认为**Zookeeper=文件系统+通知机制**，对于ZooKeeper的数据结构，每个子目录项如NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode的标识为 /NameService/Server1

简单来说，这每一个节点都可以进行增删改查操作，那么zookeeper再次基础上，增加了新的信息：

- znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据。
- znode 可以是**临时节点**，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，Zookeeper 的客户端和服务器通信采用**长连接**方式，每个客户端和服务器通过**心跳**来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了
- znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2
- znode 可以被**监控**，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是 Zookeeper 的核心特性，Zookeeper 的很多功能都是基于这个特性实现的，后面在典型的应用场景中会有实例介绍

### zookeeper的设计目标

ZooKeeper致力于提供一个高性能、高可用,且具有严格的顺序访问控制能力(**主要是写操作的严格顺序性**)的分布式协调服务。**高性能使得 ZooKeeper能够应用于那些对系统吞吐有明确要求的大型分布式系统中,高可用使得分布式的单点问题得到了很好的解决,而严格的顺序访问控制使得客户端能够基于 ZooKeeper实现一些复杂的同步原语**。下面我们来具体看一下 ZooKeeper的四个设计目标。

- 简单的数据模型：zookeeper的数据都是一个个的znode节点类型，所有数据组织成一个类似linux的文件系统，zookeeper将全量的数据全部存储在**内存**当中，一次来提高服务的吞吐量，减少延迟。
- 可构建集群：组成 ZooKeeper集群的每台机器都会在内存中维护当前的服务器状态,并且每台机器之间都互相保持着通信。**只要集群中存在超过一半的机器能够正常工作,那么整个集群就能够正常对外服务**。
- 顺序访问：对于来自客户端的每个更新请求, ZooKeeper都会分配一个全局唯一的递增编号,这个编号反映了所有事务操作的先后顺序,应用程序可以使用 ZooKeeper的这个特性来实现更高层次的同步原语。
- 高性能：zookeeper将数据存储在**内存**当中，所以非常适合以**读为场景**的应用中。

### zookeeper的基本概念

#### 集群

通常在分布式系统中,构成一个集群的每一台机器都有自己的角色,最典型的集群模式就是 Master/Slave模式(主备模式)。

在这种模式中,我们把能够处理所有写操作的机器称为 Master机器,把所有通过异步复制方式获取最新数据,并提供读服务的机器称为Slave机器。

而在 ZooKeeper中,它没有沿用传统的 Master/Save概念,而是引入了 Leader、 Follower和 Observer三种角色。 

ZooKeeper集群中的所有机器通过一个Leader选举过程来选定一台被称为“ **Leader**”的机器, Leader服务器为客户端提供**读和写服务**。除 Leader外,其他机器包括 Follower和 Observer。 Follower和 Observer都能够提供**读服务**,唯一的区别在于, Observer机器不参与 Leader选举过程,也不参与写操作的“过半写成功”策略,因此 Observer可以在不影响写性能的情况下提升集群的读性能。

#### 会话

Session是指客户端会话,在讲解会话之前,我们首先来了解一下客户端连接。在ZooKeeper中,一个客户端连接是指客户端和服务器之间的一个TCP长连接。 

ZooKeeper对外的服务端口默认是2181,客户端启动的时候,首先会与服务器建立一个TCP连接,从第一次连接建立开始,客户端会话的生命周期也开始了,通过这个连接,客户端能够通过心跳检测与服务器保持有效的会话,也能够向 ZooKeeper服务器发送请求并接受响应,同时还能够通过该连接接收来自服务器的 Watch事件通知。

 Session的 session Timeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时,只要在 session Timeout规定的时间内能够重新连接上集群中任意一台服务器,那么之前创建的会话仍然有效。

#### 数据节点( Znode)

在谈到分布式的时候,我们通常说的“节点”是指组成集群的每一台机器。然而,在ZooKeeper中,“节点”分为两类,

- 第一类同样是指构成集群的机器,我们称之为机器节点;
- 第二类则是指数据模型中的数据单元,我们称之为数据节点— ZNode。

ZooKeeper将所有数据存储在內存中,数据模型是一棵树( ZNode Tree),由斜杠(1)进行分割的路径,就是一个 Znode,例如/ o/pathI。每个 ZNode上都会保存自己的数据内容,同时还会保存一系列属性信息。在 ZooKeeper中, ZNode可以分为**持久节点和临时节点**两类。

- 所谓持久节点是一旦这个 ZNode被创建了,除非主动进行 ZNode的移除操作,否则这个 ZNode将一直保存在ZooKeeper上。
- 而临时节点就不一样了,它的生**命周期和客户端会话**绑定,一旦客户端会话失效,那么这个客户端创建的所有临时节点都会被移除。
  - 另外, ZooKeeper还允许用户为每个节点添加一个特殊的属性: SEQUENTIAL。一旦节点被标记上这个属性,那么在这个节点被创建的时候, ZooKeeper会自动在其节点名后面追加上一个整型数字,这个整型数字是一个由父节点维护的自增数字。

#### 版本

在前面我们已经提到, ZooKeeper的每个 ZNode上都会存储数据,对应于每个 ZNode,ZooKeeper都会为其维护一个叫作Stat的数据结构,Stat中记录了这个 ZNode的三个数据版本,分别是 version(当前 ZNode的版本)、 cversion(当前 ZNode子节点的版本)和 aversion(当前 ZNode的ACL版本)。

#### Watcher

Watcher(事件监听器),是 ZooKeeper中的一个很重要的特性。 ZooKeeper允许用户在指定节点上注册一些 Watcher,并且在一些特定事件触发的时候, ZooKeeper服务端会将事件通知到感兴趣的客户端上去。

> 该机制是 ZooKeeper实现分布式协调服务的重要特性。

#### ACL

ZooKeeper采用ACL( Access Control lists)策略来进行**权限控制**,类似于UNX文件

系统的权限控制。 ZooKeeper定义了如下5种权限。

- CREATE:创建子节点的权限。
- READ:获取节点数据和子节点列表的权限。
- WRITE:更新节点数据的权限。
- DELETE:删除子节点的权限。
- ADMIIN:设置节点ACL的权限。

其中尤其需要注意的是, CREATE和 DELETE这两种权限都是针对子节点的权限控制。

### zookeeper的特点

1. Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。
2. 集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器。
3. 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。
4. 更新请求**顺序执行**，来自同一个Client的更新请求按其发送顺序依次执行。
5. 数据更新原子性，一次数据更新要么成功，要么失败。
6. 实时性，在一定时间范围内，Client能读到最新数据。

### 数据结构

ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过其路径唯一标识。

![1633953596473](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/195957-811207.png)

**节点数据信息**

![1633954133012](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/200855-616482.png)

- czxid：创建节点的事务 zxid，每次修改 ZooKeeper 状态都会产生一个 ZooKeeper 事务 ID。事务 ID 是 ZooKeeper 中所有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。
- ctime：znode 被创建的毫秒数（从 1970 年开始）
- mzxid：znode 最后更新的事务 zxid
- mtime：znode 最后修改的毫秒数（从 1970 年开始）
- pZxid：znode 最后更新的子节点 zxid
- cversion：znode 子节点变化号，znode 子节点修改次数
- dataversion：znode 数据变化号
- aclVersion：znode 访问控制列表的变化号
- ephemeralOwner：如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。
- dataLength：znode 的数据长度
- numChildren：znode 子节点数量

### zookeeper提供了什么

- 文件系统
- 通知机制

### zookeeper文件系统

Zookeeper 提供一个多层级的节点命名空间（节点称为 znode ）。

与文件系统不同的是，这些节点都可以设置**关联的数据**，而文件系统中只有文件节点可以存放数据而目录节点不行。

Zookeeper 为了保证**高吞吐和低延迟**，在内存中维护了这个树状的目录结构， 这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为 1M。

### 分布式集群中为什么会有 Master？

在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举，leader负责向集群中其他的服务器同步结果。

### ZAB协议

> 也可以认为是zookeeper的工作原理

ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种**支持崩溃恢复的原子广播协议**。

Zookeeper 的核⼼是原⼦⼴播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。

ZAB 协议包括两种基本的模式：**崩溃恢复和消息广播**。

当整个 zookeeper 集群刚刚启动或者 Leader 服务器宕机、重启或者网络故障导致不存在过半的服务器与 Leader 服务器保持正常通信时，所有进程（服务器）进入崩溃恢复模式，首先选举产生新的 Leader 服务器，然后集群中Follower服务器开始与新的 Leader 服务器进行数据同步。

当集群中超过半数机器与该 Leader服务器完成数据同步之后，退出恢复模式进入消息广播模式， Leader 服务器开始接收客户端的事务请求生成事物提案来进行事务请求处理。

### 四种类型的数据节点 Znode

**PERSISTENT-持久节点**

除非手动删除，否则节点一直存在于 Zookeeper 上。

**EPHEMERAL - 临时节点**

临时节点的生命周期与**客户端会话绑定**，一旦客户端会话失效（客户端与zookeeper 连接断开不一定会话失效），那么这个客户端创建的所有临时节点 都会被移除。

**PERSISTENT_SEQUENTIAL-持久顺序节点**

基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的**自增整型数字**。

**EPHEMERAL_SEQUENTIAL - 临时顺序节点**
基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的**自增整型数字**。

![1633953422738](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/195703-944957.png)

### 请简述zookeeper的选举机制（第一次启动）

假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么：

![1633952959857](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/194920-61261.png)

1. 服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；
2. 服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的myid比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING
3. 服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；
4. 服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；
5. 服务器5启动，同4一样当小弟。

> 注意，如果按照5，4，3，2，1的顺序启动，那么5将成为Leader，因为在满足半数条件后，ZooKeeper集群启动，5的Id最大，被选举为Leader。
>
> 服务器5和服务器3,3相互选举会投ID最大的那一台服务器，所以服务器5会选举为leader。
>
> - SID：服务器ID。用来唯一标识一台ZooKeeper集群中的机器，每台机器不能重复，和myid一致。
> - ZXID：事务ID。ZXID是一个事务ID，用来标识一次服务器状态的变更。在某一时刻，集群中的每台机器的ZXID值不一定完全一致，这和ZooKeeper服务器对于客户端“更新请求”的处理逻辑有关。
> - Epoch：每个Leader任期的代号。没有Leader时同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加

### 请简述zookeeper的选举机制（非第一次启动）

![1633953147004](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/195228-254184.png)

- 当ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入Leader选举：
  - 服务器初始化启动。
  - 服务器运行期间无法和Leader保持连接。
- 而当一台机器进入Leader选举流程时，当前集群也可能会处于以下两种状态：
  - 集群中本来就已经存在一个Leader。对于第一种已经存在Leader的情况，机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器来说，仅仅需要和Leader机器建立连接，并进行状态同步即可。
  - 集群中确实不存在Leader。

![1633953321081](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/195522-250498.png)

### ZooKeeper的监听原理是什么？

在应用程序中，main()方法首先会创建zkClient，创建zkClient的同时就会产生两个进程，即**Listener进程（监听进程）和connect进程（网络连接/传输进程）**，当zkClient调用getChildren()等方法注册监视器时，connect进程向ZooKeeper注册监听器，注册后的监听器位于ZooKeeper的监听器列表中，监听器列表中记录了zkClient的IP，端口号以及要监控的路径，一旦目标文件发生变化，ZooKeeper就会把这条消息发送给对应的zkClient的Listener()进程，Listener进程接收到后，就会执行process()方法，在process()方法中针对发生的事件进行处理.

**原理图**

![1633935341118](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/145543-29912.png)

- connect进程负责去zookeeper服务器上面注册监视器。
- Listener进程负责通知客户端进程。

### Watcher机制原理

ZooKeeper提供了分布式数据的**发布/订阅功能**，一个典型的发布/订阅模型系统定义了一种一对多的订阅关系，能够让多个订阅者同时监听某一个主题对象，当这个主题对象自身状态变化时，会通知所有订阅者，使它们能够做出相应的处理。在ZooKeeper中，引入了**Watcher机制**来**实现这种分布式的通知功能。**

ZooKeeper 允许客户端向服务端注册一个Watcher监听，当服务端的一些指定事件触发了这个Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。

![1633936143252](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202110/11/150903-508416.png)

ZooKeeper的Watcher机制主要包括**客户端线程**、**客户端WatchManager**和**ZooKeeper服务器**三部分。

在具体工作流程上，客户端在向ZooKeeper 服务器注册Watcher的同时，会将Watcher 对象存储在客户端的WatchManager中。当ZooKeeper服务器端触发Watcher事件后，会向客户端发送通知，客户端线程从WatchManager中取出对应的Watcher对象来执行process方法的回调逻辑。

**Watcher特性**

1. 一次性

无论是服务端还是客户端，一旦一个Watcher被触发，ZooKeeper都会将其从相应的存储中移除。因此，开发人员在Watcher的使用上要记住的一点是**需要反复注册**。这样的设计**有效地减轻了服务端的压力**。试想，如果注册一个Watcher之后一直有效，那么，针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能的影响都非常大。

2. 客户端串行执行

客户端 Watcher回调的过程是一个**串行同步的过程**，这为我们保证了顺序，同时，需要开发人员注意的一点是，千万不要因为一个Watcher的处理逻辑影响了整个客户端的Watcher回调。

3. 轻量

WatchedEvent是ZooKeeper整个Watcher通知机制的最小通知单元，这个数据结构中只包含三部分内容：**通知状态**、**事件类型**和**节点路径**。也就是说，**Watcher通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容**。

例如针对NodeDataChanged事件，ZooKeeper的Watcher只会通知客户端指定数据节点的数据内容发生了变更，而对于原始数据以及变更后的新数据都无法从这个事件中直接获取到，而是需要客户端主动重新去获取数据一—这也是ZooKeeper的Watcher机制的一个非常重要的特性。

另外，**客户端向服务端注册 Watcher的时候，并不会把客户端真实的Watcher对象传递到服务端**，仅仅只是在客户端**请求中使用boolean类型属性进行了标记**，同时**服务端也仅仅只是保存了当前连接的ServerCnxn对象**。

如此轻量的Watcher机制设计，在网络开销和服务端内存开销上都是非常廉价的。

### Zookeeper分布式锁（⽂件系统、通知机制）

有了zookeeper的⼀致性⽂件系统，锁的问题变得容易。锁服务可以分为两类:

- ⼀个是保持独占
- 另⼀个是控制时序。

对于第⼀类，我们将zookeeper上的⼀个znode看作是⼀把锁，通过createznode的⽅式来实现。所有客⼾端都去创建/distribute_lock 节点，最终成功创建的那个客⼾端也即拥有了这把锁。⽤完删除掉⾃⼰创建的distribute_lock 节点就释放出锁。

对于第⼆类， /distribute_lock 已经预先存在，所有客⼾端在它下⾯创建临时顺序编号⽬录节点，和选master⼀样，编号最⼩的获得锁，⽤完删除，依次⽅便。

### 获取分布式锁的流程

![1639211297804](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/11/162819-607429.png)

在获取分布式锁的时候在locker节点下创建临时顺序节点，释放锁的时候删除该临时节点。客⼾端调⽤createNode⽅法在locker下创建临时顺序节点，然后调⽤getChildren(“locker”)来获取locker下⾯的所有⼦节点，注意此时不⽤设置任何Watcher。

客⼾端获取到所有的⼦节点path之后，如果发现⾃⼰创建的节点在所有创建的⼦节点序号最⼩，那么就认为该客⼾端获取到了锁。如果发现⾃⼰创建的节点并⾮locker所有⼦节点中最⼩的，说明⾃⼰还没有获取到锁，此时客⼾端需要找到⽐⾃⼰⼩的那个节点，然后对其调⽤exist()⽅法，同时对其注册事件监听器。

之后，让这个被关注的节点删除，则客⼾端的Watcher会收到相应通知，此时再次判断⾃⼰创建的节点是否是locker⼦节点中序号最⼩的，如果是则获取到了锁，如果不是则重复以上步骤继续获取到⽐⾃⼰⼩的⼀个节点并注册监听。当前这个过程中还需要许多的逻辑判断。

![1639211378517](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/11/162946-129287.png)

代码的实现主要是基于**互斥锁**，获取分布式锁的重点逻辑在于BaseDistributedLock，实现了基于Zookeeper实现分布式锁的细节。

### zookeeper中的数据复制

Zookeeper作为⼀个集群提供⼀致的数据服务，⾃然，它要在所有机器间做数据复制。数据复制的好处：

- 容错：⼀个节点出错，不致于让整个系统停⽌⼯作，别的节点可以接管它的⼯作；
- 提⾼系统的扩展能⼒ ：把负载分布到多个节点上，或者增加节点来提⾼系统的负载能⼒；
- 提⾼性能：让客⼾端本地访问就近的节点，提⾼⽤⼾访问速度。
- 从客⼾端读写访问的透明度来看，数据复制集群系统分下⾯两种：
  - 写主(WriteMaster) ：**对数据的修改提交给指定的节点**。读⽆此限制，可以读取任何⼀个节点。这种情况下客⼾端需要对读与写进⾏区别，俗称读写分离；
  - 写任意(Write Any)：对数据的修改可提交给任意的节点，跟读⼀样。这种情况下，客⼾端对集群节点的⻆⾊与变化透明。
- 对zookeeper来说，它采⽤的⽅式是**写任意**。通过增加机器，它的读吞吐能⼒和响应能⼒扩展性⾮常好，⽽写，随着机器的增多吞吐能⼒肯定下降（这也是它建⽴observer的原因），⽽响应能⼒则取决于具体实现⽅式，是延迟复制保持最终⼀致性，还是⽴即复制快速响应。

> **zookeeper中任意节点收到写请求，如果是follower节点，则会把写请求转发给leader，如果是leader节点就直接进行下一步**。

### zookeeper是如何保证事务的顺序⼀致性的？

zookeeper采⽤了递增的事务Id来标识，所有的proposal（提议）都在被提出的时候加上了zxid，zxid实际上是⼀个64位的数字，⾼32位是epoch（时期; 纪元; 世; 新时代）⽤来标识leader是否发⽣改变，如果有新的leader产⽣出来，epoch会⾃增，低32位⽤来**递增计数**。

当新产⽣proposal的时候，会依据数据库的两阶段过程，⾸先会向其他的server发出事务执⾏请求，如果超过半数的机器都能执⾏并且能够成功，那么就会开始执⾏。

### Zookeeper 下 Server⼯作状态

每个Server在⼯作过程中有三种状态：

- LOOKING：当前Server不知道leader是谁，正在搜寻
- LEADING：当前Server即为选举出来的leader
- FOLLOWING：leader已经选举出来，当前Server与之同步

### zookeeper是如何选取主leader的？

当leader崩溃或者leader失去⼤多数的follower，这时zk进⼊恢复模式，恢复模式需要重新选举出⼀个新的leader，让所有的Server都恢复到⼀个正确的状态。Zk的选举算法有两种：

- ⼀种是基于basic paxos实现的，

- 另外⼀种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。

##### Zookeeper选主流程(basic paxos)

1. 选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进⾏统计，并选出推荐的Server；
2. 选举线程⾸先向所有Server发起⼀次询问(包括⾃⼰)；
3. 选举线程收到回复后，验证是否是⾃⼰发起的询问(验证zxid是否⼀致)，然后获取对⽅的id(myid)，并存储到当前询问对象列表中，最后获取对⽅提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；
4. 收到所有Server回复以后，就计算出zxid最⼤的那个Server，并将这个Server相关信息设置成下⼀次要投票的Server；
5. 线程将当前zxid最⼤的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置⾃⼰的状态，否则，继续这个过程，直到leader被选举出来。通过流程分析我们可以得出：要使Leader获得多数Server的⽀持，则Server总数必须是奇数2n+1，且存活的Server的数⽬不得少于n+1. 每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘
   快照中恢复数据和会话信息，zk会记录事务⽇志并定期进⾏快照，⽅便在恢复时进⾏状态恢复。

![1639212138864](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/11/164220-781280.png)

##### Zookeeper选主流程(basic paxos)

fast paxos流程是在选举过程中，某Server⾸先向所有Server提议⾃⼰要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对⽅的提议，然后向对⽅发送接受提议完成的消息，重复这个流程，最后⼀定能选举出Leader。

![1639212235077](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/11/164356-819589.png)

### Zookeeper同步流程

选完Leader以后，zk就进⼊状态同步过程。

- Leader等待server连接；
- Follower连接leader，将最⼤的zxid发送给leader；
- Leader根据follower的zxid确定同步点；
- 完成同步后通知follower 已经成为uptodate状态；
- Follower收到uptodate消息后，⼜可以重新接受client的请求进⾏服务了。

![1639212306582](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/11/164507-319367.png)

### zk节点宕机如何处理？

Zookeeper本⾝也是集群，推荐配置不少于3个服务器。Zookeeper⾃⾝也要保证当⼀个节点宕机时，其他节点会继续提供服务。

如果是⼀个Follower宕机，还有2台服务器提供访问，因为Zookeeper上的数据是有多个副本的，数据并不会丢失；

如果是⼀个Leader宕机，Zookeeper会选举出新的Leader。

ZK集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在ZK节点挂得太多，只剩⼀半或不到⼀半节点能⼯作，集群才失效。所以：

- 3个节点的cluster可以挂掉1个节点(leader可以得到2票>1.5)
- 2个节点的cluster就不能挂掉任何1个节点了(leader可以得到1票<=1)

### zookeeper watch机制

Watch机制官⽅声明：⼀个Watch事件是⼀个⼀次性的触发器，当被设置了Watch的数据发⽣了改变的时候，则服务器将这个改变发送给设置了Watch的客⼾端，以便通知它们。

Zookeeper机制的特点：

1. ⼀次性触发数据发⽣改变时，⼀个watcher event会被发送到client，但是client只会收到⼀次这样的信息。
2. watcher event异步发送watcher的通知事件从server发送到client是异步的，这就存在⼀个问题，不同的客⼾端和服务器之间通过socket进⾏通信，由于⽹络延迟或其他因素导致客⼾端在不通的时刻监听到事件，由于Zookeeper本⾝提供了ordering guarantee，即客⼾端监听事件后，才会感知它所监视znode发⽣了变化。所以我们使⽤Zookeeper不能期望能够监控到节点每次的变化。Zookeeper只能保证最终的⼀致性，⽽⽆法保证强⼀致性。
3. 数据监视Zookeeper有数据监视和⼦数据监视getdata() and exists()设置数据监视，getchildren()设置了⼦节点监视。
4. 注册watcher getData、exists、getChildren
5. 触发watcher create、delete、setData
6. setData()会触发znode上设置的data watch（如果set成功的话）。⼀个成功的create() 操作会触发被创建的znode上的数据watch，以及其⽗节点上的child watch。⽽⼀个成功的delete()操作将会同时触发⼀个znode的data watch和childwatch（因为这样就没有⼦节点了），同时也会触发其⽗节点的child watch。
7. 当⼀个客⼾端连接到⼀个新的服务器上时，watch将会被以任意会话事件触发。当与⼀个服务器失去连接的时候，是⽆法接收到watch的。⽽当client重新连接时，如果需要的话，所有先前注册过的watch，都会被重新注册。通常这是完全透明的。只有在⼀个特殊情况下，watch可能会丢失：对于⼀个未创建的znode的exist watch，如果在客⼾端断开连接期间被创建了，并且随后在客⼾端连接上之前⼜删除了，这种情况下，这个watch事件可能会被丢失。
8. Watch是轻量级的，其实就是本地JVM的Callback，服务器端只是存了是否有设置了Watcher的布尔类型



