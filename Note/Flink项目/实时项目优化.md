## 实时项目优化

所有的优化，优先级最高的当然是资源的配置，资源配置调优永远放在第一位。

在Flink程序中，资源配置就是在Source,transform,sink并行度的设置，一个并行度需要一个slot,而一个slot内存的数量往往在TaskManager刚开始启动的时候，已经确定。当然也可以进行修改，比如在per-job模式中，任务来了才启动我们的taskManager，可以重新配置，在Session中不可以，因为任务来的时候，TaskManager已经启动好了。

## 消费资源配置调优

Flink 性能调优的第一步，就是为任务分配合适的资源，在一定范围内，增加资源的分配与性能的提升是成正比的，实现了最优的资源配置后，在此基础上再考虑进行后面论述的性能调优策略。

提交方式主要是 yarn-per-job，资源的分配在使用脚本提交 Flink 任务时进行指定。

标准的 Flink 任务提交脚本（Generic CLI 模式），从 1.11 开始，增加了通用客户端模式，参数使用-D <property=value>指定

> 如果有时候资源已经够了，但是人为添加超过需要的资源，这样不但不会提升性能，还有可能降低性能，比如map join中大表join小表的时候，小表默认使用的最大内存为25m，如果人为增大这个值，就可能降低性能。
>
> 1. 每一个map任务都会下载一份大表的数据，有网络io，浪费性能。
> 2. 申请资源非常的耗时，在使用yarn提交任务的时候，有一个资源的申请过程，如果分配资源太多，那么申请时间就很长。

~~~ java
bin/flink run \
-t yarn-per-job \
-d \
-p 5 \ 指定并行度
-Dyarn.application.queue=test \ 指定 yarn 队列
-Djobmanager.memory.process.size=1024mb \ 指定 JM 的总进程大小
-Dtaskmanager.memory.process.size=1024mb \ 指定每个 TM 的总进程大小
-Dtaskmanager.numberOfTaskSlots=2 \ 指定每个 TM 的 slot 数
-c com.atguigu.app.dwd.LogBaseApp \
/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar
~~~

如果是yarn-session或者standalone模式，需要在配置文件中配置资源。

[参数列表](https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html)

### 内存设置

生产资源配置：

~~~ java
bin/flink run \
-t yarn-per-job \
-d \
-p 5 \ 指定并行度
-Dyarn.application.queue=test \ 指定 yarn 队列
-Djobmanager.memory.process.size=2048mb \ JM2~4G 足够
-Dtaskmanager.memory.process.size=6144mb \ 单个 TM2~8G 足够
-Dtaskmanager.numberOfTaskSlots=2 \ 与容器核数 1core：1slot 或 1core：2slot
-c com.atguigu.app.dwd.LogBaseApp \与容器核数 1core：1slot 或 1core：2slot
/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar
~~~

Flink 是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS/TPS 来描述数据情况。

对于JobManager通常2-4G已经足够。

单个TaskManager，配置2-8G左右，对于具体而言，需要根据我们的业务去测，也就是数据量达到高峰期时候的配置内存量。

那每一个TaskManager里面配置多少个slot呢？

yarn中一个container中默认资源最大是8G，那么我们通常配置与容器的核数有关，通常是`1core：1slot 或 1core：2slot`关系，也就是与容器的核数1：1或者1：2关系。

启动一个TaskManager需要一个Container，而Container最大是8G，所以TaskManager配置内存不能超过容器的内存，通常配置2-8G.如果超过8g,那么需要扩大Container的内存。

container的核心数也可以配置或者指定，我们配置container的核心数之后，每一个TaskManager的slot数量可以参考container的核心数量。如果资源够的情况下，配置1：1，不够情况下配置1：2.配置1：2的话那么一个cpu上需要运行两个任务，运行会慢一点。

> TaskManager和Slot的配置都和yarn 的container容量有关。

在生产中，我们一般不用内存及的状态后端，所以jobManager的内存使用量一般不会太大。

### 并行度设置

#### 最优并行度计算

开发完成后，先进行**压测**。任务并行度给 10 以下，测试单个并行度的处理上限。然后**总 QPS/单并行度的处理能力 = 并行度**。

不能只从 QPS 去得出并行度，因为有些字段少、逻辑简单的任务，单并行度一秒处理几万条数据。而有些数据字段多，处理逻辑复杂，单并行度一秒只能处理 1000 条数据。

最好根据高峰期的 QPS 压测，并行度*1.2 倍，富余一些资源。比如高峰期测出来是50m/s，10个并行度，那么我就一直使用50m/s，10个并行度，跑几个小时做压测，都没有问题，那么说明没问题。*`1.2`防止估算有误差或者数据量增加。

> 如何做压测，积压kafak的数据，然后对Flink程序做压测。

#### Source 端并 行 度的配置

数据源端是 Kafka，Source 的并行度设置为 Kafka 对应 Topic 的分区数。这种情况最好。

如果已经等于 Kafka 的分区数，消费速度仍跟不上数据生产速度，考虑下 Kafka 要扩大分区，同时调大并行度等于分区数。

Flink 的一个并行度可以处理一至多个分区的数据，如果并行度多于 Kafka 的分区数，那么就会造成有的并行度空闲，浪费资源。

> 但是这种方式一定需要在最后尝试，我们调优首先做的就是增加资源。kafka分区数量增加是一个不可逆操作，增加了分区，就不能减少。

#### Transform 端并行度的配置

**Keyby 之前的算子**

一般不会做太重的操作，都是比如 map、filter、flatmap 等处理较快的算子，并行度可以和 source 保持一致。

**Keyby 之后的算子**

如果并发较大，建议设置并行度为 2 的整数次幂，例如：128、256、512；

小并发任务的并行度不一定需要设置成 2 的整数次幂；

大并发任务如果没有 KeyBy，并行度也无需设置为 2 的整数次幂；

> 如果业务很复杂，就提高并行度。

#### Sink 端并 行 度的配置

Sink 端是数据流向下游的地方，可以根据 Sink 端的数据量及下游的服务抗压能力进行评估。

如果 Sink 端是 Kafka，可以设为 Kafka 对应 Topic 的分区数。

Sink 端的数据量小，比较常见的就是监控告警的场景，并行度可以设置的小一些。

Source 端的数据量是最小的，拿到 Source 端流过来的数据后做了细粒度的拆分，数据量不断的增加，到 Sink 端的数据量就非常大。那么在 Sink 到下游的存储中间件的时候就需要提高并行度。

另外 Sink 端要与下游的服务进行交互，并行度还得根据下游的服务抗压能力来设置，

如果在 Flink Sink 这端的数据量过大的话，且 Sink 处并行度也设置的很大，但下游的服务完全撑不住这么大的并发写入，可能会造成下游服务直接被写挂，所以最终还是要在 Sink处的并行度做一定的权衡。

sink端和外部系统交互也有延迟，所以也可以使用批量提交的方式。所以对于sink，一个是降低并行度，一个是批量写入。

> 并行度，是在高峰期做压测得到的。

### RocksDB 大状态调优

状态后端的优化，在生产环境中存储的是大状态，存储在文件系统中。是一个kv数据库，性能非常高。状态，本地存储在TaaskManager内存，远程存储在文件系统。

如果使用RocksDB,开启检查点，状态就存储在远程，没有开启就存储在本地。

![1638661920079](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/075201-364852.png)

RocksDB 是基于 LSM Tree 实现的（类似 HBase），写数据都是先缓存到内存中，所以 RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record/s，性能瓶颈主要在于 RocksDB 对磁盘的读请求，所以当处理性能不够时，仅需要横向扩展并行度即可提高整个 Job 的吞吐量。以下几个调优参数：

**设置本地 RocksDB 多目录**

在 flink-conf.yaml 中配置：

~~~ java
state.backend.rocksdb.localdir:
/data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb
~~~

注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。当设置多个 RocksDB 本地磁盘目录时，Flink 会随机选择要使用的目录，所以就可能存在三个并行度共用同一目录的情况。如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。

当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。

如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘 IO 性能消耗确实比较大。

![1638662113322](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/075513-621808.png)

如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj 磁盘。可以看到sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下，从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。

![1638662158224](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/075559-649336.png)

**state.backend.incremental：开启增量检查点，默认 false，改为 true。增量检查点只有Rocks DB有，其他两个都没有。**

state.backend.rocksdb.predefined-options：SPINNING_DISK_OPTIMIZED_HIGH_MEM 设置为机械硬盘+内存模式，有条件上SSD，指定为 FLASH_SSD_OPTIMIZED

**读缓存**

state.backend.rocksdb.block.cache-size: 整 个 RocksDB 共 享 一 个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。

**写缓存**

state.backend.rocksdb.writebuffer.size: RocksDB 中，每个 State 使用一个Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M

state.backend.rocksdb.thread.num: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。

state.backend.rocksdb.writebuffer.count: 每 个 Column Family 对 应 的writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5左右。

state.backend.rocksdb.writebuffer.number-to-merge: 将数据从 writebuffer中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成 3。

state.backend.local-recovery: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据。

### CheckPoint设置

一般我们的 Checkpoint 时间间隔可以设置为分钟级别，例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5~10 分钟一次Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次 Checkpoint 之间至少暂停 4 或 8 分钟。

checkpoint是防止任务挂掉能从失败中恢复，一般情况下任务不可能很快挂掉，所以建议调大时间。

如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。

RocksDB 相关参数在 1.3 中已说明，可以在 flink-conf.yaml 指定，也可以在 Job 的代码中调用 API 单独指定，这里不再列出。

~~~ java
// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint
RocksDBStateBackend rocksDBStateBackend = new
RocksDBStateBackend("hdfs://hadoop102:8020/flink/checkpoints", true);
env.setStateBackend(rocksDBStateBackend);
// 开启 Checkpoint，间隔为 3 分钟
env.enableCheckpointing(TimeUnit.MINUTES.toMillis(3));
// 配置 Checkpoint
CheckpointConfig checkpointConf = env.getCheckpointConfig();
checkpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)
// 最小间隔 4 分钟
checkpointConf.setMinPauseBetweenCheckpoints(TimeUnit.MINUTES.toMillis(4))
// 超时时间 10 分钟
checkpointConf.setCheckpointTimeout(TimeUnit.MINUTES.toMillis(10));
// 保存 checkpoint
//这个参数是说在任务最后如果我们主动cancel任务后，会删除最后一次的checkpoint，设置为retain就会保留，如果任务是失败自己退出，那不会自动删除，会保留
checkpointConf.enableExternalizedCheckpoints(
CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
~~~

### 使 使用 用 Flink ParameterTool 读取配置

在实际开发中，有各种环境（开发、测试、预发、生产），作业也有很多的配置：算子的并行度配置、Kafka 数据源的配置（broker 地址、topic 名、group.id）、Checkpoint是否开启、状态后端存储路径、数据库地址、用户名和密码等各种各样的配置，可能每个环境的这些配置对应的值都是不一样的。

如果你是直接在代码⾥⾯写死的配置，每次换个环境去运行测试作业，都要重新去修改代码中的配置，然后编译打包，提交运行，这样就要花费很多时间在这些重复的劳动力上了。在 Flink 中可以通过使用 ParameterTool 类读取配置，它可以读取环境变量、运行参数、配置文件。ParameterTool 是可序列化的，所以你可以将它当作参数进行传递给算子的自定义函数类。

![1638663599257](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/082000-579994.png)

### 压测方式

压测的方式很简单，先在 kafka 中积压数据，之后开启 Flink 任务，出现反压，就是处理瓶颈。相当于水库先积水，一下子泄洪。数据可以是自己造的模拟数据，也可以是生产中的部分数据。

## 反压处理

### 反压产生的机制

![1638663971068](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1638663971068.png)

在Flink中，每一个Task都有一个读写缓存，还每有处理的数据存放在读缓存，处理完的数据放在写缓存，如果中间某一个task处理数据慢，那么就会导致其都缓存中存在数据的挤压，所以产生了一个连锁反应，导致上游所有的task读写缓存都存在数据的挤压，那么数据源kafka采用的是拉去数据的方式，所以source不会去kafka中读数据，所以就产生了数据的延迟。

如果source端采用的是推模式，那么就会产生内存的溢出，kafka只会产生数据延迟高，因为是拉模式。

### spark streaming反压机制

![1638664332238](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1638664332238.png)

Spark streamming会有一个单独的Executor来接收数据，他会和Driver之间进行交互，是否接收数据，如果Driver下游的多个任务中，有一个产生了反压现象，那么他会主动给Driver报告产生反压，然后Driver会通知Executor接收数据的线程接收慢一点。

这两种模式哪一种好点？

> spark的反压是否是一个全局的模式，就是下游某一个任务产生反压，那么他会向Driver报告，Driver会通知Executor发送给下游所有任务数据量少一点，那么整个集群的吞吐量就会降低。
>
> 所以是一个任务会影响整个任务吞吐量，表现在所有任务的数据量少了，但是此时可能其他任务还没有受什么影响，浪费资源。而Flink不会这样，Flink是一个逐级反馈过程。

Flink反压的好处，是一个链式反应，如果某一个任务产生反压，那么上游的算子可能会帮助产生反压的算子分担一点数据量，这样一级一级的反应，可能到source端就消失了，还可以正常消费数据。

### 反压处理

反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。

反压机制是指系统能够自己检测到被阻塞的 Operator，然后自适应地降低源头或上游数据的发送速率，从而维持整个系统的稳定。Flink 任务一般运行在多个节点上，数据从上游算子发送到下游算子需要网络传输，若系统在反压时想要降低数据源头或上游算子数据的发送速率，那么肯定也需要网络传输。所以下面先来了解一下 Flink 的网络流控（Flink 对网络数据流量的控制）机制。

### 反压现象及定位

Flink 的反压太过于天然了，导致无法简单地通过监控 BufferPool 的使用情况来判断反压状态。Flink 通过对运行中的任务进行采样来确定其反压，如果一个 Task 因为反压导致处理速度降低了，那么它肯定会卡在向 LocalBufferPool 申请内存块上。那么该 Task 的stack trace 应该是这样：

~~~ java
java.lang.Object.wait(Native Method)
o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)
o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) [...]
~~~

监控对正常的任务运行有一定影响，因此只有当 Web 页面切换到 Job 的BackPressure 页面时，JobManager 才会对该 Job 触发反压监控。默认情况下，JobManager 会触发 100 次 stack trace 采样，每次间隔 50ms 来确定反压。Web 界面 看 到 的 比 率 表 示 在 内 部 方 法 调 用 中 有 多 少 stack trace 被 卡 在LocalBufferPool.requestBufferBlocking()，例如: 0.01 表示在 100 个采样中只有 1 个被卡在 LocalBufferPool.requestBufferBlocking()。采样得到的比例与反压状态的对应关系如下：

- OK: 0 <= 比例 <= 0.10
- LOW: 0.10 < 比例 <= 0.5
- HIGH: 0.5 < 比例 <= 1

Task 的状态为 OK 表示没有反压，HIGH 表示这个 Task 被反压。其实就是表示内存使用量的关系，内存使用量超过0.5，说明内存不够，快产生反压。

### 利用 Flink Web UI 定位产 生

在 Flink Web UI 中有 BackPressure 的页面，通过该页面可以查看任务中 subtask的反压状态，如下两图所示，分别展示了状态是 OK 和 HIGH 的场景。

排查的时候，先把 operator chain 禁用，方便定位。因为一个任务连中包含若干个算子，我们需要具体找到哪一个算子产生反压。

![1638665701379](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/085513-275912.png)

从下面可以看到Ratio比率很接近1，也就是内存快满了，所以产生反压。

![1638665713819](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/085515-667868.png)

通常会对每一个算子的一个并行度进行反压检测，也就是说有几个并行度，就检测几个反压。如果有某一个并行度产生反压，那往往是数据倾斜导致。

可以看到，针对每一个任务都会有两块内存，读和写缓存：

![1638665852778](https://tprzfbucket.oss-cn-beijing.aliyuncs.com/hadoop/202112/05/085733-695524.png)

web ui适合临时的查看任务情况，不适合监控，需要配合监控工具使用。

### 利用 Metrics 定位反压位置

利用监控项监控。

当某个 Task 吞吐量下降时，基于 Credit 的反压机制，上游不会给该 Task 发送数据，所以该 Task 不会频繁卡在向 Buffer Pool 去申请 Buffer。反压监控实现原理就是监控Task 是否卡在申请 buffer 这一步，所以遇到瓶颈的 Task 对应的反压⻚⾯必然会显示OK，即表示没有受到反压。

如果该 Task 吞吐量下降，造成该 Task 上游的 Task 出现反压时，必然会存在：该Task 对应的 InputChannel 变满，已经申请不到可用的 Buffer 空间。如果该 Task 的InputChannel 还能申请到可用 Buffer，那么上游就可以给该 Task 发送数据，上游 Task也就不会被反压了，所以说遇到瓶颈且导致上游 Task 受到反压的 Task 对应的InputChannel 必然是满的（这⾥不考虑⽹络遇到瓶颈的情况）。从这个思路出发，可以对该 Task 的 InputChannel 的使用情况进行监控，如果 InputChannel 使用率 100%，那么 该 Task 就是 我们要 找的 反压 源。 Flink 1.9 及以 上版 本 inPoolUsage 表 示inputFloatingBuffersUsage 和 inputExclusiveBuffersUsage 的总和。

![1638666511218](C:\Users\MrR\AppData\Roaming\Typora\typora-user-images\1638666511218.png)

反压时，可以看到遇到瓶颈的该 Task 的 inPoolUage 为 1。

### 反压的原因及处理

先检查基本原因，然后再深入研究更复杂的原因，最后找出导致瓶颈的原因。下面列出从最基本到比较复杂的一些反压潜在原因。

注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。

#### 系统资源

检查涉及服务器基本资源的使用情况，如 CPU、网络或磁盘 I/O，目前 Flink 任务使用最主要的还是内存和 CPU 资源，本地磁盘、依赖的外部存储资源以及网卡资源一般都不会是瓶颈。如果某些资源被充分利用或大量使用，可以借助分析工具，分析性能瓶颈（JVMProfiler+ FlameGraph 生成火焰图）。

在启动任务之前，一定要去进行压力测试，测试出任务数据的最高峰，然后根据最高峰的数据率去分配资源，否则的话，某一时刻数据率非常高，资源不够就会产生压测。

## 数据倾斜

数据倾斜也会引起反压，比如一个任务中有5个子任务，那么可能有某一个任务数据量太大产生反压，这种情况往往是数据倾斜导致的问题。有几个分区，就有几个并行度。

## KafkaSource调优

## FlinkSql调优



## 如何聊优化

做过哪些优化或者解决过什么问题，或者遇到什么问题？这些都是在问优化问题。

1. 第一步，说明**业务场景**，在做什么业务，或者执行什么sql的时候。
2. 第二步：**遇到了什么问题**，往往通过监控工具或者报警系统，发现了什么问题。
3. 第三步：**排查问题**，如果是反压，可能是数据量真的很大，之前还没有做过压测，导致现在所有的并行度全部出现了反压，高峰期扛不住，所以这种情况只能通过添加机器来解决。如果某一个并行度或者任务出现反压，那么可能出现了数据倾斜，我们针对数据倾斜，做了什么解决。如果是mr任务，那么可以通过日志查看，有一两个任务由于数据量大执行特别慢，而其他任务已经执行完成。少数任务卡在了99%.在Flink中是通过反压方式报警提醒。
4. 第四步：**解决问题**，说明解决问题，使用了什么方案解决。